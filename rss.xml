<?xml version="1.0" encoding="ISO-8859-1"?>
<rss version="2.0">
<channel>
<title>Harvesting Clouds</title>
<link>http://HarvestingClouds.com</link>
<description>Blog about all things regarding private and public clouds</description>
<language>en-us</language>
<item>
<title>Azure Awareness Month - Webinar #5 Video - How to Use Azure Site Recovery for Backup, Migration, and Disaster Recovery</title>
<description><![CDATA[<p>This last session in the Azure Awareness month covered Azure Site Recovery services which include Azure Backup and Azure Recovery Services. The session discussed how to provide Backup and Disaster Recovery. It also provided a brief strategy overview for Migration. As usual this is demo heavy session.</p>
<p>You can check the video here:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/NpAc1XFdkJM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></description>
<link>http://HarvestingClouds.com/post/azure-awareness-month-webinar-5-video-how-to-use-azure-site-recovery-for-backup-migration-and-disaster-recovery</link>
<pubDate>Sat, 30 Mar 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Windows Virtual Desktop in Azure - Public Preview Alert</title>
<description><![CDATA[<p><strong>Windows Virtual Desktop</strong> is a new service that is available as a Public Preview at the time of writing of this blog. This is a desktop and app virtualization service that runs on the cloud.</p>
<h3>Pre-requisites</h3>
<p>You need the following to be able to use this service:</p>
<ol>
<li>Azure AD (Active Directory) - If you have an Azure subscription, then you already have this. </li>
<li>Windows Server Active Directory in sync with Azure Active Directory</li>
<li>Azure subscription, containing a virtual network that either contains or is connected to the Windows Server Active Directory - this is where the virtualization will be created and configured</li>
</ol>
<p>Additionally, the Azure VM you create for Windows Virtual Desktop must be:</p>
<ol>
<li>Standard domain-joined or Hybrid AD-joined. Virtual machines can't be Azure AD-joined.</li>
<li>Running one of the following supported OS images: Windows 10 Enterprise multi-session or Windows Server 2016</li>
</ol>
<h3>Working with the Service</h3>
<p>At a very high level the steps involve the following:</p>
<ol>
<li>Grant Azure Active Directory permissions to the Windows Virtual Desktop Preview service</li>
<li>Assign the TenantCreator application role to a user in your Azure Active Directory tenant</li>
<li>Create a Windows Virtual Desktop Preview tenant</li>
<li>Create a Windows Virtual Desktop Host Pool - available as an offering from the Azure Marketplace right now</li>
</ol>
<img src="/images/15540720605ca141fc3f08c.png" alt="Provision a host pool">
<p>You can learn more about this preview and detailed step by step getting started guide here: <a href="https://docs.microsoft.com/en-us/azure/virtual-desktop/overview" target="_blank">Windows Virtual Desktop Preview</a></p>]]></description>
<link>http://HarvestingClouds.com/post/windows-virtual-desktop-in-azure-public-preview-alert</link>
<pubDate>Mon, 25 Mar 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Awareness Month - Webinar #4 Video - Security in Azure - How to Make Your Environment More Secure</title>
<description><![CDATA[<p>The fourth session in the Azure Awareness month was all about security in Azure. We discussed different options around security features available in Azure. The focus of this session was to make your environment more secure. </p>
<p>You can check the video recording here:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/L95Ax9W-L2c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></description>
<link>http://HarvestingClouds.com/post/azure-awareness-month-webinar-4-video-security-in-azure-how-to-make-your-environment-more-secure</link>
<pubDate>Sat, 23 Mar 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Differences between Azure Functions and Azure Logic Apps</title>
<description><![CDATA[<p><strong>Azure Functions</strong> and <strong>Azure Logic Apps</strong> are key offerings from Microsoft Azure that provide you <strong>Serverless Computing</strong>. They both can help run business logic for your applications and can also automate a lot of tasks in Azure. Azure Functions which can execute code in almost any modern language. Whereas Azure Logic Apps which are designed in a web-based designer and can execute logic triggered by Azure services without writing any code.</p>
<p>The major difference between the two is that Azure Functions provide you with lot more control over the code that gets executed but you have to author and manage that code. Whereas with Azure Logic Apps you are working only in a designer and configuring the activities. You get less control but at the same time, you do not have to manage the underlying code as well. You only need to manage the properties of your activities (or <em>logic blocks</em>) in your <strong><em>workflow</em></strong>.</p>
<p>Additionally, here are more differences between these two services:</p>
<style>
table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}
</style>
<table>
<thead>
<tr>
<th></th>
<th>Functions</th>
<th>Logic Apps</th>
</tr>
</thead>
<tbody>
<tr>
<td><b>State</b></td>
<td>Normally stateless, but Durable Functions provide state</td>
<td>Stateful</td>
</tr>
<tr>
<td><b>Development</b></td>
<td>Code-first (imperative)</td>
<td>Designer-first (declarative)</td>
</tr>
<tr>
<td><b>Connectivity</b></td>
<td>About a dozen built-in binding types, write code for custom bindings</td>
<td>Large collection of connectors, Enterprise Integration Pack for B2B scenarios, build custom connectors</td>
</tr>
<tr>
<td><b>Actions</b></td>
<td>Each activity is an Azure function; write code for activity functions</td>
<td>Large collection of ready-made actions</td>
</tr>
<tr>
<td><b>Monitoring</b></td>
<td>Azure Application Insights, Log Analytics via Application Insights connector</td>
<td>Azure portal, Log Analytics</td>
</tr>
<tr>
<td><b>Management</b></td>
<td>REST API, Visual Studio</td>
<td>Azure portal, REST API, PowerShell, Visual Studio</td>
</tr>
<tr>
<td><b>Execution context</b></td>
<td>Can run locally or in the cloud</td>
<td>Runs only in the cloud.</td>
</tr>
</tbody>
</table>]]></description>
<link>http://HarvestingClouds.com/post/differences-between-azure-functions-and-azure-logic-apps</link>
<pubDate>Tue, 19 Mar 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Awareness Month - Webinar #3 Video - ARM Template Basics: Automated & Repeatable Deployments in Microsoft Azure</title>
<description><![CDATA[<p>This third webinar in the Azure Awareness month series was around ARM Template Basics. In this session, I talked about how to perform automated &amp; repeatable deployments in Microsoft Azure leveraging ARM Templates. We discussed how to author ARM Templates as well as how to kick start the development. </p>
<p>You can check the video recording here:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MZTb9NtZOh4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></description>
<link>http://HarvestingClouds.com/post/azure-awareness-month-webinar-3-video-arm-template-basics-automated-repeatable-deployments-in-microsoft-azure</link>
<pubDate>Sun, 17 Mar 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Awareness Month - Webinar #2 Video - Automate Your Azure Environment with Azure Automation and Azure Functions</title>
<description><![CDATA[<p>In this session around automating your environment, we looked at how to work with Azure Automation and Azure Functions. The session included demos to get you started in these technologies. You can check the video below. </p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Bl_6atthAOk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></description>
<link>http://HarvestingClouds.com/post/azure-awareness-month-webinar-2-video-automate-your-azure-environment-with-azure-automation-and-azure-functions</link>
<pubDate>Sat, 09 Mar 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Awareness Month - Webinar #1 Video - Getting Started with Azure – Basics, Tips, and Tricks</title>
<description><![CDATA[<p>We started the Azure Awareness month with the first webinar in the series last Friday. This session was about &quot;Getting Started with Azure – Basics, Tips, and Tricks&quot;. If you missed this session then you can watch this recorded webinar below on YouTube.</p>
<p>In this session we talked about:</p>
<ol>
<li><strong>Cloud Computing and Azure</strong> - to understand the place of Microsoft Azure in the Cloud Computing world and how we fit in</li>
<li><strong>Basic Concepts in Azure</strong> - to lay the right foundation on which all concepts will be built</li>
<li><strong>General Services in Azure</strong> - to provide you with the most essential services in Microsoft Azure</li>
<li><strong>Tips &amp; Tricks</strong> - to give you that edge and make you more efficient in usage of Microsoft Azure</li>
</ol>
<iframe width="560" height="315" src="https://www.youtube.com/embed/5X1UxHmdyUA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></description>
<link>http://HarvestingClouds.com/post/azure-awareness-month-webinar-1-video-getting-started-with-azure-basics-tips-and-tricks</link>
<pubDate>Sun, 03 Mar 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Tips & Tricks -  Series Index</title>
<description><![CDATA[<p>Microsoft Azure has lots of services to offer. Sometimes you can feel overwhelmed and lost. These tips and tricks will be your guiding star to make your tasks a bit easier. These will increase your efficiency. Even if you save a few seconds on the task at hand it can accumulate to huge time savings over a larger period of time. </p>
<p>This blog is an <strong>Index</strong> of various blogs in the series &quot;Azure Tips &amp; Tricks&quot;:</p>
<ol>
<li><a href="http://harvestingclouds.com/post/azure-tips-tricks-quickly-run-commands-or-script-on-an-azure-virtual-machine-vm-without-logging-into-the-vm/" target="_blank">Quickly run commands or script on an Azure Virtual Machine (VM) without logging into the VM</a></li>
<li><a href="http://harvestingclouds.com/post/azure-tips-tricks-bootstrap-automation-for-new-resources-for-repeated-deployments-in-multiple-environments/" target="_blank">Bootstrap Automation for <strong>New</strong> resources for repeated deployments in multiple environments</a></li>
<li><a href="http://harvestingclouds.com/post/azure-tips-tricks-bootstrap-automation-for-existing-resources-for-repeated-deployments-in-multiple-environments/" target="_blank">Bootstrap Automation for <strong>Existing</strong> resources for repeated deployments in multiple environments</a></li>
<li><a href="http://harvestingclouds.com/post/azure-tips-tricks-find-solutions-to-most-common-problems-for-any-resource/" target="_blank">Find solutions to most common problems for any resource</a></li>
<li><a href="http://harvestingclouds.com/post/azure-tips-tricks-quickly-navigate-azure-portal-and-search-documentation-with-ease/" target="_blank">Quickly navigate Azure Portal and search documentation with ease</a></li>
<li><a href="http://harvestingclouds.com/post/azure-tips-tricks-quickly-assign-and-check-the-azure-policy-and-initiative-compliance-status-of-your-resources-with-new-azure-feature/" target="_blank">Quickly assign and check the Azure Policy and Initiative Compliance status of your resources with new Azure feature</a></li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/azure-tips-tricks-series-index</link>
<pubDate>Mon, 25 Feb 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Script Samples - Series Index</title>
<description><![CDATA[<p>This blog is an <strong>Index</strong> of various blogs and related script samples in the series &quot;<strong>Azure Script Samples</strong>&quot;:</p>
<ol>
<li><a href="http://harvestingclouds.com/post/script-sample-format-ea-billing-usage-csv-for-tags/" target="_blank">Format EA Billing Usage Csv for Tags</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-export-all-azure-automation-account-runbooks-and-variables/" target="_blank">Export All Azure Automation Account Runbooks and Variables</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-export-all-oms-log-analytics-saved-searches/" target="_blank">Export All OMS Log Analytics Saved Searches</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-check-azure-site-recovery-asr-prerequisite-services/" target="_blank">Check Azure Site Recovery (ASR) Prerequisite Services</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-set-azure-site-recovery-asr-prerequisite-services/" target="_blank">Set Azure Site Recovery (ASR) Prerequisite Services</a></li>
<li><a href="http://harvestingclouds.com/post/updating-a-custom-rbac-role-in-azure/" target="_blank">Updating a Custom RBAC Role in Azure</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-azure-automation-runbook-for-asr-recovery-plan/" target="_blank">Azure Automation - Runbook for ASR Recovery Plan</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-azure-automation-get-vm-information-from-asr-recovery-plan-context/" target="_blank">Azure Automation - Get VM Information from ASR Recovery Plan Context</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-azure-automation-sending-email-notification/" target="_blank">Azure Automation - Sending Email Notification</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-azure-automation-get-vm-information-from-actual-azure-virtual-machine/" target="_blank">Azure Automation - Get VM Information from actual Azure Virtual Machine</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-azure-automation-check-vm-availability/" target="_blank">Azure Automation - Check VM Availability</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-apply-locks-on-various-azure-resources/" target="_blank">Apply Locks on Various Azure Resources</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-check-for-pending-reboots-on-various-vms-in-your-environment/" target="_blank">Check for Pending Reboots on various VMs in your environment</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-apply-rbac-role-to-users-on-resources/" target="_blank">Apply RBAC Role to Users on Resources</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-getting-azure-resource-reports/" target="_blank">Getting Azure Resource Reports</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-generate-azure-resources-report-by-tags-v30/" target="_blank">Generate Azure Resources Report by Tags v3.0</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-set-tags-on-azure-resources/" target="_blank">Set Tags on Azure Resources</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-creating-multiple-resource-groups-with-rbac-role-assignment/" target="_blank">Creating Multiple Resource Groups with RBAC Role Assignment</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-checking-if-the-prompt-for-current-script-is-elevated-or-not/" target="_blank">Checking if the Prompt for current script is Elevated or not</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-vm-operations-setting-up-the-vm-backup/" target="_blank">VM Operations - Setting up the VM Backup</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-vm-operations-convert-vm-to-managed-disk-vm/" target="_blank">VM Operations - Convert VM to Managed Disk VM</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-vm-operations-change-azure-vm-size/" target="_blank">VM Operations - Change Azure VM Size</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-vm-operations-apply-hub-licensing-to-existing-vms/" target="_blank">VM Operations - Apply HUB Licensing to Existing VMs</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-vm-operations-export-vm-configurations/" target="_blank">VM Operations - Export VM Configurations</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-vm-operations-working-with-vm-snapshots-and-moving-the-managed-vms-across-subscriptions-and-regions/" target="_blank">VM Operations - Working with VM Snapshots and moving the Managed VMs across subscriptions and regions</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-removing-locks-from-azure-resources/" target="_blank">Removing Locks from Azure Resources</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-generate-report-for-route-tables-with-associated-subnets-and-related-information/" target="_blank">Generate Report for Route Tables with associated Subnets and related information</a></li>
<li><a href="http://harvestingclouds.com/post/script-sample-disassociate-and-associate-subnets-to-route-tables/" target="_blank">Disassociate and Associate Subnets to Route Tables</a></li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/azure-script-samples-series-index</link>
<pubDate>Sat, 23 Feb 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Tips & Tricks - Quickly assign and check the Azure Policy and Initiative Compliance status of your resources with new Azure feature</title>
<description><![CDATA[<p>Now you can assign policies, initiatives directly from the resource blade. You can also check the compliance status of the policies and take corrective actions on them.
You can now access this feature, not just from the <strong>Subscriptions</strong> blade but also from the <strong>Resource Group</strong> and <strong>individual resources</strong> blade.</p>
<p>To access this information from the Resource Group level, just navigate to your resource group and search for &quot;<strong>Policies</strong>&quot; option under Settings of the blade. </p>
<img src="/images/15513074625c7712c6bf854.png" alt="Policies Setting at Resource Group level">
<p>To access this from a resource e.g. a Virtual Machine, navigate to the VM and search for &quot;Policies&quot; setting. This is under &quot;Operations&quot; section of the VM. </p>
<img src="/images/15513075245c7713047de46.png" alt="Policies Setting at Resource level">
<p>In the above example, the policy to apply tag is not compliant. You can click on individual policy names and investigate further.  </p>
<p>As soon as I apply the tags and the policies compliance is checked next time, it will change the status to Compliant (as shown below). As a best practice, you should be reviewing the compliance status of your environment at periodic intervals. </p>
<img src="/images/15513126205c7726ec59bc7.png" alt="Policies Compliance status after taking corrective action">]]></description>
<link>http://HarvestingClouds.com/post/azure-tips-tricks-quickly-assign-and-check-the-azure-policy-and-initiative-compliance-status-of-your-resources-with-new-azure-feature</link>
<pubDate>Thu, 21 Feb 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Updates - New VM-series for Confidential Computing</title>
<description><![CDATA[<p>A new VM-series backed by specialized hardware, which will include the latest generation of Intel SGX. </p>
<ul>
<li>Based on Trusted Execution Environments: Intel SGX, Virtualization Based Security (VBS) </li>
<li>Comm application patterns: Protect data confidentiality, integrity, and sensitive IP </li>
<li>Protect data and code in use: Isolated portion of processor and memory, code and data cannot be viewed/modified </li>
<li>Cloud offering: TEE-enabled compute platform, cloud attestation, first-party‒enabled services </li>
<li>Centrally combine data sources, Communicate with secure endpoints, licensing and DRM</li>
</ul>
<img src="/images/15538714305c9e324654528.png" alt="Marking a Service as Favorite">
<p>At the time of writing of this blog, this feature is limited preview access under NDA (announced via blog) of a specialized Virtual Machine Series (DC-series) that will become part of the  Azure Compute portfolio and the first installment of the broader Azure Confidential Computing (ACC) initiative. Put simply, confidential computing offers protection that to date has been missing from public clouds: <strong>encryption of data while in use</strong>.</p>
<p>You can read more about this here: <a href="https://azure.microsoft.com/en-us/blog/azure-confidential-computing/" target="_blank">Azure confidential computing
</a></p>]]></description>
<link>http://HarvestingClouds.com/post/azure-updates-new-vm-series-for-confidential-computing</link>
<pubDate>Mon, 04 Feb 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Tips & Tricks - Quickly navigate Azure Portal and search documentation with ease</title>
<description><![CDATA[<p>Navigating Azure Portal can become challenging. You want to navigate to different Azure Resources or Resource Groups or want to check a particular Service in Azure. Instead of going to &quot;All Services&quot; and searching for your resource, if you can save a little time by searching more efficiently, it can add up over time and make you more efficient as well. So here are few tips on how you can navigate with ease in Azure.</p>
<h3>1. Pin/Favorite most common Services and re-ordering them</h3>
<p>For a recent project, I was working with Azure Route Table a lot. I was navigating to these multiple times and making a lot of changes. Instead of going to &quot;All Services&quot; and searching for Route Table service every time, you search for it only once and Star it to mark it as Favorite. It will start showing up in the Services list to the left under Favorites.  </p>
<img src="/images/15513414255c779771e2940.png" alt="Marking a Service as Favorite">
<p>Next, you want to re-order the Services under Favorites, so you don't have to scroll to navigate to most common services. You can do so by hovering the cursor on the service name under Favorites, and then click and drag on the ellipse (i.e. 3 dots to the right of the service name) as shown below. </p>
<img src="/images/15513414435c7797839cd64.png" alt="Re-ordering the service in Favorites">
<p>Now you will have the most commonly visited service at the tip of your hands. </p>
<h3>2. Pin most common Resources to your dashboard</h3>
<p>Another scenario I faced in one of the projects was that every time I logged in, I had to make some changes to a VM and connect to the same using it's dynamically assigned Public IP. That meant navigating to this VM, every day and starting the VM and connecting to it. I had to start the VM as auto-shutdown was configured on the VM to save cost. Instead of navigating to the Virtual Machines and then searching for the VM and then navigating to it, you can simply Pin the VM to your dashboard. Next time you open the portal, the VM will be right there on the Dashboard. </p>
<img src="/images/15513414555c77978fbf2b8.png" alt="Pinning a resource to a dashboard">
<p>You can similarly Pin other resources like SQL Databases, etc.</p>
<h3>3. Searching for Resources, Resource Groups, Services and Documentation using the new Search bar</h3>
<p>One of the cool features I stumbled upon is the new search bar in the Azure portal. It had made navigating Azure portal so much easier. You just start searching for the resource or Resource Group or Service name in the text box at the top of the portal. You don't even have to type the whole thing. Just start typing and the results will start popping up. Look for different sections in the suggestions box the pops up under the text box. Click on your desired resource or Resource Group or Service. </p>
<img src="/images/15513414665c77979a37437.png" alt="Navigating the smart way">
<p>One advantage of this search box is that you can not just search for resources or resource groups or services, you can also search for Marketplace solutions and Documentation. I have leveraged this feature to search for documentation a lot. You can't remember everything and you have to reference documentation every now and then. Instead of googling the documentation, you can quickly search for it, without leaving the Azure portal and access it. You don't have to worry about navigating away from the Azure portal. The documentation will open up in a separate tab automatically. </p>
<p>I hope that these tips will help you increase your efficiency. Let us know which tip helped you and which you already knew about, in the comments below. </p>]]></description>
<link>http://HarvestingClouds.com/post/azure-tips-tricks-quickly-navigate-azure-portal-and-search-documentation-with-ease</link>
<pubDate>Fri, 01 Feb 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Tips & Tricks - Find solutions to most common problems for any resource</title>
<description><![CDATA[<p>When working with Azure resources, you will be faced with the challenge of diagnosing, troubleshooting and solving issues related to your resources. Opening a Microsoft Support case should be the last resort. </p>
<p>This tip is generic in nature that shows you where to look for various solutions to the most common problems related to a resource. You can find this option under any resource as &quot;<strong>Diagnose and solve problems</strong>&quot;</p>
<img src="/images/15513430465c779dc6376de.png" alt="Diagnose and solve problems">
<h3>Troubleshooting Workflow</h3>
<p>Using this a common troubleshooting workflow will look like this:</p>
<ol>
<li>Click on the &quot;<em>Diagnose and solve problems</em>&quot; option under your resource's blade.</li>
<li>Look for Resource Health. I have had a scenario where after a lot of troubleshooting around Azure Automation, we came to know that the service was experiencing issues in our intended region (US South Central). A quick check from this will rule out the obvious.</li>
<li>Next, you want to check the &quot;<strong>Activity logs</strong>&quot;. You can check those right from this screen. You can find if anybody changed the configurations which might have resulted in errors you are observing. </li>
<li>Next section shows you some of the <strong>most common issues</strong> and how to resolve these. Scroll through these. Sometimes these include step by step guides to help you. Follow these and ensure you did not miss a step.</li>
<li>If nothing works, you can finally open a <strong>Microsoft Support request</strong> right from this screen. <strong>Note</strong>: If you can't find your issue under common problems, I highly recommend that you search for it in forums like StackOverflow etc. before you open the Microsoft Support request. If it is a business critical issue, then I would recommend that you open the support ticket first and then search for the issue in the interest of time. </li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/azure-tips-tricks-find-solutions-to-most-common-problems-for-any-resource</link>
<pubDate>Fri, 25 Jan 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Tips & Tricks - Bootstrap Automation for Existing resources for repeated deployments in multiple environments</title>
<description><![CDATA[<p>When you want to automate resource deployment you will either create the automation templates for an existing set of resources or you will create templates for resources that have not been created yet. For former situation, e.g. you have performed a Proof of Concept and now you want to automate the deployment of these set of resources. </p>
<p>You want to automate this because you want to deploy these resources multiple times at different times. You want to deploy these in a predictive fashion without any errors due to manual mistakes creeping in. The number of resources can be fairly large and you will actually end up saving time on the creation of these automation templates instead of manually creating everything.</p>
<h3>Generating Automation for an existing set of resources</h3>
<p>To generate automation for an existing set of resources, either go to the resource group or to any of the resource. Under <strong>Settings</strong> look for the option for &quot;<strong>Automation script</strong>&quot;. Depending upon number of resources and type of resources, it may take some time to generate the template.</p>
<p><strong>Note</strong>: Even though the option says &quot;Automation script&quot; it still generates an ARM Template for the automation. </p>
<img src="/images/15513440755c77a1cb534d8.png" alt="Automation Template generation for the resources">
<p>Once the template is ready you can do the following:</p>
<ol>
<li>You can inspect the template right there on the screen.</li>
<li>You can download the template or directly add it to your library. Once in the library, you can modify it and share it with your team. Or you can directly Deploy it by clicking on the deploy button.</li>
<li>Look out for the errors regarding the resources that can't be exported. E.g. Key vault policies can't be exported by this wizard.</li>
<li>Next, the Template and Parameters are shown. You can also view, Azure CLI, PowerShell, .Net or Ruby code samples to deploy the template.</li>
<li>Make sure that you inspect the template thoroughly. Remove the resources that you don't need. </li>
<li>Also, there will be a lot of hard-coded values in this auto-generated template. It will still be better than starting everything from scratch and doing everything manually. </li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/azure-tips-tricks-bootstrap-automation-for-existing-resources-for-repeated-deployments-in-multiple-environments</link>
<pubDate>Thu, 24 Jan 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Tips & Tricks - Bootstrap Automation for New resources for repeated deployments in multiple environments</title>
<description><![CDATA[<p>When you want to automate resource deployment you will either create the automation templates for an existing set of resources or you will create templates for resources that have not been created yet. For the latter situation, e.g. automating the deployment of a new set of resources, you will have to start the automation from scratch. Instead of starting from scratch, you can bootstrap your automation via ARM Templates.</p>
<h3>Reuse Existing</h3>
<p>You can first search for your resources or scenarios in the Microsoft Quick Start templates. Note that these are sample templates from Microsoft and community members. These templates can be found here: <a href="https://github.com/Azure/azure-quickstart-templates" target="_blank">Azure Quickstart Templates</a>. </p>
<p>When you click on a template name you will see multiple files as shown below. Actual template file will always be named as &quot;<strong><em>azuredeploy.json</em></strong>&quot;. The related parameters file will be named as &quot;<strong><em>azuredeploy.parameters.json</em></strong>&quot;. All other files are optional and are the supporting files for the template.</p>
<img src="/images/15513749945c781a929e78b.png" alt="Sample Template in Quickstart Repo on GitHub">
<p>You can download these files and edit them. You can fork them directly in GitHub and start editing in a source control environment, but your changes will be public (unless you change your repository to Private). If you do not want to make changes and want to directly deploy one of the template, simply click on the &quot;<strong>Deploy to Azure</strong>&quot; button. This will open Azure portal and you will need to sign into your subscription. Then the template deployment blade will open with the template information already populated. You will just need to select right Subscription, Resource Group and provide various parameters. </p>
<p>It is really simple and saves you lot of time instead of authoring the template from scratch. Why re-invent the wheel. Reuse instead!</p>
<h3>Generating template for new resource creation in Azure</h3>
<p>If you are creating new resource from the Azure portal and you have to perform this same action multiple times, you should consider using an ARM Template. If you want to create an ARM Template for a resource for which you can't find a template, even then you can use Azure portal to bootstrap the template creation, rather than starting from scratch. </p>
<p>Simple go ahead and start creating your resource from Azure portal. At the last screen, instead of actually creating the resource, look for a link with the name as &quot;<strong>Download a template for automation</strong>&quot;. This will open another blade which will show you the template. You can download the template from here. It will also include the related parameters file and a script to perform the deployment of your template. The below screenshot shows you the option for Storage Account creation.</p>
<img src="/images/15513748915c781a2b8fa9a.png" alt="Option for Download a template for automation">
<p><strong>Note</strong>: This option will not be available for all resource types. Microsoft is adding this option to more and more resources everyday. </p>]]></description>
<link>http://HarvestingClouds.com/post/azure-tips-tricks-bootstrap-automation-for-new-resources-for-repeated-deployments-in-multiple-environments</link>
<pubDate>Tue, 15 Jan 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Tips & Tricks - Quickly run commands or script on an Azure Virtual Machine (VM) without logging into the VM</title>
<description><![CDATA[<p>When you are working with Azure Virtual Machines (VM), multiple times you will need to run some command on the VM. For this, you will need to connect to the VM, login and then open the console to run the command and then actually execute it. Instead of going through the trouble of connecting to the VM and logging in during the connection process, you can simple Run the command on the VM directly from the Azure portal. </p>
<p><strong>Note 1</strong>: This feature uses the Azure VM Agent on the VM and will not work if the agent is missing. </p>
<p>To leverage this option navigate to your VM in the portal and select the option to &quot;<strong>Run Command</strong>&quot; under &quot;Operations&quot; section of the VM.</p>
<img src="/images/15513791435c782ac72f69d.png" alt="Run Command Option">
<p>As shown above, you can Run Complete PowerShell script right from this screen. There are various options for common scenarios like checking IP Config which simply executes &quot;<em>ipconfig /all</em>&quot;. You can check these common scenarios and the underlying script by clicking on them. If you want to execute then simply click &quot;Run&quot; in the popup blade. </p>
<img src="/images/15513791525c782ad04aff0.png" alt="Running common scenarios">
<p><strong>Note 2</strong>: You will need to wait for the Output to appear. It takes some time and you just have to be patient. Once you click run, it gets disabled and the operation is running. Once the operation is complete you will see the output in the window. Do not navigate away from this popup blade/window. </p>
<h3>Example of Running PowerShell Script</h3>
<p>Let us say that you want to install IIS on the VM for quick testing. This can be done easily by running the below PowerShell:</p>
<pre><code>Install-WindowsFeature -name Web-Server -IncludeManagementTools</code></pre>
<p>Click on the &quot;<strong>RunPowerShellScript</strong>&quot; option. In the popup blade, type the script and hit Run. Wait for the output to appear. Do not navigate away from the window. You can check the status as indicated below.</p>
<img src="/images/15513791605c782ad85f19e.png" alt="Running any PowerShell Script">
<p>That's all there is to it. For more information you can check the official documentation that can be found here: <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/run-command" target="_blank">Run PowerShell scripts in your Windows VM with Run Command</a></p>]]></description>
<link>http://HarvestingClouds.com/post/azure-tips-tricks-quickly-run-commands-or-script-on-an-azure-virtual-machine-vm-without-logging-into-the-vm</link>
<pubDate>Thu, 10 Jan 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Disassociate and Associate Subnets to Route Tables</title>
<description><![CDATA[<p>If you need to test connectivity of your environment without the User Defined Routes then the easiest and most non-disruptive way is to simply disassociate the subnets from the Route Tables. Once your testing is complete, then you can simply reassociate the subnets back to the original Route Tables. </p>
<p><strong>Note</strong>: Use these scripts with caution as these will affect the networking and communication between resources. Go through the scripts and ensure that you know what the scripts are doing before executing these in your environment. Also, always test these in a dev environment before running in any other environment. </p>
<h3>Script samples and how they work</h3>
<p>The scripts in this sample are:</p>
<ol>
<li><strong>Disassociate-SubnetsFromUDRs.ps1</strong> - This script disassociates the subnets from the Route Tables</li>
<li><strong>Associate-SubnetsToUDRs.ps1</strong> - This script reassociates the subnets back to the Route Tables. This uses a CSV file as an input. A sample of the CSV file is also provided along with the script. </li>
</ol>
<p>The <strong>disassociation script</strong> fetches the Route Tables and then gets the associated Subnet details from there. It then removes the association by setting the route table property on the subnet to <strong><em>$null</em></strong> as shown below:</p>
<pre><code>Set-AzureRmVirtualNetworkSubnetConfig `
-Name $subnetName `
-VirtualNetwork $virtualNetwork `
-AddressPrefix $subnetAddressPrefix `
-RouteTable $null |
Set-AzureRmVirtualNetwork</code></pre>
<p>The <strong>association script</strong> works in an exactly opposite way. The major difference is that this script fetches the information from a CSV file as provided along with the sample. This can also be autogenerated (prior to disassociation). The script sample for generating this CSV can be found here: <a href="http://harvestingclouds.com/post/script-sample-generate-report-for-route-tables-with-associated-subnets-and-related-information/" target="_blank">Generate Report for Route Tables with associated Subnets and related information</a></p>
<p>The command for creating the association of Subnet to the Route Table looks like below. </p>
<pre><code>Set-AzureRmVirtualNetworkSubnetConfig `
-Name $subnetName `
-VirtualNetwork $virtualNetwork `
-AddressPrefix $subnetAddressPrefix `
-RouteTable $routeTable |
Set-AzureRmVirtualNetwork </code></pre>
<h3>Location of the Scripts</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/User%20Defined%20Routes%20(UDRs)%20or%20Route%20Tables%20Related%20Scripts" target="_blank">User Defined Routes (UDRs) or Route Tables Related Scripts</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-disassociate-and-associate-subnets-to-route-tables</link>
<pubDate>Mon, 07 Jan 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Generate Report for Route Tables with associated Subnets and related information</title>
<description><![CDATA[<p>When you have custom User Defined Routes (UDRs) in your environment on your Route Tables, you will have these Route Tables associated with various Subnets. If you want to <strong>report on the association of Route Tables with Subnets</strong> and any related information then you can use this sample script. </p>
<p>One use case is to use this script to export the associations so that if any association gets deleted then you can recreate later by reference to this report. Another script sample uses the output of this script to perform the association of the subnets to the Route Tables. That sample can be found here: <a href="http://harvestingclouds.com/post/script-sample-disassociate-and-associate-subnets-to-route-tables/" target="_blank">Disassociate and Associate Subnets to Route Tables</a></p>
<h3>Script Requirements and Workings</h3>
<p>The script only requires you to update the path for the output csv report. </p>
<p>The script connects to the Azure and fetches all Route Tables by using the below command.</p>
<pre><code>$routeTables = Get-AzureRmRouteTable</code></pre>
<p>It then finds all the subnet linked on those Route Tables by using the Subnets property on the route table object as shown below.</p>
<pre><code>$routeSubnets = $routeTable.Subnets</code></pre>
<p>The script then iterates over all the subnets one by one. It then finds the related subnet information like Subscription id, the virtual network of the subnet and the Resource Group of the Virtual Network. It saves all this information to an array object named <strong>$results</strong>.</p>
<p>Finally, the script outputs all the results using the Export-Csv cmdlet:</p>
<pre><code>$results | Export-Csv -Path $PathToOutputCSVReport -NoTypeInformation</code></pre>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/User%20Defined%20Routes%20(UDRs)%20or%20Route%20Tables%20Related%20Scripts" target="_blank">Report-UDRsWithSubnetInfo.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-generate-report-for-route-tables-with-associated-subnets-and-related-information</link>
<pubDate>Sun, 06 Jan 2019 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Removing Locks from Azure Resources</title>
<description><![CDATA[<p>In one of the earlier posts, we discussed the benefits of using locks and why you should apply these (to avoid accidental deletion) to any critical and important resources in Azure. We also saw a script sample to apply the locks on all key resources in your environment. The same can be reviewed again here: <a href="http://harvestingclouds.com/post/script-sample-apply-locks-on-various-azure-resources/" target="_blank">Apply Locks on Various Azure Resources</a>.</p>
<p>Now once you have locks, you won't be able to remove the resources or remove configurations from them. To be able to remove configurations or to remove the resources itself you will need to remove the locks. You can do so in an automated fashion by using this script sample.</p>
<h3>Script Requirements and Workings</h3>
<p>The script does not have any specific requirements. </p>
<p>The script sample removes lock from all Azure Route Tables, but the same concept can be applied to any type of resource. It first fetches the Route Tables by using the below command:</p>
<pre><code>$routeTables = Get-AzureRmRouteTable</code></pre>
<p>Then it simply removes the lock by using the below command:</p>
<pre><code>Remove-AzureRmResourceLock -LockName DoNotDelete -ResourceGroupName $routeTable.ResourceGroupName -ResourceName $routeTable.Name -ResourceType $routeTable.Type -Force</code></pre>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Remove-Locks" target="_blank">Remove-Locks.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-removing-locks-from-azure-resources</link>
<pubDate>Sun, 16 Dec 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - VM Operations - Working with VM Snapshots and moving the Managed VMs across subscriptions and regions</title>
<description><![CDATA[<p>This script sample is a combination of two scripts. At the time of the writing of this blog, the feature to move managed disks or images built using Managed disk VMs is not available in Azure. A workaround for this is as follows.</p>
<h3>The two scripts</h3>
<p>The two scripts provided are: </p>
<ol>
<li><strong>Copy-SnapshotToStorage.ps1</strong> - This copies the Snapshot to a Storage account. This storage account can be in different subscription or even different region. The script uses the &quot;<strong><em>Start-AzureStorageBlobCopy</em></strong>&quot; cmdlet and also shows the progress of the copy operation. </li>
<li><strong>Create-VMFromSnapshot.ps1</strong> - This creates a VM from a Snapshot in a Storage account. It creates a Managed Disks VM leveraging the snapshot.</li>
</ol>
<h3>Scenario 1 - Moving a Managed VM or a Disk across subscriptions or regions</h3>
<p>You will need to follow the below steps to move a VM built using Managed disks across subscriptions or regions: </p>
<ol>
<li>First, create a snapshot on the VM</li>
<li>Then move the snapshot to a Storage account in the target area. Use the first script here.</li>
<li>Then create a VM using the snapshot by leveraging the second script provided in this post.</li>
</ol>
<h3>Scenario 2 - Moving the Image across subscriptions or regions</h3>
<p>You will need to follow the below steps to move an Image built using Managed disks across subscriptions or regions: </p>
<ol>
<li>Create a new VM using the image</li>
<li>Create a snapshot on the VM</li>
<li>Move the snapshot across to a Storage Account in the target area. Use the first script here.</li>
<li>Create a VM using the snapshot in the target area using the second script.</li>
<li>Finally, capture the VM to an image. Delete the image and snapshots from both source and target once image is created in the target area.</li>
</ol>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Working%20with%20Azure%20VM%20Snapshots" target="_blank">Working with Azure VM Snapshots</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-vm-operations-working-with-vm-snapshots-and-moving-the-managed-vms-across-subscriptions-and-regions</link>
<pubDate>Fri, 14 Dec 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - VM Operations - Export VM Configurations</title>
<description><![CDATA[<p>Exporting a VM configuration is a best practice that you should be doing before altering any configurations on the Virtual Machine. Any action that can corrupt the VM or its configurations should be preceded by exporting of VM's configurations. This allows you to be able to refer back to these configurations and recreate the VM in case of any unexpected scenario. </p>
<p>With the Script sample in this post, you can take the export of multiple VMs by providing the same in a csv configuration file. This file is very simple and the creation of this file itself can also be automated. </p>
<p>The script can also be altered to take an export of all the VMs in the environment. </p>
<h3>Script Requirements</h3>
<p>The script requires you to populate the configurations CSV file. The sample CSV file is also provided along with the script. This configurations file should have the following columns:</p>
<ol>
<li>Computer - Name of the VM in Azure</li>
<li>ResourceGroupName - Resource Group of the VM</li>
</ol>
<h3>Script Working</h3>
<p>This is a very simple but one of the most reusable script. It simply takes the export in two steps.</p>
<p>First it fetches the VM as shown below:</p>
<pre><code>$currentVm = Get-AzureRmVM -ResourceGroupName $ResourceGroupName -Name $virtualMachineName -ErrorAction Stop</code></pre>
<p>Then, it takes the export by using the below command.</p>
<pre><code>$currentVm | ConvertTo-Json -Depth 100 | Out-File -FilePath $fileName</code></pre>
<p><strong>The Trick</strong>: The most important thing is the Depth parameter when using ConvertTo-Json cmdlet. If you do not specify this then nested properties may not export properly. Specifying a large enough depth ensures that all nested properties are also exported. </p>
<p>The script encapsulates all this logic into a reusable function. It also provides the best practices template to invoke this function.
<strong>Note</strong>: Check the comments marked with &quot;<strong>ToDo</strong>&quot; where you should be making the changes. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20VM%20Operations" target="_blank">Export-VMConfig.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-vm-operations-export-vm-configurations</link>
<pubDate>Tue, 11 Dec 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - VM Operations - Apply HUB Licensing to Existing VMs</title>
<description><![CDATA[<p>If you have older VMs in the environment and you wan to leverage Microsoft's Hybrid Use Benefit i.e. HUB licensing then this script can update multiple VMs in your environment in an automated way for you. </p>
<p><strong>NOTE</strong>: </p>
<ul>
<li>Before you apply the HUB licensing, please make sure whether you are eligible for HUB licensing benefits or not. Refer the official documentation here: <a href="https://azure.microsoft.com/en-ca/pricing/hybrid-benefit/" target="_blank">Azure Hybrid Benefit</a></li>
</ul>
<h3>Script Requirements</h3>
<p>The script requires you to populate the configurations CSV file. The sample CSV file is also provided along with the script. This configurations file should have the following columns:</p>
<ol>
<li>Computer - Name of the VM in Azure</li>
<li>OSType - Windows or Linux</li>
<li>ResourceGroupName - Resource Group of the VM</li>
<li>HUBLicensingNeeded - yes or no. The HUB Licensing is set for only the VMs for which this value is set to yes.</li>
</ol>
<h3>Script Working</h3>
<p>In its entirety, the HUB licensing is just a switch for &quot;<strong>LicenseType</strong>&quot; on the VM. For Windows servers, if it's value is set to &quot;<strong>Windows_Server</strong>&quot; then this means that you own a Windows Server license and are claiming the HUB Licensing benefits on this VM. </p>
<p>The script applied the HUB licensing in 3 steps. First, it fetches the VM.</p>
<pre><code>$currentVm = Get-AzureRmVM -ResourceGroupName $ResourceGroupName -Name $virtualMachineName -ErrorAction Stop</code></pre>
<p>Then it applies the HUB licensing as shown below.</p>
<pre><code>$currentVm.LicenseType = "Windows_Server"</code></pre>
<p>Lastly, it updates the VM object.</p>
<pre><code>Update-AzureRmVM -VM $currentVm -ResourceGroupName $ResourceGroupName</code></pre>
<p><strong>Note</strong>: As a best practice, the script also checks whether the HUB Licensing is already applied or not. If the HUB licensing is already there on the VM then the Script does applies it again. There may be small downtime when the VM is updating and therefore a short downtime should be planned when doing this operation. </p>
<p>The script encapsulates all this logic into a reusable function. It also provides the best practices template to invoke this function.
<strong>Note</strong>: Check the comments marked with &quot;<strong>ToDo</strong>&quot; where you should be making the changes. </p>
<p>You can find more related details in official documentation here: <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/hybrid-use-benefit-licensing#convert-an-existing-vm-using-azure-hybrid-benefit-for-windows-server" target="_blank">Azure Hybrid Benefit for Windows Server</a></p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20VM%20Operations" target="_blank">Apply-HubLicensing.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-vm-operations-apply-hub-licensing-to-existing-vms</link>
<pubDate>Fri, 07 Dec 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - VM Operations - Change Azure VM Size</title>
<description><![CDATA[<p>If you want to change the size of multiple Azure VMs in your environment, then you can leverage this script sample to update the sizes in one go. You will want to change the size of the VMs periodically after thoroughly reviewing the consumption. This can optimize your environment and can save you money in the process as well. </p>
<h3>Script Requirements</h3>
<p>The script requires you to populate the configurations CSV file. The sample CSV file is also provided along with the script. This configurations file should have the following columns:</p>
<ol>
<li>Computer - Name of the VM in Azure</li>
<li>OSType - Windows or Linux</li>
<li>ResourceGroupName - Resource Group of the VM</li>
<li>SizeConversionNeeded - yes or no</li>
<li>NewVMComputeSize - new size of the VM. e.g. Standard_DS4_v2. This should conform to the VM size names. For Windows VMs these names can be found by navigating to one of the sizes related link here: <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes" target="_blank">Sizes for Windows virtual machines in Azure</a></li>
</ol>
<h3>Script Working</h3>
<p>The script works in 3 simple steps. In the first step the script fetches the Azure VM using Get-AzureRmVM cmdlet and ensures that it exists. </p>
<pre><code>$currentVm = Get-AzureRmVM -ResourceGroupName $ResourceGroupName -Name $virtualMachineName -ErrorAction Stop</code></pre>
<p>The second step is to update the size property for the Hardware profile of the vm object.</p>
<pre><code>$currentVm.HardwareProfile.VmSize = $vmSize </code></pre>
<p>Lastly, the script updates the VM using the below command.</p>
<pre><code>Update-AzureRmVM -VM $currentVm -ResourceGroupName $ResourceGroupName</code></pre>
<p><strong>Note</strong> that there will be a restart on the VM during the size conversion. Do plan for an outage although very minimal. Do keep some buffer in the outage window to account for any unforeseen issues that may occur during the conversion. I have converted various VMs and have never encountered any issues. Still, when I plan for an outage, as a best practice I do plan for a larger window. </p>
<p>The script encapsulates all this logic into a reusable function. It also provides the best practices template to invoke this function.
<strong>Note</strong>: Check the comments marked with &quot;<strong>ToDo</strong>&quot; where you should be making the changes. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20VM%20Operations" target="_blank">Change-AzureVMSize.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-vm-operations-change-azure-vm-size</link>
<pubDate>Mon, 03 Dec 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - VM Operations - Convert VM to Managed Disk VM</title>
<description><![CDATA[<p>If you want to convert multiple VMs from older Storage Account disks to Managed Disks you can leverage the automation capabilities provided by Microsoft via PowerShell. This script sample uses these capabilities in a managed and reusable way.</p>
<h3>Script Requirements</h3>
<p>The script requires you to populate the configurations CSV file. The sample CSV file is also provided along with the script. This configurations file should have the following columns:</p>
<ol>
<li>Computer - Name of the VM in Azure</li>
<li>OSType - Windows or Linux</li>
<li>ResourceGroupName - Resource Group of the VM</li>
<li>ManagedDiskConversion - yes or no. Set this to yes for the VMs for which you want to perform the conversion from older disk type to Managed Disk VMs</li>
</ol>
<h3>Script Working</h3>
<p>The process of Managed Disk conversion is different for a VM with Availability Set and one without it. For this reason the script first checks for the Availability Set of the VM.</p>
<pre><code>$avSet = Get-AzureRmAvailabilitySet -ResourceGroupName $rgName -Name $asName -ErrorAction Stop</code></pre>
<p>It then checks and sets the <strong>Managed</strong> flag on the Availability Set by using the following command.</p>
<pre><code>Update-AzureRmAvailabilitySet -AvailabilitySet $avSet -Managed -ErrorAction Stop</code></pre>
<p>It then stops the VM, converts it to Managed Disk Vm and then waits for 600 seconds. The conversion is performed by following command.</p>
<pre><code>ConvertTo-AzureRmVMManagedDisk -ResourceGroupName $rgName -VMName $vm.Name</code></pre>
<p><strong>Note</strong>: The script does not start the VM to avoid unnecessary costs. E.g. if the VM was stopped to begin with and you didn't want to start it. Unnecessary starting of a VM will incur you charges that you do not want. You can still update the script to start the VMs by simply adding the cmdlet for <strong><em>Start-AzureRmVM</em></strong> if you want.</p>
<p>The script encapsulates all this logic into a reusable function. It also provides the best practices template to invoke this function.
<strong>Note</strong>: Check the comments marked with &quot;<strong>ToDo</strong>&quot; where you should be making the changes. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20VM%20Operations" target="_blank">Convert-VMToManagedDisk.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-vm-operations-convert-vm-to-managed-disk-vm</link>
<pubDate>Sun, 02 Dec 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - VM Operations - Setting up the VM Backup</title>
<description><![CDATA[<p>If you want to set up VM Backup for multiple VMs in your environment you can leverage this script. This script lets you provide a CSV file as an input to the script and configures the Backup on all the VMs as per the configurations.</p>
<h3>Script Requirements</h3>
<p>The script requires you to populate the configurations CSV file. The sample CSV file is also provided along with the script. This configurations file should have the following columns:</p>
<ol>
<li>Computer - Name of the VM in Azure</li>
<li>OSType - Windows or Linux</li>
<li>ResourceGroupName - Resource Group of the VM</li>
<li>EnableBackup - yes or no</li>
<li>RecoveryServicesVaultName - Recovery Services Vault Name for the Backup</li>
<li>BackupProtectionPolicy - Backup Protection Policy name inside the Recovery Services Vault. E.g. Daily-30--Weekly-8--Monthly-6</li>
</ol>
<h3>Script Working</h3>
<p>The script first fetches the Azure Recovery Services vault using the below command.</p>
<pre><code>$recoveryServicesVault = Get-AzureRmRecoveryServicesVault -Name $RecoveryServicesVaultName</code></pre>
<p>It then uses this information to set the Recovery Services Vault Context as shown below.</p>
<pre><code>$recoveryServicesVault | Set-AzureRmRecoveryServicesVaultContext -ErrorAction Stop</code></pre>
<p>It then fetches the container for the VM Backup using below command. Through this, it checks if the Backup is already configured on the VM or not. If it is already configured then no further action is taken. </p>
<pre><code>$namedContainerCheck = Get-AzureRmRecoveryServicesBackupContainer -ContainerType "AzureVM" -Status "Registered" -FriendlyName $virtualMachineName</code></pre>
<p>The Backup policy is fetched next:</p>
<pre><code>$policy = Get-AzureRmRecoveryServicesBackupProtectionPolicy -WorkloadType "AzureVM" | where {$_.Name -eq $BackupProtectionPolicy}</code></pre>
<p>Finally, the Backup is configured on the VM using the below command.</p>
<pre><code>Enable-AzureRmRecoveryServicesBackupProtection -Policy $policy -Name $virtualMachineName -ResourceGroupName $ResourceGroupName -ErrorAction Stop</code></pre>
<p>Once the Backup is successfully configured, as a best practice, we need to trigger an initial Backup of the VM. The same is performed by the below commands.</p>
<pre><code>Write-Host "Fetching the Recovery Services Backup Container"
$namedContainer = Get-AzureRmRecoveryServicesBackupContainer -ContainerType "AzureVM" -Status "Registered" -FriendlyName $virtualMachineName
Write-Host "Fetching the Recovery Services Backup Item"
$item = Get-AzureRmRecoveryServicesBackupItem -Container $namedContainer -WorkloadType "AzureVM"
Write-Host "Triggering a Backup on the VM"
$job = Backup-AzureRmRecoveryServicesBackupItem -Item $item</code></pre>
<p>The script encapsulates all this logic into a reusable function. It also provides the best practices template to invoke this function.
<strong>Note</strong>: Check the comments marked with &quot;<strong>ToDo</strong>&quot; where you should be making the changes. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20VM%20Operations" target="_blank">Enable-VMBackup.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-vm-operations-setting-up-the-vm-backup</link>
<pubDate>Fri, 23 Nov 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Checking if the Prompt for current script is Elevated or not</title>
<description><![CDATA[<p>Multiple times I have run into scenarios where the end users were reporting issues with the script but eventually, it turned out to be an access issue. The script required to be executed from an Elevated console (i.e. Run As Administrator) but the end user was trying to execute it as a normal user. Wouldn't it be nice if the script can check itself if it is being executed from an Elevated prompt or not and would report the same as a requirement.</p>
<p>This script sample can be reused in multiple scenarios and in multiple scripts. It will ensure that the script is executed only via an elevated prompt or else it will throw error with relevant details for the end user to take corrective action. </p>
<h3>Script workings</h3>
<p>The script first fetches the current Windows security principal by using the below commands:</p>
<pre><code>$WindowsID = [System.Security.Principal.WindowsIdentity]::GetCurrent()
$WindowsPrincipal = New-Object System.Security.Principal.WindowsPrincipal($WindowsID)</code></pre>
<p>It then fetches the Administrator Role related details using below commands:</p>
<pre><code>$adminRole = [System.Security.Principal.WindowsBuiltInRole]::Administrator</code></pre>
<p>Now that it have both the required information, the script checks if the current Windows Principal is part of the Administrator role or not by using below condition:</p>
<pre><code>if ($WindowsPrincipal.IsInRole($adminRole))
{
    return $True
}
else
{
    return $False
}</code></pre>
<p>The <strong>complete script sample</strong> turns this into a reusable logic and provides a template for using this with other code. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Get-IsElevated" target="_blank">Get-IsElevated.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-checking-if-the-prompt-for-current-script-is-elevated-or-not</link>
<pubDate>Mon, 19 Nov 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Creating Multiple Resource Groups with RBAC Role Assignment</title>
<description><![CDATA[<p>You can automate the creation of the <strong>Resource Groups</strong> and also assign <strong>RBAC roles</strong> to these Resource Groups. This script sample shows you how you can accomplish this. It serves as a starting point for the automation of the resource group creation. You can modify the script as per your requirements.</p>
<h3>How the Script works</h3>
<p>The script takes the names of the Resource groups to be created as an array. You can modify the script to take this input from a CSV file along with other parameters like the location of the Resource Group and the related role assignments. </p>
<p>The script uses the below command to create a Resource Group.</p>
<pre><code>New-AzureRmResourceGroup -Name $rg -Location eastus</code></pre>
<p>Note that the location is hard-coded in the above command. This can also be parameterized. </p>
<p>It then uses the below command to make the role assignments on this Resource Group.</p>
<pre><code>New-AzureRmRoleAssignment -ObjectId $ADGroup01.Id -RoleDefinitionName "Contributor" -ResourceGroupName $rg</code></pre>
<p>Note that you can use the same command to make the assignments for a group, user or an app registration. You just need to provide the appropriate Id for the ObjectId parameter. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Create-ResourceGroupsWithRBACAssignments" target="_blank">Create-ResourceGroupsWithRBACAssignments.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-creating-multiple-resource-groups-with-rbac-role-assignment</link>
<pubDate>Thu, 15 Nov 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Set Tags on Azure Resources</title>
<description><![CDATA[<p>Earlier we fetches the Azure resources report with the Tagging related information. You can review that script to fetch the report here: <a href="http://harvestingclouds.com/post/script-sample-generate-azure-resources-report-by-tags-v30/" target="_blank">Script Sample - Generate Azure Resources Report by Tags v3.0</a></p>
<p>The output CSV generate by that script can be updated. It can be corrected for missing tag values or incorrect tag values, etc. The updated csv then can become an Input to the script sample discussed in this blog. This script will read from the updated CSV file and will apply the tags back to the Azure.</p>
<p><strong>Note</strong>: This script takes lot of time because the underlying cmdlet takes lot of time to set the tags (i.e. &quot;<strong><em>Set-AzureRmResource</em></strong>&quot; cmdlet). So it is advised that you filter to only the resources for which you want to update the tags.</p>
<h3>Script Workings</h3>
<p>As mentioned earlier, this script inputs the CSV file generated earlier. The schema for this input file can be checked from the output of Get script from the link mentioned above. </p>
<p>The script then connects to Azure and updates the Tags. If the Tags were not present already then it adds the tagging information. It uses the below command to update the tags.</p>
<pre><code>Set-AzureRmResource -Tag $r.Tags -ResourceId $r.ResourceId -Force</code></pre>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Tagging%20Reports" target="_blank">Set-AzureRmTags.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-set-tags-on-azure-resources</link>
<pubDate>Wed, 07 Nov 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Generate Azure Resources Report by Tags v3.0</title>
<description><![CDATA[<p>This is version 3.0 of an older script which generates the report for Azure resources by Tags. This is one of the most important script that ensures that you are compliant in your environment. You can pull reports on the CSV generated by this script to see which resources are missing tags or which resources do not have the correct tags applied.</p>
<p><strong>What is updated in this version</strong>: This version now can parse the Tags. It even factors for the spacing between the tag names. You will need to tweak the script to parse your own tags. Sample is provided in the script. You will need to modify the lines between 87 and 145 in the lined script. </p>
<p>You can still refer the older script here along with the details mentioned in that post: <a href="http://harvestingclouds.com/post/script-sample-generate-azure-resources-report-by-tags/" target="_blank">Script Sample - Generate Azure Resources Report by Tags</a></p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Tagging%20Reports" target="_blank">Get-AzureRmTagsReport - v3.0.ps1</a></p>
<h3>Next Steps - Setting the updated Tags</h3>
<p>Now, what do you do witht he output CSV report that you got from running this script. You will update this csv file by making the corrections. If any resources will be missing any tags then you will add the same. After that this CSV file will become an input to another script, which will Set the changes for the Tags back to Azure resources. This script can be found here: <a href="http://harvestingclouds.com/post/script-sample-set-tags-on-azure-resources/" target="_blank">Script Sample - Set Tags on Azure Resources</a>.</p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-generate-azure-resources-report-by-tags-v30</link>
<pubDate>Sat, 03 Nov 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Getting Azure Resource Reports</title>
<description><![CDATA[<p>This post is for two script samples in one. You can fetch reports on Azure resources with the script sample in this post. The two scripts in this post are:</p>
<ol>
<li><strong>Get-AzureRmAllResourceReport</strong> - which fetches the report for all Azure resources in all subscriptions</li>
<li><strong>Get-AzureRmResourceReportForOneResourceGroup</strong> - which fetches the report for Azure resources in a particular Resource Group</li>
</ol>
<h3>Script Inputs</h3>
<p>All you need to do is to provide the path for the output csv file. In the second script you also need to provide the name of the Resource Group.</p>
<p>The script will iterate all Azure resources and will provide an output csv file. For Azure VM and SQL the script will also provide additional information. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Get-AzureRmResourceReport" target="_blank">Scripts to Fetch Azure Resource Reports</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-getting-azure-resource-reports</link>
<pubDate>Thu, 25 Oct 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Apply RBAC Role to Users on Resources</title>
<description><![CDATA[<p>If you have to assign a role to users onto multiple resources, this script can reduce your workload and can do the heavy lifting for you. Simply provide the inputs and you can provide access to the users at any scope i.e. Subscriptions, Resource Group or individual resource. </p>
<p>Currently, the script factors in Virtual Machines as individual resources, but these can be replaced by any resource type. </p>
<h3>Inputs and Script Requirements</h3>
<p>The script takes in the following inputs:</p>
<ol>
<li>csvLocation - this is the CSV containing the name of the resources on which you want to provide the role-based access. An example CSV is also provided with the script. It is simply two column CSV. The first column being the VM Name (or resource name if you want to generalize) and the second column is the Resource Group containing that resource.</li>
<li>role - This is the role that you want to assign. E.g. &quot;Virtual Machine Operator&quot;</li>
<li>Scope - this can be one of the 3 values viz. &quot;VirtualMachine&quot;, &quot;Subscription&quot; or &quot;ResourceGroup&quot;</li>
<li>Usernames - this can be an array of the user names who will be assigned the access on the resources</li>
<li>Groupnames - if groups need to be assigned access then they can be mentioned here</li>
<li>RBACOperatorFlag - a boolean value with the default value of true. You set this to false when you are making changes and do not want the script to perform any actions. You do not need to worry about this parameter for most scenarios and can ignore this. </li>
</ol>
<h3>Example Execution</h3>
<p>Example execution of the script will look like below. </p>
<pre><code>Apply-RBACRoleToResources -csvLocation "C:\Users\aman\Documents\AzureVM.csv" -role "Virtual Machine Operator" -Scope "VirtualMachine" -UserNames "user1,user2" -GroupNames "abac" </code></pre>
<p>The above command will read the VM names and their related REsource Groups form the AzureVM.csv file. This will assign &quot;Virtual machine operator&quot; role at the Virtual Macine level. This role will be assigned to the user1 and user2 users along with abac Group name. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Apply-RBACRoleToResources" target="_blank">Apply-RBACRoleToResources.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-apply-rbac-role-to-users-on-resources</link>
<pubDate>Mon, 22 Oct 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Check for Pending Reboots on various VMs in your environment</title>
<description><![CDATA[<p>Often times you will need to check if there are any VMs/Servers in your environment which have Pending Reboot on them. You will want to schedule and perform a planned reboot on these VMs after going through proper processes.</p>
<p>The script sample in this post is about checking multiple Vms in your environment and check if there is any Pending Reboot on any of the VMs. The script uses WMI queries to check if there is any pending reboot on the machine. </p>
<p>This script was not authored by me. I received it from one of my colleagues and I couldn't track the original author of this script. Nevertheless, this is a very useful script for system administrators. </p>
<h3>Script Usage</h3>
<p>The script can be used as shown below:</p>
<pre><code>C:\Users\aman\Downloads&gt; import-module .\Get-PendingReboot.ps1
C:\Users\aman\Downloads&gt; $Servers = Get-Content C:\Users\aman\Downloads\Servers.txt
C:\Users\aman\Downloads&gt; Get-PendingReboot -Computer $Servers | Export-Csv C:\Users\aman\Downloads\PendingRebootReport.csv -NoTypeInformation</code></pre>
<p>In the 2nd command above, the Server.txt file should simply have the list of names of the computers in your environment with one computer name per line. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Get-PendingReboot" target="_blank">Get-PendingReboot.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-check-for-pending-reboots-on-various-vms-in-your-environment</link>
<pubDate>Mon, 15 Oct 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Apply Locks on Various Azure Resources</title>
<description><![CDATA[<p>Locks is a very important but very less known feature in Azure. This feature is available for all resources in Azure. This prevents unintended operation on a particular resource. </p>
<p>You have two types of locks in Azure:</p>
<ol>
<li><strong>ReadOnly</strong> - You won't' be able to alter any configuration of the resource</li>
<li><strong>DoNotDelete</strong> - You will be able to add configurations but will not be able to remove configurations or even delete the resource</li>
</ol>
<p><strong>Do Not Delete</strong> is the lock, that as a best practice, you should apply on all critical resources in the environment. Once this lock is there on the resources, even the global administrator will not be able to delete the resources. The only way to delete the resources will be to delete the lock first and then delete the resources. </p>
<h3>The Script Sample Details</h3>
<p>This script sample leverages this concept of locks and uses the below cmdlet to apply the locks on various critical resources in the environment. </p>
<pre><code>New-AzureRmResourceLock -LockLevel CanNotDelete -LockName DoNotDelete -ResourceName $vNet.Name -ResourceType $vNet.Type -ResourceGroupName $vNet.ResourceGroupName -LockNotes "Do Not Delete Lock" -Confirm -Force</code></pre>
<p>The above command uses <strong><em>New-AzureRmResourceLock</em></strong> cmdlet to create the Do Not Delete lock on a Virtual Network. </p>
<p>This script iterates through all subscriptions that your account has access to and then applies the lock to all resources of type:</p>
<ol>
<li>Virtual Network</li>
<li>Route Tables</li>
<li>Express Routes</li>
<li>Virtual Network Gateways</li>
<li>Virtual Network Gateway Connections</li>
<li>Recovery Services Vaults (i.e. ASR Vaults)</li>
</ol>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Apply-LocksOnVariousAzureResources" target="_blank">Apply-LocksOnVariousAzureResources.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-apply-locks-on-various-azure-resources</link>
<pubDate>Thu, 11 Oct 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Azure Automation - Check VM Availability</title>
<description><![CDATA[<p>One of the most common scenarios is to check if a VM is available or not. If you want to connect to a VM and take some actions, it is a best practice to first check if the VM is available and responsive or not. This script sample performs multiple checks. You can tweak these checks to include only the ones that you need. </p>
<p>Also, there are some pieces of information that you can get only from inside the VM. E.g. the domain to which a VM is connected. This scripts ensures that the script is available. If it is available then it can also fetch such pieces of information for you. </p>
<h3>What does the Script check</h3>
<p>The script makes multiple checks which include:</p>
<ol>
<li>Ping check using the <strong><em>Test-Connection</em></strong> cmdlet</li>
<li>PS Remoting checking by trying to open a PS Session using <strong><em>New-PSSession</em></strong> cmdlet</li>
<li>The script also tries to connect to the WMI and list various object using <strong><em>Get-Wmiobject</em></strong> cmdlet</li>
<li>Similar to the PS Remoting check it also checks for WS Man by using the <strong><em>Test-WSMan</em></strong> cmdlet</li>
</ol>
<p>It can then fetch VM related information by either remoting into it or by using WMI. E.g. it can fetch domain of the VM by using below WMI cmdlet.</p>
<pre><code>$Domain = $wmi.GetStringValue($HKLM, $MachineKey, "NV Domain").sValue</code></pre>
<h3>Script Inputs and Requirements</h3>
<p>The script only takes below two input parameters:</p>
<ol>
<li>NameOfTheVM</li>
<li>IPOfVM</li>
</ol>
<p>The script requires a credential object with the name as &quot;<strong><em>AutomationCredentialName</em></strong>&quot;. These credential should have login and remoting access into the VM. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20Automation%20-%20Check%20VM%20Availability" target="_blank">Check-VMAvailability.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-azure-automation-check-vm-availability</link>
<pubDate>Fri, 05 Oct 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Azure Automation - Get VM Information from actual Azure Virtual Machine</title>
<description><![CDATA[<p>Many times, when automating your infrastructure, you will need to work with Azure Virtual Machines. Based on just the name of the VM and it's Resource Group, you will need to know about it's Network Interfaces and it's IP address. </p>
<p>This Runbook sample connects to Azure using Service Principal and fetches various VM properties. The Runbook can be tweaked to fetch more or less information as per the requirements. </p>
<h3>Script Input and Requirements</h3>
<p>The script takes the below inputs:</p>
<ol>
<li>VM Name - name of the VM </li>
<li>ResourceGroupNameofVM - name fo the Resource Group in which the VM exists</li>
<li>Azure Tenant Id - Id of the Tenant. This is the Azure Active Directory's Id which can be found in Azure AD properties in the portal</li>
<li>Subscription Id - Id of the Subscription in which the VM exists</li>
</ol>
<p>The VM also requires a credential Object i.e. a Service Pricipal, which will have access to the Virtual machine. </p>
<h3>Working of the Script</h3>
<p>The script uses the below command to log into the Azure using the Service Principal.</p>
<pre><code>$AzureRMConn = Login-AzureRmAccount -ServicePrincipal -Credential $Cred -TenantId $AzureTenantId -ErrorAction SilentlyContinue -ErrorVariable LoginError</code></pre>
<p>If the connection is successful then the Subscription is selected using below cmdlet.</p>
<pre><code>$ConnSubs = Select-AzureRmSubscription -SubscriptionId $SubscriptionID</code></pre>
<p>Then the actual VM information is fetched using below cmdlet.</p>
<pre><code>$VM = Get-AzureRmVM -ResourceGroupName $VMResourceGroup -Name $VMName</code></pre>
<p>This fetches basic VM information. To fetch detailed information regarding the network interface of the VM, below cmdlets are used.</p>
<pre><code>$nicId = $VM.NetworkProfile.NetworkInterfaces[0].Id
$VMNetowrkInterface = Get-AzureRmNetworkInterface -ResourceGroupName $VMResourceGroup -Name $nicId.Split('/')[8]
$VMMainIPConfig = $VMNetowrkInterface | Get-AzureRmNetworkInterfaceIpConfig
$strVMMainNicName = $VMMainIPConfig.Name
$strVMMainNicIPAddress = $VMMainIPConfig.PrivateIpAddress</code></pre>
<p>The first cmdlet above fetches the Id of the first Network Interface on the VM. Then the second cmdlet uses this Id to fetch the actual network interface. The third cmdlet uses Get-AzureRmNetwrokInterfaceIpConfig to fetch the IP configuration on this network interface. Then last two lines use this Ip configuration to fetch the Private IP address on the VM. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20Automation%20-%20Get%20VM%20Information%20From%20Azure" target="_blank">Get-VMInfoFromAzure.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-azure-automation-get-vm-information-from-actual-azure-virtual-machine</link>
<pubDate>Mon, 24 Sep 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Azure Automation - Sending Email Notification</title>
<description><![CDATA[<p>Sending email notifications from Azure Automation is a requirement that you will encounter a lot when working with any automation scenario. You may want to send an email notification on the completion of the job or on the failure of the job. </p>
<p>This script sample helps you bootstrap the Runbook for you. </p>
<h3>Script Inputs and Requirements</h3>
<p>The script takes the following <strong>input parameters</strong>:</p>
<ol>
<li>SMTP Server URL</li>
<li>SMTP Server Port</li>
<li>User Credentials - Boolean value to check if user credentials are required or not</li>
<li>Credential Name - Credential name for the credential asset in your Azure Automation Account</li>
<li>EnableSsl - Boolean value to select if you want to enable SSL or not</li>
<li>Email From - From address for the email</li>
<li>Email To - Receipient of the email</li>
<li>Email Subject - Subject of the email
9 Message Body - This can be html. So you can compose your email to make it look more professional</li>
</ol>
<p>The script also requires you to have a Credential asset in your Azure Automation Account. It's name is specified in the 4rth parameter above. This credential is used to connect to the SMTP server. In the sample this smtp server is an O365 email account. </p>
<h3>Script Working</h3>
<p>The script uses a SMTP Client to send the email. This client is created using the below cmdlet.</p>
<pre><code>$SmtpClient = New-Object System.Net.Mail.SmtpClient $SMTPServerUrl, $SMTPServerPort</code></pre>
<p>where $SMTPServerUrl is the URL of the SMTP server and $SMTPServerPort is the port number used by the SMTP server.</p>
<p>Later, this client is used to send the email by using it's <strong><em>send</em></strong> function as below. </p>
<pre><code>$SmtpClient.Send($Message)</code></pre>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20Automation%20-%20Send%20Email%20Notification" target="_blank">Send-EmailNotificationFromAzureAutomationRunbook.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-azure-automation-sending-email-notification</link>
<pubDate>Sat, 22 Sep 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Azure Automation - Get VM Information from ASR Recovery Plan Context</title>
<description><![CDATA[<p>This script sample is to parse the Recovery Plan Context from Azure Site Recovery. When an Azure Automation Runbook is invoked directly from Azure Site Recovery, it passes an object called <strong>Recovery Plan Context</strong>.  This has all the crucial information regarding the failover operation like:</p>
<ol>
<li>Recovery Plan Name</li>
<li>Failover Type</li>
<li>Failover DIrection</li>
<li>Subscription Name</li>
<li>Resource Group Name</li>
<li>VM Name</li>
</ol>
<h3>Sample Recovery Plan Context</h3>
<p>A sample Recovery Plan Context looks like below. Please note that this is formatted appropriately here. In the Runbook it will look flattened i.e. instead of multiple lines, the context will be contiguous and will be presented in a single long line. Its data type is Object when coming from ASR Recovery Plan.</p>
<pre><code>{
   "RecoveryPlanName":"RecoveryPlan-Test",
   "FailoverType":"Test",
   "FailoverDirection":"PrimaryToSecondary",
   "GroupId":"Group1",
   "VmMap":{
      "aaaaaa-1111-1111-1111-aaaaaaaaaaa":{
         "SubscriptionId":"bbbbbbbb-2222-2222-2222-bbbbbbbbbbb",
         "ResourceGroupName":"rg-dr-test-asr",
         "CloudServiceName":null,
         "RoleName":"VMName-test",
         "RecoveryPointId":"cccccccccc-3333-3333-3333-cccccccccccc",
         "RecoveryPointTime":"\/Date(1550773609103)\/"
      }
   }
}</code></pre>
<h3>How does this Script work</h3>
<p>The script first uses &quot;<strong><em>ConvertFrom-Json</em></strong>&quot; cmdlet to parses the object to valid PowerShell object. It then uses the VMMap property of the Recovery Plan Context object to get various pieces of the information. </p>
<p>The script also uses a <strong><em>foreach</em></strong> look to factor for multiple Vms in the Recovery Plan. It will <strong>output</strong> an array of the VMs with the required information, which can then be consumed in your logic to perform actual functions on the VMs. E.g. You can then connect to the VM and install some extensions on it. </p>
<h3>How to Invoke This Runbook/Script</h3>
<p>This script can be directly invoked from Azure Site Recovery (ASR)'s Recovery Plan or can be invoked from a master Runbook. It is a best practice to invoke this Runbook from a master Runbook. A sample for this Runbook is provided and discussed here: <a href="http://harvestingclouds.com/post/script-sample-azure-automation-runbook-for-asr-recovery-plan/" target="_blank">Script Sample - Azure Automation - Runbook for ASR Recovery Plan</a></p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20Automation%20-%20ASR%20Recovery%20Plan" target="_blank">ASR-Get-VMSInfoFromASRContext.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-azure-automation-get-vm-information-from-asr-recovery-plan-context</link>
<pubDate>Wed, 12 Sep 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Azure Automation - Runbook for ASR Recovery Plan</title>
<description><![CDATA[<p>When you are automating Azure Site Recovery, you will be working with <strong>Azure Automation Runbooks</strong>. You will invoke these from within the <strong>Azure Site Recovery Plan</strong>.  The input parameter to this Runbook will be a <strong>Recovery Plan Context</strong>, that will automatically be passed by the ASR Recovery Plan. This Recovery plan context will contain information like: </p>
<ol>
<li>Recovery Plan Name</li>
<li>Failover Type</li>
<li>Failover DIrection</li>
<li>Subscription Name</li>
<li>Resource Group Name</li>
<li>VM Name</li>
</ol>
<p>The Azure Automation Runbook should be able to receive this information and parse it. </p>
<h3>Best Practice and this Script Sample</h3>
<p>As a best practice, you should leverage the main Runbook to perform operational functions. E.g. If the automation that you want to trigger on Failover, requires connection to the company's internal network, then you would want it to run on a Hybrid worker (which will be connected to your internal network). As such, I recommend that the parsing of the recovery plan context should be done in another Runbook, which should be invoked by the main Runbook. </p>
<p>This sample PowerShell Runbook accepts the Recovery Plan context and passes it to the second Runbook. That Runbook may or may not run on a Hybrid worker. </p>
<h3>Sample Recovery Plan Context</h3>
<p>A sample Recovery Plan Context looks like below. Please note that this is formatted appropriately here. In the Runbook it will look flattened i.e. instead of multiple lines, the context will be contiguous and will be presented in a single long line. Its data type is Object when coming from ASR Recovery Plan. </p>
<pre><code>{
   "RecoveryPlanName":"RecoveryPlan-Test",
   "FailoverType":"Test",
   "FailoverDirection":"PrimaryToSecondary",
   "GroupId":"Group1",
   "VmMap":{
      "aaaaaa-1111-1111-1111-aaaaaaaaaaa":{
         "SubscriptionId":"bbbbbbbb-2222-2222-2222-bbbbbbbbbbb",
         "ResourceGroupName":"rg-dr-test-asr",
         "CloudServiceName":null,
         "RoleName":"VMName-test",
         "RecoveryPointId":"cccccccccc-3333-3333-3333-cccccccccccc",
         "RecoveryPointTime":"\/Date(1550773609103)\/"
      }
   }
}</code></pre>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Azure%20Automation%20-%20ASR%20Recovery%20Plan" target="_blank">ASR-RunbookForRecoveryPlans.ps1</a></p>
<h3>Runbook for Parsing the Recovery Plan Context</h3>
<p>The Runbook for parsing the Recovery Plan Context is discussed here: <a href="http://harvestingclouds.com/post/script-sample-azure-automation-get-vm-information-from-asr-recovery-plan-context/" target="_blank">Script Sample - Get VM's Information from ASR Context</a>. The Runbook/script in that link is the one that is being invoked by the Runbook discussed in this blog post. </p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-azure-automation-runbook-for-asr-recovery-plan</link>
<pubDate>Wed, 05 Sep 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Updating a Custom RBAC Role in Azure</title>
<description><![CDATA[<p>In one of the earlier blog, we saw how to add a custom role in Azure to manage Role Based Access Control (RBAC) at more granular level. You can review the earlier post here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-custom-rbac-roles/" target="_blank">Demystifying Azure Security - Custom RBAC Roles</a>.</p>
<p>In this post, we will see how to make quick changes to this custom role leveraging Azure PowerShell script.</p>
<h3>Making the updates</h3>
<p>Just like any other Azure PowerShell script, you will connect to Azure and select the right subscription by using the following cmdlets.</p>
<pre><code>Add-AzureRmAccount
Select-AzureRmSubscription -SubscriptionName $SubscriptionName</code></pre>
<p>Then you will fetch the current custom role by using the below cmdlet.</p>
<pre><code>$role = Get-AzureRmRoleDefinition $roleName</code></pre>
<p>Optionally, if you want to <strong>inspect the current role</strong>, then you can use below cmdlet to generate JSON file for the current custom role.</p>
<pre><code>Get-AzureRMRoleDefinition -Name $roleName | ConvertTo-Json</code></pre>
<p>Then you can make updates to the &quot;$role&quot; object. One such sample could be as outlined below which adds the action to allow stopping of the VMs and then updates the description of the custom role.</p>
<pre><code>$role.Actions.Add("Microsoft.Compute/virtualMachines/deallocate/action")
$role.Description = "Can monitor, Start, Stop and restart virtual machines."</code></pre>
<p>Finally, the role is updated back to the Azure portal by using the below cmdlet.</p>
<pre><code>Set-AzureRmRoleDefinition -Role $role</code></pre>
<h3>Location of the Complete Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Update-CustomRoleSample" target="_blank">Update-CustomRoleSample.ps1</a></p>]]></description>
<link>http://HarvestingClouds.com/post/updating-a-custom-rbac-role-in-azure</link>
<pubDate>Wed, 29 Aug 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Set Azure Site Recovery (ASR) Prerequisite Services</title>
<description><![CDATA[<p>When you want to use Azure Site Recovery (ASR) either for Migration or for Disaster Recovery (DR), you will need to enable replication. Under the hood, this involves installation of the Mobility Services agent. This has few pre-requisites for some services to be enabled and set up appropriately. </p>
<p>In the last script sample, we saw how to check remote computers if these services are set correctly or not. That sample can be found here: <a href="http://harvestingclouds.com/post/script-sample-check-azure-site-recovery-asr-prerequisite-services/" target="_blank">Script Sample - Check Azure Site Recovery (ASR) Pre-requisites</a>.</p>
<p>In this sample, we will see how to set these services correctly on the remote computers.</p>
<h3>How it works</h3>
<p>This script will query the remote computers and set the service status for the 3 ASR Required services. These 3 services are:</p>
<ol>
<li>Volume Shadow Copy(VSS) </li>
<li>COM+ System Application(COMSysApp)</li>
<li>Microsoft Distributed Transaction Coordinator Service (MSDTC)</li>
</ol>
<p>The script will set the startup type of the services to &quot;<strong><em>automatic</em></strong>&quot; and will then &quot;<strong><em>start</em></strong>&quot; the service. It uses the below cmdlets to perform these actions.</p>
<pre><code>Set-Service -name 'msdtc' -ComputerName $Computer -StartupType Automatic
Start-Sleep -s 5
Get-Service -Name 'msdtc' -ComputerName $computer| Start-Service</code></pre>
<p>The cmdlets to configure other two services is similar. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/ASR%20Pre-Requisites" target="_blank">Set-ASRPrerequisiteServivces.ps1 on GitHub</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-set-azure-site-recovery-asr-prerequisite-services</link>
<pubDate>Thu, 23 Aug 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Check Azure Site Recovery (ASR) Prerequisite Services</title>
<description><![CDATA[<p>When you want to use Azure Site Recovery (ASR) either for Migration or for Disaster Recovery (DR), you will need to enable replication. Under the hood, this involves installation of the Mobility Services agent. This has few pre-requisites for some services to be enabled and set up appropriately. </p>
<p>If you have a large environment then you can do this in an automated fashion by leveraging this script sample. You can learn more about these services related requirements by checking the &quot;<strong><em>Checking Services</em></strong>&quot; section in here: <a href="http://harvestingclouds.com/post/troubleshooting-azure-site-recovery-asr-data-replication-initiation-issues-part-2/" target="_blank">Troubleshooting Azure Site Recovery (ASR) - Data Replication Initiation Issues - Part 2</a></p>
<h3>How it works</h3>
<p>This script will query the remote computers and check for service status for the 3 ASR Required services, and export to CSV. These 3 services are:</p>
<ol>
<li>Volume Shadow Copy(VSS) </li>
<li>COM+ System Application(COMSysApp)</li>
<li>Microsoft Distributed Transaction Coordinator Service (MSDTC)</li>
</ol>
<p>The script uses WMI to fetch the data from the remote computer(s) and checks if these services are in the &quot;<strong><em>running</em></strong>&quot; state and whether or not the startup type is set to &quot;<strong><em>automatic</em></strong>&quot;. If both conditions are not met then the state is reported as &quot;<em>Set Incorrectly</em>&quot;. If both conditions are met then the result is reported as &quot;<em>Set Correctly</em>&quot;</p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/ASR%20Pre-Requisites" target="_blank">Check-ASRPrerequisiteServices.ps1 on GitHub</a></p>
<h3>Next Script  - Setting these services automatically</h3>
<p>Next, we will check the script sample to set these services appropriately. This sample can be found here: <a href="http://harvestingclouds.com/post/script-sample-set-azure-site-recovery-asr-prerequisite-services/" target="_blank">Script Sample - Set Azure Site Recovery (ASR) Pre-requisite Services</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-check-azure-site-recovery-asr-prerequisite-services</link>
<pubDate>Mon, 20 Aug 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Format EA Billing Usage Csv for Tags</title>
<description><![CDATA[<p>As a system administrator, you will need to work with Enterprise Agreement (EA) related billing data a lot. As such you will need to pull lots of reports based on this consumption and usage related csv file that you get from the EA portal i.e. at address: <a href="https://ea.azure.com" target="_blank"><a href="https://ea.azure.com">https://ea.azure.com</a></a>. Tagging is one of the important components. But it comes as a composite JSON in one column. You will want to split this into multiple columns, with one column for each tag. </p>
<p>The script sample in this blog performs exactly this parsing for you. You will need to tweak the script to include the tags as per your environment. </p>
<blockquote>
<p><strong>Note</strong>: The EA portal is only available to the EA customer. If you are not an EA customer, then if you try to login, you will get the following error:Invalid User: The account provided is not a valid user of the Microsoft Azure Enterprise Portal. Please sign in with a valid account. If you believe you have received this message in error, please contact Support.</p>
</blockquote>
<h3>Sample Input EA Azure Billing CSV</h3>
<p>The tags section in the CSV will look similar to below <strong>JSON format key-value pairs</strong> under the column for &quot;<strong>Tags</strong>&quot;:</p>
<pre><code>{  "ApplicationOwner": "aman@domain.com",  "BusinessUnit": "Infrastructure",  "ApplicationType": "Test",  "Department": "Infrastructure"}</code></pre>
<p>Now as a requirement, you will want to split this to separate columns for the Tags for &quot;Application Owner&quot;, &quot;Business Unit&quot;, &quot;ApplicationType&quot; and &quot;Department&quot;. </p>
<h3>How does the script parse this JSON</h3>
<p>The script parses the JSON by splitting it into multiple values and then saving the new object as an output CSV. Right now these tag values are hard coded into the script and needs to be tweaked as per your requirements. Look out for the names of the variables and output member names with same name as that of the Tags mentioned above. These are the ones that you will need to alter. </p>
<p>The script also takes into factor if the tag was added incorrectly with space in the name instead of pascal casing (i.e. name with no space between two words and first letter capital for each word).</p>
<h3>Location of the Script</h3>
<p>The script is located in the GitHub along with samples for Input and Output CSV files at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Format-EABillingUsageCsvForTags" target="_blank">GitHub location for Format Billing CSV Script</a></p>
<blockquote>
<p><strong>Note</strong>: An alternative to using the billing related csv file is to leverage <strong>PowerBI</strong> and build dashboards by consuming the EA related billing APIs which are available out of the box. These are in preview at the time of the writing of this blog. Parsing tags is still an issue with these. We will cover these in one of the future blog post. </p>
</blockquote>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-format-ea-billing-usage-csv-for-tags</link>
<pubDate>Wed, 08 Aug 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Export All OMS Log Analytics Saved Searches</title>
<description><![CDATA[<p>If you work with OMS Log Analytics then you end up working with lots of queries. To manage the most used queries specific to your environment you save these as Saved Searches. These can be reusable in other projects as well. Therefore, you will want to export these saved searches. </p>
<h3>How it works</h3>
<p>This is the most simple but very useful script sample. It exports by simply using the below cmdlet. You can use the below cmdlet alone as well instead of using the whole script, if you are already logged into Azure and have the right subscription selected.</p>
<pre><code>(Get-AzureRmOperationalInsightsSavedSearch -ResourceGroupName $ResourceGroupName -WorkspaceName $WorkspaceName).Value.Properties | ConvertTo-Json</code></pre>
<p>Note that it uses &quot;<strong><em>ConvertTo-Json</em></strong>&quot; cmdlet to export the Saved Searches to JSON output. You can later reuse this JSON to import these saved searches into a different environment as well. </p>
<h3>Location of the Script</h3>
<p>You can find this script in GitHub at this location: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Export-OMSLogAnalyticsSavedSearches" target="_blank">Export Log Analytics Saved Searches</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-export-all-oms-log-analytics-saved-searches</link>
<pubDate>Wed, 25 Jul 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Export All Azure Automation Account Runbooks and Variables</title>
<description><![CDATA[<p>When you work with Azure Automation, you build lots of Runbooks over time. When the volume grows, you can't just export each and every Runbook one by one. Add to it the fact, if you have multiple Azure Automation accounts. It becomes a nightmare to download all the Runbooks one by one. </p>
<p>There are two ways to export all the Runbooks all at once. </p>
<h3>First method - Using Azure Automation PowerShell ISE Add-On</h3>
<p>You can install the Azure Automation PowerShell ISE Add-on and connect to your Azure Automation Account. Then you can download all the Runbooks locally and copy the same. This Add-On can be downloaded from GitHub or PowerShell gallery as per the instructions provided here: <a href="https://github.com/azureautomation/azure-automation-ise-addon" target="_blank">Azure Automation PowerShell ISE Add-On</a></p>
<h3>Second method - Using this script sample</h3>
<p>This script sample requires you to update the input variables for your environment i.e. related to your subscription, Azure Automation account name etc. Then it connects to the Azure using below cmdlet.</p>
<pre><code>Add-AzureRmAccount
Select-AzureRmSubscription -SubscriptionName $SubscriptionName</code></pre>
<p>Then it exports the Azure Automation Runbooks using below cmdlets. Here it first fetches all the Runbooks and then uses &quot;<em>Export-AzureRmAutomationRunbook</em>&quot; cmdlet to export these to a local folder.</p>
<pre><code>$AllRunbooks = Get-AzureRmAutomationRunbook -AutomationAccountName $AutomationAccountName -ResourceGroupName $ResourceGroupName
$AllRunbooks | Export-AzureRmAutomationRunbook -OutputFolder $OutputFolder</code></pre>
<p>Finally, it exports the variables in the environment by using the below cmdlets. Here it first fetches the variables and then uses &quot;Export-Csv&quot; cmdlet to export the variables to a csv file. </p>
<pre><code>$variables = Get-AzureRmAutomationVariable -AutomationAccountName $AutomationAccountName -ResourceGroupName $ResourceGroupName
$variablesFilePath = $OutputFolder + "\variables.csv"
$variables | Export-Csv -Path $variablesFilePath -NoTypeInformation</code></pre>
<h3>Location of the script</h3>
<p>This script is located in the Github here: <a href="https://github.com/HarvestingClouds/PowerShellSamples/tree/master/Scripts/Export-AllAutomationRunbooksAndVariables" target="_blank">Export Azure Automation Runbooks and Variables</a></p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-export-all-azure-automation-account-runbooks-and-variables</link>
<pubDate>Mon, 16 Jul 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Book published on Amazon - Quick and Practical Guide to ARM Templates</title>
<description><![CDATA[<p><strong>Quick and Practical Guide to ARM Templates</strong> is a very non-conventional book. The objective of the book is to help you &quot;Become Experts in Developing ARM Templates for Microsoft Azure without any prior knowledge&quot;. This book is for both a beginner and intermediate users. </p>
<p><strong>Update [03-27-2018]</strong>: This book is available for <strong>FREE</strong> for a limited time only (on 27, 28 and 29th March, 2018 till midnight Pacific Time). <a href="https://www.amazon.com/dp/B07C8LSBSN" target="_blank"><strong>Download Now!!!</strong></a></p>
<p>This is a quick and practical approach to learning ARM Templates for Azure. It covers only the most essential topics that you will need 95% of the time while working with ARM Templates. The goal is to have you working faster and quicker on ARM Templates without compromising any of the best practices. </p>
<p>The book assumes that you do not have prior knowledge of ARM Templates. If you have no development background then this book is for you. It starts by building the fundamentals on which ARM Templates are built.  The more practical approach means less theory and more focus on the practical aspects that can help you start working and delivering on ARM Templates.</p>
<p>You can view the book on Amazon.com here: <a href="https://www.amazon.com/dp/B07C8LSBSN">https://www.amazon.com/dp/B07C8LSBSN</a></p>
<p>You can also search for the book in your local Amazon marketplace.</p>
<img src="/images/15217860175ab49ca149064.png" alt="Quick and Practical Guide to ARM Templates">]]></description>
<link>http://HarvestingClouds.com/post/book-published-on-amazon-quick-and-practical-guide-to-arm-templates</link>
<pubDate>Tue, 27 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Getting Started with Virtual Machine Serial Console access</title>
<description><![CDATA[<p>This is the feature we all have been waiting for. This gives power back to administrators in the cloud. <strong>Virtual Machine Console Access</strong> is now available in Preview in Azure. The console is called <strong>Special Administrative Console</strong> (SAC). With this you can perform advanced troubleshooting like correcting firewall rules by fixing iptables, recovering filesystems, locking down the network, interacting and modifying the bootloader etc. </p>
<p><strong>Note</strong>: Windows images on Azure do not have <strong>Special Administrative Console</strong> (SAC) enabled by default. SAC is supported on server versions of Windows but is not available on client versions (e.g. Windows 10, Windows 8 or Windows 7).</p>
<h3>How to Enable SAC on Windows VM</h3>
<p>On a Linux VM, the serial access will be available but for Windows VM you need to perform additional steps in the VM to enable this. The steps to enable are:</p>
<ul>
<li>Login to your VM via Remote Desktop</li>
<li>
<p>From an Administrative command prompt run the following commands </p>
<p><code>bcdedit /ems {current} on</code></p>
<p><code>bcdedit /emssettings EMSPORT:1 EMSBAUDRATE:115200</code></p>
</li>
</ul>
<ul>
<li>Reboot the system for the SAC console to be enabled</li>
</ul>
<h3>How to Access</h3>
<p>Currently, this option (in Preview at the time of writing this blog) is available only via the portal. To access the Serial Console go to:</p>
<ol>
<li>The Virtual Machine (VM) for which you want to access the Serial Console</li>
<li>Ensure that the VM is up and running</li>
<li>Navigate to the  &quot;<strong>Serial console (Preview)</strong>&quot; option under &quot;Support + Troubleshooting&quot; section in VM settings</li>
</ol>
<p>If the VM is not up and running then you will view the below error.</p>
<pre><code>The serial console connection to the VM encountered an error: 'Bad Request'
 (400) - The VM is in a stopped deallocated state.  Please start the VM and retry the serial console connection.</code></pre>
<p>When the VM is starting you will see the below message, which you are connected to the Serial Console.</p>
<img src="/images/15222511335abbb57d69110.png" alt="Connecting to SAC">
<p>Once connected you can run commands like <code>ch -?</code> for information on using channels and simply type <code>?</code> for general help.</p>
<img src="/images/15222511405abbb5841a716.png" alt="Connected SAC and Help options">
<p><strong>Note</strong>: The console automatically will disconnect after a period of inactivity.</p>
<p>The access is being opened to more and more subscriptions and regions. If you don't see the access today, you just need to wait and it should be coming soon.</p>]]></description>
<link>http://HarvestingClouds.com/post/getting-started-with-virtual-machine-serial-console-access</link>
<pubDate>Mon, 26 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Series Index</title>
<description><![CDATA[<p>Understanding the security is of utmost importance in designing any application architecture. When bringing your applications or infrastructure to Azure or even designing new applications in Azure, you need to be aware of all the ways you can make your application/design more secure by leveraging various features Azure has to offer.</p>
<p>This series talks about various aspects of <strong>security</strong> as related to different aspects of Azure.</p>
<p>This blog is an <strong>Index</strong> of various blogs in the series &quot;Demystifying Azure Security&quot;:</p>
<ol>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-rbac-roles/" target="_blank">Understanding RBAC Roles</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-custom-rbac-roles/" target="_blank">Custom RBAC Roles</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-just-in-time-vm-access/" target="_blank">Just In Time VM access</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-policies-1-basics/" target="_blank">Azure Policies - Basics</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-policies-2-assigning-a-policy/" target="_blank">Azure Policies - Assigning a Policy</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-creating-a-custom-policy-part-1-viewing-definition-of-an-existing-policy/" target="_blank">Creating a Custom Policy - Part 1 - Viewing Definition of an existing Policy</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-creating-a-custom-policy-part-2-understanding-the-policy-structure/" target="_blank">Creating a Custom Policy - Part 2 - Understanding the Policy Structure</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-creating-a-custom-policy-part-3-defining-your-custom-policy/" target="_blank">Creating a Custom Policy - Part 3 - Defining your Custom Policy</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-policies-initiative-definitions/" target="_blank">Azure Policies - Initiative Definitions</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-transparent-data-encryption/" target="_blank">Azure SQL Database - Transparent Data Encryption</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-auditing-threat-detection/" target="_blank">Azure SQL Database - Auditing &amp; Threat Detection</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-set-server-firewall/" target="_blank">Azure SQL Database - Set Server Firewall</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-firewall-rule-for-virtual-networks/" target="_blank">Azure SQL Database - Firewall Rule for Virtual Networks</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-and-azure-storage-service-endpoints-on-virtual-network/" target="_blank">Azure SQL Database and Azure Storage - Service Endpoints on Virtual Network</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-dynamic-data-masking/" target="_blank">Azure SQL Database - Dynamic Data Masking</a></li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-series-index</link>
<pubDate>Tue, 20 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Site Recovery (ASR) - Series Index</title>
<description><![CDATA[<p>Outages can happen anytime. There can be different reasons that you may encounter disruptions in the service. It is always good to be prepared. Azure Site Recovery (ASR) ensure business continuity by providing you with a backup plan in case of an outage or a disaster level event. </p>
<p>This series talks about various aspects of working with <strong>Azure Site Recovery</strong> or <strong>ASR</strong>. </p>
<p>This blog is an <strong>Index</strong> of various blogs in the series:</p>
<ul>
<li><a href="http://harvestingclouds.com/post/troubleshooting-azure-site-recovery-asr-data-replication-not-working/" target="_blank">Troubleshooting Azure Site Recovery (ASR) - Data Replication Not Working</a></li>
<li><a href="http://harvestingclouds.com/post/troubleshooting-azure-site-recovery-asr-data-replication-initiation-issues-part-2/" target="_blank">Troubleshooting Azure Site Recovery (ASR) - Data Replication Initiation Issues</a></li>
<li><a href="http://harvestingclouds.com/post/azure-site-recovery-asr-new-feature-added-to-target-resource-groups/" target="_blank">Azure Site Recovery (ASR) - New feature added to target Resource Groups</a></li>
<li><a href="http://harvestingclouds.com/post/suspending-and-resuming-azure-site-recovery-asr-replication-on-a-single-or-multiple-servers/" target="_blank">Suspending and Resuming Azure Site Recovery (ASR) Replication on a single or multiple servers</a></li>
<li><a href="http://harvestingclouds.com/post/asr-setup-for-vms-running-in-vmware-without-vmware-level-user-access/" target="_blank">ASR Setup for VMs running in VMWare without VMware level User Access</a></li>
<li><a href="http://harvestingclouds.com/post/getting-started-azure-site-recovery-asr-in-new-azure-portal/" target="_blank">Getting Started - Azure Site Recovery (ASR) In New Azure Portal</a></li>
</ul>]]></description>
<link>http://HarvestingClouds.com/post/azure-site-recovery-asr-series-index</link>
<pubDate>Mon, 19 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure SQL Database - Dynamic Data Masking</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p><strong>Dynamic Data Masking</strong> is a feature of Azure SQL Databases, that allows you to hide the sensitive data. E.g. your database contains information regarding the Credit Cards of your customers. When exposing the database you want to ensure that the credit cards are not exposed. They should automatically be presented in the format &quot;xxxx-xxxx-xxxx-1234&quot; i.e. only exposing the last 4 digits. </p>
<h3>Feature Basics</h3>
<p>This feature can be accessed by navigating to your database and then clicking on the &quot;<strong>Dynamic Data Masking</strong>&quot; option under settings. By default, there are no masks applied. Click on &quot;<strong>+ Add mask</strong>&quot; to add a new mask.</p>
<p><strong>Note</strong> that whatever masks you apply are not applied to the administrators. Additionally, you can provide the SQL users who should be excluded from masking. </p>
<p>Azure SQL Database will also automatically try to recommend the fields that should be masked.</p>
<img src="/images/15224833815abf40b536b22.png" alt="Navigating to the Dynamic Data Masking Option">
<h3>Adding Masking Rules</h3>
<p>When adding Masking Rules you provide below information:</p>
<ol>
<li>Name for the mask is auto-populated (based on your selections)</li>
<li>Schema</li>
<li>Table in that schema</li>
<li>Column in the table, where mask should be applied</li>
<li>Masking Criteria</li>
</ol>
<p><strong>Note</strong> that the Masking criteria vary based on the type of the column. E.g. If a column does not have numerical value then the masking criteria for &quot;Number (random number range)&quot; will show as disabled.</p>
<img src="/images/15224833865abf40bad27e8.png" alt="Adding Masking Rule">
<p>The different masking criteria that you can apply are:</p>
<ol>
<li>Default value  (0, xxxx, 01-01-1900)</li>
<li>Credit Card value (xxxx-xxxx-xxxx-1234)</li>
<li>Email (aXXX@XXXX.com)</li>
<li>Number (random number range)</li>
<li>Custom string (prefix [padding] suffix)</li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-sql-database-dynamic-data-masking</link>
<pubDate>Sun, 11 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure SQL Database and Azure Storage - Service Endpoints on Virtual Network</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>Azure <strong>Service Endpoints</strong> allow access to SQL or Storage services over the network, without going out of the network. </p>
<p>To configure this feature, you can navigate to your Virtual Network and then under the settings, select the &quot;<strong>Service endpoints</strong>&quot;. Click on &quot;+Add&quot; to add a Service Endpoint.</p>
<img src="/images/15224831715abf3fe37c1c1.png" alt="Navigating to Service Endpoints">
<p>In the popup, select the provider for which you want to configure the Service Endpoint. </p>
<p>Service Endpoints on the Virtual Networks are available for:</p>
<ol>
<li>Microsoft.Sql provider</li>
<li>Microsoft.Storage provider</li>
</ol>
<p>Also, select the subnet on which you want to configure the Service Endpoint and then hit &quot;<strong>Add</strong>&quot;.</p>
<img src="/images/15224831785abf3fea2a4a8.png" alt="Adding Service Endpoint">
<p>It will take some time (approximately 15 minutes) to configure the Service Endpoints at the backend. Once configured, you will see the configured endpoints in the portal as shown below.</p>
<img src="/images/15224831835abf3fef3682d.png" alt="Deployed Service Endpoint">
<p><strong>Note</strong> that even after you configure service endpoint for SQL you will need to allow access at the SQL Server level as well. Service Endpoint ensures that the communication will happen at the network level. The Firewall configuration for the network is needed to allow that communication via Firewall on the SQL Server. This is explained in detail here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-firewall-rule-for-virtual-networks/" target="_blank">Azure SQL Database - Firewall Rule for Virtual Networks</a></p>
<p>Overall, this is a very powerful feature that is easy to configure and provides you with lots of flexibility. </p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-sql-database-and-azure-storage-service-endpoints-on-virtual-network</link>
<pubDate>Sat, 10 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure SQL Database - Firewall Rule for Virtual Networks</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>Before going through this blog, please make sure you have covered these:</p>
<ul>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-set-server-firewall/" target="_blank">Set Server Firewall for Azure SQL Databases</a></li>
<li><a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-and-azure-storage-service-endpoints-on-virtual-network/" target="_blank">Service Endpoints for Azure SQL on Virtual Network</a>. <strong>Note</strong> that this is pre-requisite for the firewall rules to be configured on the Virtual Network level.</li>
</ul>
<p>Setting up the <strong>Firewall Rule for Virtual Networks</strong>, at the SQL Server level, enables you to allow access to a subnet in a Virtual Network in Azure on all the SQL Databases on the SQL Server. The firewall rules are always set at the server level, hence any rule you put will allow access on all the databases on the SQL Server.</p>
<p>Setting up the Rules is very simple. You navigate to the firewall settings for the SQL Database/SQL Server (as discussed in the previous blog). Then you focus on the lower section on the blade for Virtual Network Firewall rules as shown below.</p>
<img src="/images/15225265255abfe93da6c61.png" alt="Virtual Network Firewall Rules section">
<p>Note that you can:</p>
<ol>
<li>Add existing virtual network</li>
<li>Create a new virtual network (and provide access)</li>
</ol>
<p>As a best practice, you should plan the virtual network and subnets before the configurations on the SQL Server Firewall. </p>
<p>When you click on &quot;Add existing virtual network&quot; you are presented with the below wizard. Here you select:</p>
<ol>
<li>The name for the rule. This could be any descriptive name for the rule.</li>
<li>The Subscription where the virtual network exists</li>
<li>Virtual network where you want to allow the access</li>
<li>Subnet name within the Virtual network where you want to allow the access</li>
</ol>
<img src="/images/15224829335abf3ef529038.png" alt="Create/Update Virtual Network Firewall Rule">
<p>Below, is the screenshot of the rule with values populated. If the Service Endpoint is not enabled for the &quot;Microsoft.Sql&quot; provider then you will view a message for the same and the wizard will attempt to enable the same.</p>
<img src="/images/15224829395abf3efbae000.png" alt="Virtual Network Firewall Rule Populated">
<p>Thats all there is to it. Just hit Ok and then hit Save to apply the rule.</p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-sql-database-firewall-rule-for-virtual-networks</link>
<pubDate>Sun, 04 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure SQL Database - Set Server Firewall</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>Azure SQL Databases have a powerful layer of security at the SQL Server level. This layer is provided by the SQL Server <strong>Firewall</strong>. Azure provides you granular control to configure this firewall and to manage who gets access to your Azure SQL Database. By default, everything is blocked by the firewall. If you want to get access to Azure SQL Database then you will have to configure the Firewall at the SQL Server level. Only the IP addresses you configure have access to the SQL Databases on the Server.</p>
<p>Another key point to understand is that once you configure a rule then because that rule is applied at the server level, it is applied to all the SQL Databases on that Server. So it is important to ensure that you segregate your databases on different SQL Servers if you don't want to share the access to those databases.</p>
<h3>Accessing the Firewall Settings</h3>
<p>You can access the firewall settings by navigating to your Azure SQL <strong>Database</strong>. Then at the top of the blade, you will find the option for &quot;<strong>Set server firewall</strong>&quot;. Click on this button to access the firewall settings.</p>
<img src="/images/15224826395abf3dcf84ca9.png" alt="Set Server Firewall option on Azure SQL Database">
<p>Another way to access the settings is on Azure SQL <strong>Servers</strong>. Navigate to the related Azure SQL Server for your database. Under the settings, find the option for &quot;<strong>Firewalls and virtual networks</strong>&quot;. Clicking on this will also take you to the same firewall settings as the settings are set at the server level in both ways.</p>
<img src="/images/15224826455abf3dd50a424.png" alt="Firewall option on Azure SQL Server">
<h3>Firewall Rule Settings</h3>
<p>Let us look at the firewall rule settings in more details. </p>
<ol>
<li>First, you have the option to enable or disable the <strong>access to Azure services</strong> as the toggle for &quot;Allow access to Azure services&quot;. This is set to On by default when creating the Database and SQL server. You can disable it here. This option allows Azure services to provide monitoring data and recommend changes at the database level.</li>
<li>Second, you have the option to <strong>configure a Rule</strong>. This is where you configure which IP Addresses have access to the SQL Server.</li>
<li>One commonly used Rule is to open the access for <strong>Client IP</strong>. This is the IP address of the machine from where you are connected to the Azure Portal. This is provided for the common scenario where you want to connect and access the SQL database via SSMS (i.e. SQL Server Management Studio) or by any other way from your development box. There is a button to simply add the rule for allowing Client IP by clicking on &quot;<strong>+Add client IP</strong>&quot;. The client IP is also authomatically displayed under the &quot;Allow access to Azure services&quot; section</li>
<li>Lastly, there is an option to allow access to the SQL databases from a particular Subnet in a Virtual Network in Azure, without having to manually configure their IP addresses. This feature requires <strong>Service Endpoints</strong> to be configured at the Virtual Network level. We will discuss this feature in a later blog. </li>
</ol>
<img src="/images/15224826505abf3ddaaf183.png" alt="Firewall Settings">
<p>A typical Rule contains:</p>
<ol>
<li>A <strong>Rule Name</strong> - which could be any descriptive name for the rule. E.g. it could be name of the single VM for which you want to configure the access or it could be name of the network for which you are opening the access</li>
<li><strong>Start IP</strong> - Start of the IP address range for which you want to open the access. E.g. if you want to open access for 10.20.1.0/24 then the Start IP will be 10.20.1.0</li>
<li><strong>End IP</strong> - End of the IP address range for which you want to open the access. E.g. if you want to open access for 10.20.1.0/24 then the End IP will be 10.20.1.255</li>
</ol>
<p>Note: If you want to provide access to only one IP address then provide that IP address for both Start and End IP fields.</p>
<p>Note in the image below that:</p>
<ol>
<li>The client IP rule was added by clicking on the &quot;+Client IP&quot; button. Note that the Start and End IP are same for this rule</li>
<li>Second is a custom rule which allows access to the SQL Server from a specific VM hosting Website (which will need access to the database)</li>
</ol>
<img src="/images/15224826565abf3de09ea3f.png" alt="Setting Firewall Rules">
<p>Once you are done configuring, just hit Save to apply the firewall rules.</p>
<p>Next, we will learn about setting up the <a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-firewall-rule-for-virtual-networks/" target="_blank">Firewall Rule for Virtual Networks</a> and also <a href="http://harvestingclouds.com/post/demystifying-azure-security-azure-sql-database-and-azure-storage-service-endpoints-on-virtual-network/" target="_blank">Service Endpoints for Azure SQL on Virtual Network</a></p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-sql-database-set-server-firewall</link>
<pubDate>Fri, 02 Mar 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Resources - New Azure Log Analytics Language Cheat Sheets</title>
<description><![CDATA[<p>Azure Log Analytics (part of the OMS suite) has a very versatile query language. To investigate and report on the data you need to know the query language at least at the basic level. Recently the language had a complete overhaul with new syntax coming in and various new features being incorporated into the new language. This blog post talks about the resources to quickly learn the new syntax. Specifically, if you know the older syntax or you know T-SQL syntax then how to translate that knowledge. </p>
<h3>Older to new Query Language syntax</h3>
<p>If you have been working with the older Log Analytics query syntax, then you have two options to convert that knowledge to newer query language syntax:</p>
<ol>
<li>Use the in portal legacy syntax converter and learn as you convert</li>
<li>Use the Microsoft provided Cheat Sheet</li>
</ol>
<p>When you navigate to OMS log analytics portal and go to the Log search section, there you will see a link above the query text window for &quot;<strong>Show legacy language converter</strong>&quot;. When you click on this link a new text box will appear above the query text box. Type your legacy query and then click on &quot;<strong>Convert</strong>&quot; button. The query will be converted into the new language syntax. Click Run to execute the query. If there will be any errors in the query you will be notified of the same. </p>
<img src="/images/15212342425aac314289ed9.png" alt="OMS Language Converter">
<p>In the above example, &quot;Event&quot; type is being fetched and then only Source, EventLog, EventID properties are selected. In the older format the query syntax used to be:</p>
<pre><code>Type=Event | select Source, EventLog, EventID</code></pre>
<p>In the newer format the same query looks as below:</p>
<pre><code>Event | project Source, EventLog, EventID</code></pre>
<p>Pointers for key query syntax can be found in the complete cheat sheet which can be found here: <a href="https://docs.loganalytics.io/docs/Learn/References/Legacy-to-new-to-Azure-Log-Analytics-Language" target="_blank">Legacy to new Azure Log Analytics Query Language cheat sheet</a></p>
<h3>T-SQL to new Query Language syntax</h3>
<p>If you are well versed in the T-SQL query syntax and are new to OMS Azure Log Analytics, then you can easily translate that to the Log Analytics query language with the help of the cheat sheet provided by Microsoft for the key syntax.</p>
<p>E.g. if we want to select records for only columns name and resultCode from a table named dependencies then the query syntax in T-SQL will look like:</p>
<pre><code>SELECT name, resultCode FROM dependencies</code></pre>
<p>Syntax for the same query in newer Log Analytics language will look like:</p>
<pre><code>dependencies 
| project name, resultCode</code></pre>
<p>As you might have guessed already, &quot;project&quot; is a key word in newer language to select specific columns. Selecting a table is as simple as typing the name of the table.</p>
<p>The complete cheat sheet can be found here: <a href="https://docs.loganalytics.io/docs/Learn/References/SQL-to-Azure-Log-Analytics" target="_blank">SQL to Azure Log Analytics query language cheat sheet</a></p>]]></description>
<link>http://HarvestingClouds.com/post/resources-new-azure-log-analytics-language-cheat-sheets</link>
<pubDate>Mon, 19 Feb 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure SQL Database - Transparent Data Encryption</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p><strong>Transparent Data Encryption</strong> (<strong>TDE</strong>) is the automated encryption of your data at rest. If configured it encrypts your database, backups of the database and transactional log files at rest. Normally this is configured by default to provide you with an additional layer of security. If this is not configured then you will get a recommendation to configure it in the Azure Security Center. </p>
<p>Turing Off Transparent data encryption will result in decryption of the complete database and will leave your data vulnerable. When you turn it back On then the database will be encrypted again. Depending upon the size of your database, it may take some time to turn the TDE on or off due to the underlying encryption/decryption process.</p>
<p>This service does not require any changes at the application level. Behind the scene, transparent data encryption performs real-time I/O encryption and decryption of the data at the page level. Each page is decrypted when it's read into memory and then encrypted before being written to disk.</p>
<p><strong>Note</strong>: Even if the database is encrypted with TDE, when you take an export of the database (e.g. creation of BACPAK file) then the backup file is created without encryption. You need to ensure that you safeguard/encrypt the backup files before sharing these on an open network.</p>
<h3>Configuring TDE at the Database level</h3>
<p><strong>Transparent Data Encryption</strong> (<strong>TDE</strong>) can be enabled or disabled at every individual Database level. The configuration is a very simple toggle between on and off. To configure this, navigate to your Azure SQL Database. In the settings, select &quot;Transparent Data Encryption&quot;. The set the value for &quot;Data Encryption&quot; On or Off. </p>
<p>Notice the Encryption status. If you want your data to be encrypted, then the encryption status should say &quot;Encrypted&quot; with a green tick mark. </p>
<img src="/images/15221022915ab9701353492.png" alt="Configuring Transparent Data Encryption">
<h3>Configuring to use your own Key with TDE</h3>
<p>You can <strong>use your own Key for encryption</strong> with Transparent Data Encryption. If you do not configure to use your own key, then a service managed certificate is used for encryption and decryption.</p>
<p>To do this you will need to upload your key to an Azure Key Vault or generate a new key within the Key Vault, which is very easy to configure. Once you have a key in an Azure Key Vault, you will be able to use the same with Transparent Data Encryption (TDE). </p>
<p>This setting can't be configured at a Database level. Instead, this has to be configured at the server level. Navigate to the underlying Azure SQL Server (where the SQL Database is hosted). Then follow the below steps:</p>
<ol>
<li>In the settings, click on the Transparent Data Encryption</li>
<li>Select &quot;Yes&quot; to Use your own key. </li>
<li>Then click on &quot;Select a Key&quot; and then select the key from your Azure Key Vault. Alternatively, you can select to &quot;Enter Key Identifier&quot;. </li>
<li>Once the key is configured, select &quot;Save&quot; to save the settings.</li>
</ol>
<img src="/images/15221022965ab9701842974.png" alt="Using own Key for encryption with Transparent Data Encryption">
<p>This option provides you with all the security at the data level (at rest) while ensuring you have complete control over the process.</p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-sql-database-transparent-data-encryption</link>
<pubDate>Sat, 17 Feb 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure SQL Database - Auditing & Threat Detection</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p><strong>Auditing &amp; Threat Detection</strong> in Azure SQL Database is a very simple to configure yet very powerful security feature. <strong>Auditing</strong> feature audits all activity on your database to a Storage Account. You can determine the number of days for which you want to retain the data. It helps you remain compliant. In an event of any failure or compliance breach, you can go to the audit logs and can pinpoint the exact cause of the issue if this feature is enabled. </p>
<p><strong>Threat Detection</strong> is an advanced feature, where Microsoft runs various algorithms under the hood and determines the pattern and identifies any potential attacks on your data. E.g. SQL Injection or patterns like SQL Injection can be detected when this feature is enabled. Please note that the Threat Detection feature has additional cost linked to it. It costs $15/server/month. It will be free for the first 60 days. Note that you can enable Auditing without enabling Threat Detection. But you can't enable Threat Detection without enabling Auditing on the data first. </p>
<p>SQL Threat Detection integrates alerts with Azure Security Center. If any anomalous activity is detected an alert is raised, you can get notification via email and can also review the same within the portal. You get real-time actionable alerts. Each alert also contains the information regarding how to mitigate the alert.</p>
<h3>Configurations</h3>
<p>To configure Auditing and Threat Detection at the database level, navigate to the database. Then follow the below steps:</p>
<ol>
<li>In the database settings, click on &quot;Auditing and Threat Detection&quot;</li>
<li>You can optionally configure the settings at the Server level by click on the link &quot;View server settings&quot;</li>
<li>Next, toggle the &quot;Auditing&quot; setting on or off. Select the storage account and retention in the number of days. </li>
<li>Next, you can configure the &quot;Threat Detection&quot; on or off. If you toggle it on, then you have the option of selecting which type of Threats you want to detect.</li>
<li>You also have the option of configuring Email notifications which work with the Threat Detection.</li>
</ol>
<img src="/images/15220454795ab89227d4b60.png" alt="Auditing and Threat Detection Configuration">
<p>When configuring Audit Logs Storage, you can select any subscription under the tenant and a storage account in that subscription. You can then select Retention in number of Days. When this number is set to Zero then that means unlimited retention. You can select a maximum of 3285 number of days for this value. You can also select whether to use a Primary or Secondary key while accessing the Storage Account for writing the logs.</p>
<img src="/images/15220454845ab8922cbf5d7.png" alt="Audit Logs Storage Configurations">
<p>Under Threat Detection types, you can select any one or all of the following types:</p>
<ol>
<li>SQL injection</li>
<li>SQL injection vulnerability</li>
<li>Anomalous client login</li>
</ol>
<img src="/images/15220454895ab8923166b5d.png" alt="Threat Detection Types">
<h3>Enabling at Database level vs Server level</h3>
<p>If Blob Auditing or Threat Detection are enabled on the server, they will always apply to the database, regardless of the database settings.</p>
<p>At the server level, the configuration is almost identical. You need to navigate to the related Azure SQL Server first (instead of the SQL Database). Notice at the top of the below screenshot, it says &quot;SQL server&quot; instead of &quot;SQL database&quot;. Then navigate to it's &quot;Auditing and Threat Detection&quot; section and perform the configurations similar to above sections. </p>
<img src="/images/15220454945ab8923632728.png" alt="Audit and Threat Detection at the server level">]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-sql-database-auditing-threat-detection</link>
<pubDate>Sat, 10 Feb 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Custom RBAC Roles</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>Before going through this blog, please ensure that you have visited the basics of RBAC Roles in general, explained in a primer here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-rbac-roles/" target="_blank">Demystifying Azure Security - RBAC Roles</a>.</p>
<p>This blog explains an easy approach to understand and create your own Custom RBAC Roles in Azure (ARM model). We will inspect an existing simple role and will reverse engineer the way to create Custom RBAC Roles. </p>
<h3>Inspecting Existing Role's Definition</h3>
<p>Start by inspecting any existing Role's Definition. To do this run the cmdlet <strong>Get-AzureRMRoleDefinition</strong> and provide the name of any built-in RBAC Role. For this blog, run the below script to inspect the &quot;Reader&quot; and &quot;Virtual Machine Contributor&quot; roles.</p>
<pre><code>Login-AzureRMAccount

Get-AzureRMRoleDefinition -Name "Reader" | ConvertTo-Json | Out-File C:\rbacrole-reader.json

Get-AzureRMRoleDefinition -Name "Virtual Machine Contributor" | ConvertTo-Json | Out-File C:\rbacrole-VMContributor.json</code></pre>
<p>If you open and inspect the &quot;rbacrole-reader.json&quot; file you will see the JSON similar to below:</p>
<pre><code>{
    "Name":  "Reader",
    "Id":  "aaaa11a1-3333-48ef-bd42-f606fba81ae7",
    "IsCustom":  false,
    "Description":  "Lets you view everything, but not make any changes.",
    "Actions":  [
                    "*/read"
                ],
    "NotActions":  [

                   ],
    "AssignableScopes":  [
                             "/"
                         ]
}</code></pre>
<p>Notice above that there are below sections in the definition:</p>
<ol>
<li><strong>Name</strong> - Name of the role</li>
<li><strong>Id</strong> - unique guid for the role</li>
<li><strong>IsCustom</strong> - boolean value. &quot;true&quot; for the Custom Roles and &quot;false&quot; for the built-in roles</li>
<li><strong>Description</strong> - description of the role</li>
<li><strong>Actions</strong> - Allowed list of actions for the Role</li>
<li><strong>NotActions</strong> - Not Allowed list of actions for the Role</li>
<li><strong>AssignableScopes</strong> - Scope at which this role can be assigned. E.g. all the subscription Ids. It's mandatory that the RBAC role contains the explicit subscription IDs where it is used otherwise you will not be able to use the role.</li>
</ol>
<h3>Understanding Actions and NotActions</h3>
<p>As mentioned before, Actions describe the allowed list of action for the Role whereas the NotActions describe the not allowed actions for the Role. You can use wildcard * and special syntax to define the Actions and NotActions, as per the Microsoft documentation here: <a href="https://docs.microsoft.com/en-us/azure/active-directory/role-based-access-control-custom-roles" target="_blank">Create custom roles for Azure Role-Based Access Control</a>:</p>
<p>Operation strings that contain wildcards (*) grant access to all operations that match the operation string. For instance:</p>
<ul>
<li><code>*/read</code> grants access to read operations for all resource types of all Azure resource providers.</li>
<li><code>Microsoft.Compute/*</code> grants access to all operations for all resource types in the Microsoft.Compute resource provider.</li>
<li><code>Microsoft.Network/*/read</code> grants access to read operations for all resource types in the Microsoft.Network resource provider of Azure.</li>
<li><code>Microsoft.Compute/virtualMachines/*</code> grants access to all operations of virtual machines and its child resource types.</li>
<li><code>Microsoft.Web/sites/restart/Action</code> grants access to restart websites.</li>
</ul>
<p>Use <code>Get-AzureRmProviderOperation</code> (in PowerShell) or <code>azure provider operations show</code> (in Azure CLI) to list operations of Azure resource providers. You may also use these commands to verify that an operation string is valid and to expand wildcard operation strings.</p>
<pre><code>Get-AzureRMProviderOperation Microsoft.Compute/virtualMachines/*/action | FT Operation, OperationName

Get-AzureRMProviderOperation Microsoft.Network/*</code></pre>
<h3>Defining a Custom role</h3>
<p>Let's define a custom role, who can start or restart a VM in Azure but can't open a support ticket.</p>
<p>Save the following code as &quot;yourCustomRole01.json&quot; file on your C drive (or any other location).</p>
<pre><code>{
  "Name": "Virtual Machine Start and Restart",
  "Id": "7ed03a9f-b372-4341-ba8d-38ef8e614038",
  "IsCustom": true,
  "Description": "Can restart virtual machines but can't open support tickets.",
  "Actions": [
    "Microsoft.Compute/virtualMachines/start/action",
    "Microsoft.Compute/virtualMachines/restart/action"
  ],
  "NotActions": [
    "Microsoft.Support/*"
  ],
  "AssignableScopes": [
    "/subscriptions/aaaaaaaa-1111-1111-1111-111111111111",
    "/subscriptions/aaaaaaaa-2222-2222-2222-222222222222",
    "/subscriptions/aaaaaaaa-3333-3333-3333-333333333333/resourceGroups/RG-Prod-USE2"
  ]
}</code></pre>
<p>Notice the Action and NotAction area are set as per the requirements.</p>
<p>Also, note that under Assignable Scope the role is available for assignment to all Resources and Resource Groups in the first two subscriptions but only in Resource Group named &quot;RG-Prod-USE2&quot; for the third subscription in the list.</p>
<h3>Creating New Custom role</h3>
<p>Once you have the definition ready in a JSON file, you can use the &quot;<strong>New-AzureRMRoleDefinition</strong>&quot; cmdlet to create the Custom Role Definition, as shown below. Make sure to alter the path to the json file as per your environment.</p>
<pre><code>New-AzureRMRoleDefinition -InputFile "C:\yourCustomRole01.json"</code></pre>
<p>Now you will be able to use this new Custom Role while assigning access to someone. You will be able to tweak the access and provide only the access that you need to your internal and external resources.</p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-custom-rbac-roles</link>
<pubDate>Sun, 04 Feb 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure Policies - Initiative Definitions</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p><strong>Initiative Definitions</strong> are a great way to combine and apply multiple policies together. They are a group of policy definitions to achieve a singular goal. These are the Microsoft recommended way to use the Policies.  </p>
<p>E.g. You want to combine a set of policy definitions for the compute and related resources. You want to apply following set of policies:</p>
<ol>
<li>Allowed locations - to restrict locations where resources can be deployed</li>
<li>Allowed SKUs - to restrict SKUs for the VMs e.g. VMs can be created with SKU Standard_DS2_v2 only</li>
<li>Enforce Tags and it's default value - to enforce the usage of Tags on resources</li>
</ol>
<p>Instead of managing and assigning the policies separately, you can club these policies together in an <strong>Initiative Definition</strong> and then assign the same.</p>
<h3>Defining Initiative Definition</h3>
<p>To create a new Initiative Definition you go to your Subscription and then go to the Policies section under Settings. Within Policies go to the <strong>Definitions</strong> section under Authoring. Here you can click on the &quot;Initiative Definitions&quot; tab to view the existing definitions. Click on the &quot;+Initiative definition&quot; to create a new Initiative Definition.</p>
<img src="/images/15220373735ab8727dc341d.png" alt="Creating Initiative Definition">
<p>The new Initiative Definition blade with open up. Here you can create the definition. </p>
<ol>
<li>The first section, as shown below, is similar to a Policy definition. You will provide the basic information here like Definition location, Name, Description and Category. </li>
<li>On the right side, select the Policies which want to be part of this Initiative Definition in the section &quot;Available Definitions&quot;. Select and Add all the policies that you want to group together.</li>
<li>Configure parameters for the selected Policies in the &quot;Policies and Parameters&quot; section. If you selected a wrong policy, you can delete the policy in this section as well. </li>
</ol>
<img src="/images/15220373785ab87282e8f55.png" alt="Initiative Definition Blade">
<h3>Initiative Parameters</h3>
<p>Continuing with the previous section of the Policy definition, you have lots of options when configuring parameters under the &quot;Policies and Parameters&quot; section. You can either &quot;<strong>Set value</strong>&quot; right within the Initiative Definition, or you can &quot;<strong>Use Initiative Parameter</strong>&quot;. Set the values within the definition if you don't want to change during the assignment. If you want to set the values dynamically then use the Initiative Parameters.</p>
<p><strong>Initiative Parameters</strong> are used to parameterize the Initiative Definition. These can be set from the list of allowed values (a subset of all the values) during the assignment.</p>
<img src="/images/15220373835ab87287b145a.png" alt="Options for setting values for Parameters">
<p>When you select to use an Initiative Parameter for any value, then a parameter is automatically created. </p>
<img src="/images/15220373895ab8728d4f4b0.png" alt="Initiative Parameters">
<p>You can create your own Initiative parmeters as well. If an Initiative parameter is not being used then you won't be able to save that definition. You will get &quot;Bad Request&quot; while trying to save the definition.</p>
<img src="/images/15220373945ab87292397f6.png" alt="Option for creating new Initiative Parameter">
<h3>Assigning Initiative Definition</h3>
<p>The Initiative assignment is exactly same as the Policy Assignment. All the options are similar as well. You provide value to the parameters within the Initiative Definition for all the policies, during the assignment.</p>
<img src="/images/15220373995ab872973ab67.png" alt="Assigning Initiative Definition">]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-policies-initiative-definitions</link>
<pubDate>Sun, 28 Jan 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Fixing the error - The subscription is not registered to use provider</title>
<description><![CDATA[<p>This blog shows you simple step to resolve the provider registration error, that you can come across while working programmatically with Azure (in Azure Resource Manager model).</p>
<p>You can come across this error while running a PowerShell cmdlet or anywhere else: <strong>The subscription 'xxxxxx-xxxx-xxx-xxxx-xxxxxxxx' is not registered to use providername</strong>. E.g. in the below screenshot I got the error <em>The subscription '8c665920-2c37-419f-81fb-99d737cc4697' is not registered to use microsoft.insights</em>. </p>
<img src="/images/15212142425aabe322e6b05.png" alt="Provider Error">
<p>To resolve this error you will need to navigate to the Azure Portal (i.e. <a href="https://portal.azure.com">https://portal.azure.com</a> ). Authenticate to the portal with your credentials. Navigate to Subscriptions section. If you don't see this section then click on &quot;All Services&quot; and then search for Subscription. Once in the subscriptions blade, select the subscription for which you have been getting the error. If there was only one subscription then click on that. In the selected subscription's blade follow the below steps:</p>
<ol>
<li>Click on the Resource Providers</li>
<li>In the list of providers, find the one for which you have been getting error and then click on &quot;Register&quot; link in front of it</li>
<li>The provider will go into &quot;Registering&quot; state</li>
<li>Once completed, the provider will then go into &quot;Registered&quot; state</li>
</ol>
<img src="/images/15212142785aabe346ede90.png" alt="Subscription Settings">
<p>Now retry the cmdlet or API you were trying before. It will not generate the provider error now. </p>]]></description>
<link>http://HarvestingClouds.com/post/fixing-the-error-the-subscription-is-not-registered-to-use-provider</link>
<pubDate>Mon, 22 Jan 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Creating a Custom Policy - Part 3 - Defining your Custom Policy</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>As discussed earlier, the policies provide a way to control what is allowed and what is not allowed in your environment. Earlier we looked at how to view the definition of existing policies and discussed the structure of the policies in detail. Now it's time to put the knowledge together and define a policy.</p>
<h3>Getting the JSON ready for the Policy Definition</h3>
<p>Have the JSON ready for the Policy Definition. If you are going to deploy the policy via Portal then all you need is the Policy Rule portion of the definition. For this post, let's use the sample policy definition for &quot;<em>Only allow a certain VM platform image</em>&quot; located here: <a href="https://docs.microsoft.com/en-us/azure/azure-policy/scripts/allow-certain-vm-image">https://docs.microsoft.com/en-us/azure/azure-policy/scripts/allow-certain-vm-image</a></p>
<p>Various other samples are provided by Microsoft at this link: <a href="https://docs.microsoft.com/en-us/azure/azure-policy/json-samples" target="_blank">Templates for Azure Policy</a></p>
<p>This sample policy, enforces the end users to use only a certain version of the Ubuntu only. </p>
<p>The policy definition looks as below:</p>
<pre><code>{
    "type": "Microsoft.Authorization/policyDefinitions",
    "name": "platform-image-policy",
    "properties": {
        "displayName": "Only allow a certain VM platform image",
        "description": "This policy ensures that only UbuntuServer, Canonical is allowed from the image repository",
        "parameters": {},
        "policyRule": {
            "if": {
                "allOf": [
                    {
                        "field": "type",
                        "in": [
                            "Microsoft.Compute/disks",
                            "Microsoft.Compute/virtualMachines",
                            "Microsoft.Compute/VirtualMachineScaleSets"
                        ]
                    },
                    {
                        "not": {
                            "allOf": [
                                {
                                    "field": "Microsoft.Compute/imagePublisher",
                                    "in": [
                                        "Canonical"
                                    ]
                                },
                                {
                                    "field": "Microsoft.Compute/imageOffer",
                                    "in": [
                                        "UbuntuServer"
                                    ]
                                },
                                {
                                    "field": "Microsoft.Compute/imageSku",
                                    "in": [
                                        "14.04.2-LTS"
                                    ]
                                },
                                {
                                    "field": "Microsoft.Compute/imageVersion",
                                    "in": [
                                        "latest"
                                    ]
                                }
                            ]
                        }
                    }
                ]
            },
            "then": {
                "effect": "deny"
            }
        }
    }
}</code></pre>
<p>Look at the policy rule section. Let's look at various sections more closely.</p>
<ol>
<li><strong><em>if</em></strong> condition defines the policy condition</li>
<li><strong><em>allOf</em></strong> ensures that all the conditions must be true</li>
<li><strong><em>field type</em></strong> is specified as Disks, Virtual Machines and VM scale sets under Microsoft.Compute</li>
<li>Next section defines a nested condition, which evaluates true if actual VM image used is not equal to combination of all of the conditions specified. </li>
<li><strong><em>then</em></strong> section defines what should happen. In this case it says to <strong><em>deny</em></strong> the operation, i.e. the VM provisioning will fail with validation error for not conforming to the policy.</li>
</ol>
<p>Note that there are no parameters used in this policy definition.</p>
<h3>Defining Policy using Portal</h3>
<p>For defining the policies via Azure Portal, navigate to the Policies under settings of your Subscription. Click on the Definitions and then click on &quot;<strong>+Policy Definition</strong>&quot; at the top.</p>
<img src="/images/15218358115ab55f2336e11.png" alt="Defining New Policy">
<p>A new blade will open where you can define the policy. Provide the details as follows:</p>
<ol>
<li>Provide the policy definition's location. This will be your subscription in which you want the definition to exist.</li>
<li>Provide a Display Name and Description of the policy. Try to be as descriptive as possible</li>
<li>Either create a new Category for the policy or use one of the existing ones</li>
<li>Copy and paste your policy definition. In the portal, only provide the &quot;policyRule&quot; section. </li>
</ol>
<p>The section for policyRule will look something similar to below.</p>
<pre><code>{
    "policyRule": {
      all content here
    }
}</code></pre>
<img src="/images/15218358165ab55f28ad5a4.png" alt="New Policy Definition Blade">
<p>Hit <strong>Save</strong> to create the Policy Definition. You can now start assigning this policy in your environment.</p>
<h3>Defining Policy using PowerShell</h3>
<p>Ensure that you have latest version of Azure PowerShell installed. Then using PowerShell to deploy the policy is as easy as executing below two cmdlets:</p>
<ol>
<li><strong><em>New-AzureRmPolicyDefinition</em></strong> - to create the policy definition </li>
<li><strong><em>New-AzureRMPolicyAssignment</em></strong> - to use the policy and assign the policy at a scope defined</li>
</ol>
<p>Store the policy in a json file on your computer. Ensure that you only save the &quot;if-then&quot; condition in curly parenthesis. This will be used as an input to the cmdlet. The file should look similar to below:</p>
<pre><code>{
            "if": { &lt;&lt;content here&gt;&gt;},
            "then":{ &lt;&lt;content here&gt;&gt;}
}</code></pre>
<p>To create the policy definition use a code similar to below. Ensure to update the file name and path as per your environment.</p>
<pre><code>$definition = $definition = New-AzureRmPolicyDefinition -Name "RestrictingUbuntuVMVersion" -DisplayName "Restrict Ubuntu version for VM Deployment" -Description "Detailed Description here" -Policy "C:\temp\CustomPolicyDefinition.json"</code></pre>
<p>To use the policy, do the assignment using a code similar to below:</p>
<pre><code>$assignment = New-AzureRMPolicyAssignment -Name &lt;customAssignmentName&gt; -Scope &lt;SubscriptionId&gt;  -PolicyDefinition $definition</code></pre>
<p>That is all there is to defining and using your custom policy.</p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-creating-a-custom-policy-part-3-defining-your-custom-policy</link>
<pubDate>Sun, 14 Jan 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Just In Time VM access</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p><strong>Just in time VM access</strong> is a feature under <strong>Azure Security Center</strong>. In simple terms it allows you to control access to a VM. When you enable JIT, all access is locked down on the VM on all ports. This is done via Network Security Group (NSG) rules. The access is granted only for the duration allowed and also only on the ports requested. And then everything is locked down again at the end of the duration.</p>
<p>The attackers are leveraging various ways to get into your environment. One such way is using Bots to automate and Brute Force method to attempt entering in your environment. If you need to access a VM, in your environment, from Internet without VPN (e.g. to change some files on a Web app VM) then you are potentially opening up 3389 port on the VM and that can become a target for the attackers. Locking down the VM except when you need it and only for the duration of the requirement, reduces these risks significantly. </p>
<h3>Pre-requisites</h3>
<p>The key pre-requisites to be able to use this feature are:</p>
<ol>
<li>The Azure Security Center needs to be upgraded to Advanced Security as shown below</li>
<li>The VM on which you want to configure JIT access should have a Network Security Group (NSG) linked to it. If it doesn't have any then you can create one and associate it with the network interface of the VM.</li>
</ol>
<p>The first pre-requisite requires you to upgrade the Security Center to Advanced Security that comes with <strong>Standard Tier</strong>. You can upgrade from any of the advanced features. The portal will automatically prompt you to upgrade as shown below:</p>
<img src="/images/15212356695aac36d55f4be.png" alt="Advanced Security in Security Center">
<h3>How to Access and Configure it</h3>
<p>To configure this go to Azure <strong>Security Center</strong>. Within Security Center go to the &quot;Advanced Cloud Defense&quot; category and then click on &quot;Just in time VM access&quot; link as shown below:</p>
<img src="/images/15212354315aac35e7b2c2d.png" alt="JIT in Security Center">
<p>Next follow the below steps to enable JIT on a VM:</p>
<ol>
<li>Ensure that you are in the JIT section of Security Center</li>
<li>Go to the &quot;Recommended&quot; tab. This will show you all the VMs in your environment. </li>
<li>Click on the VM for which you want to enable Just in Time access. You can select multiple VMs here.</li>
<li>Click on the &quot;Enable JIT on x VMs&quot; button</li>
</ol>
<img src="/images/15212369445aac3bd0c8b26.png" alt="Enabling JIT">
<p>You will be presented with various settings for enabling JIT. These settings can be configured as:</p>
<ol>
<li>By default, various common ports are configured for JIT access with 3 hours time range. E.g. 3389 for RDP. Click on any of the default ports to tweak the settings.</li>
<li>You can add more ports and protocols by clicking on the &quot;Add&quot; button at the top.</li>
<li>Tweak the settings on the new blade that opens up.</li>
<li>Click on &quot;Save&quot; to save your settings.</li>
</ol>
<img src="/images/15212369535aac3bd974b55.png" alt="Settings while enabling JIT">
<p>That's all there is to it. Once JIT is configured you can view the VM in the &quot;Configured&quot; tab.</p>
<h3>Requesting access and activity log in JIT</h3>
<p>You can request access and perform various other management operations by following below steps:</p>
<ol>
<li>Once the JIT is configured, the VM will show up in the &quot;Configured&quot; tab as shown below. </li>
<li>Click on the VM that you want to manage or request access to. </li>
<li>As soon as you click, &quot;Request access&quot; button will start showing. You can click the button and it will ask for the number of hours that you need the access and the corresponding ports on which you need the access. Please note that only ports configured earlier can be granted access.</li>
<li>Click on the ellipse i.e. 3 dots in front of the VM record and you will see various options. Using these options you can view Properties. You can view the Activity log for previous requests and any attempts on attack. You can also Edit or Remove the access.</li>
</ol>
<img src="/images/15212369585aac3bde79755.png" alt="Requesting access and activity log in JIT">
<h3>Adding non-Azure computers</h3>
<p>You can add non-Azure computers for an extra fee per Node. Cost is calculated based on 15 USD/Node/Month. Resources that count as a node are VMs and computers. The selected security tier will be applied to current and new resources. For more information, visit <a href="https://docs.microsoft.com/en-us/azure/security-center/security-center-pricing" target="_blank">Microsoft Security Center's Standard tier for enhanced security</a> information page.</p>
<p>Ensure that you have onboarded the non-Azure computers to a linked Azure Log Analytics workspace. This is required in order to onboard non-Azure computers to Security Center.</p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-just-in-time-vm-access</link>
<pubDate>Wed, 10 Jan 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Troubleshooting Azure Site Recovery (ASR) - Data Replication Initiation Issues - Part 2</title>
<description><![CDATA[<p>This is the second part of troubleshooting Azure Site Recovery (ASR) Data replication issues. Part 1 is located here:  <a href="http://harvestingclouds.com/post/troubleshooting-azure-site-recovery-asr-data-replication-not-working/" target="_blank">Troubleshooting Azure Site Recovery (ASR) - Data Replication Not Working</a></p>
<p>Go through the part 1 before this blog as that talks about the location of logs and various preliminary steps. This blog talks about the issue where you are not even able to enable Data Replication for a server. </p>
<h3>Preliminary Checks</h3>
<p>Perform the following basic checks related to connectivity:</p>
<ul>
<li>Ensure that the ASR management server has connectivity with Azure.</li>
<li>Ensure connectivity between the management server (process server) and the source machine which you are trying to replicate. Ensure that the source machine is accessible from the process server.
<ul>
<li>You will also need to enable and allow “File and Printer Sharing” on the source machine in the Windows Firewall. </li>
</ul></li>
<li>Ensure that the account that you use for enabling the protection has administrator rights on the source machine. This is needed for installation of the Mobility services agent on the source machine.</li>
<li>Also allow Windows Management Instrumentation (WMI) in the Windows Firewall, if not already.</li>
<li>Disable remote User Account Control (UAC) if you are using local administrator account to install the mobility service.</li>
</ul>
<h3>Checking Services</h3>
<p>Also, check whether the following services are running and configured correctly on the source machine:</p>
<ol>
<li>If <strong>Volume Shadow Copy(VSS)</strong> service Startup Type is set to Disabled, change it to Automatic.</li>
<li>If <strong>COM+ System Application(COMSysApp)</strong> service Startup Type is to Disabled, change it to Automatic.</li>
<li>If the Microsoft <strong>Distributed Transaction Coordinator Service (MSDTC)</strong> Startup Type is set to Disabled, change it to Automatic.</li>
<li>If the service Startup Types were already set to Automatic, check if COM+ enumeration succeeds. a) You check COM+ enumeration by Component Services (comexp.msc) b) Browse to Component Service -&gt; Computers -&gt; My Computer -&gt; COM+ Application. You should be able to expand the System Application node and see the contents under that node. c) Ensure that Volume Shadow Copy Service is listed under Component Service -&gt; Computers -&gt; My Computer -&gt; DCOM config</li>
</ol>
<h3>Alternate Option</h3>
<p>If for any reason you can't enable any of the above steps, e.g. if you can't provide admin access to the service account on the source machine or disable remote UAC, then you have an alternative option. You can install the Mobility services agent on the source machine prior to enabling the protection. You will still need the services to be configured appropriately but will not need admin access while enabling the protection. To install the mobility services agent, you have various options:</p>
<ul>
<li><a href="vmware-azure-mobility-install-configuration-mgr.md">Install using software deployment tools like System Center Configuration Manager</a></li>
<li><a href="vmware-azure-mobility-deploy-automation-dsc.md">Install with Azure Automation and Desired State Configuration (Automation DSC)</a></li>
<li><a href="vmware-azure-install-mobility-service.md#install-mobility-service-manually-by-using-the-gui">Install manually from the UI</a></li>
<li><a href="vmware-azure-install-mobility-service.md#install-mobility-service-manually-at-a-command-prompt">Install manually from a command prompt</a></li>
<li><a href="vmware-azure-install-mobility-service.md#install-mobility-service-by-push-installation-from-azure-site-recovery">Install using the Site Recovery push installation</a></li>
</ul>
<p>All these options are described in details here: <a href="https://docs.microsoft.com/en-us/azure/site-recovery/vmware-azure-install-mobility-service" target="_blank">Install the Mobility service</a></p>
<h3>Next Steps</h3>
<p>Restart the job after ensuring all the pre-requisites as described in previous sections. If you are still facing issues, revisit the Part 1 of the troubleshooting series as mentioned at the beginning of this blog. Let us know in comments below if the issue persists. </p>]]></description>
<link>http://HarvestingClouds.com/post/troubleshooting-azure-site-recovery-asr-data-replication-initiation-issues-part-2</link>
<pubDate>Mon, 08 Jan 2018 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Creating a Custom Policy - Part 2 - Understanding the Policy Structure</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a>. Ensure that you have read earlier blogs about the basics of Azure Policies before reading this blog.</p>
<h3>Structure of a Policy JSON Definition</h3>
<p>Policy definitions are written in JSON. The policy definition contains elements for:</p>
<ul>
<li>mode</li>
<li>parameters</li>
<li>display name</li>
<li>description</li>
<li>policy rule
<ul>
<li>logical evaluation</li>
<li>effect</li>
</ul></li>
</ul>
<p>Let's understand these components with the help of an example. Earlier we saw how to find the definition of existing policies. One such policy is: &quot;Allowed locations&quot;. This policy restricts the location in Azure where resources can be deployed. The definition for the policy looks like below:</p>
<pre><code>{
  "properties": {
    "mode": "all",
    "parameters": {
      "allowedLocations": {
        "type": "array",
        "metadata": {
          "description": "The list of locations that can be specified when deploying resources",
          "strongType": "location",
          "displayName": "Allowed locations"
        }
      }
    },
    "displayName": "Allowed locations",
    "description": "This policy enables you to restrict the locations your organization can specify when deploying resources.",
    "policyRule": {
      "if": {
        "not": {
          "field": "location",
          "in": "[parameters('allowedLocations')]"
        }
      },
      "then": {
        "effect": "deny"
      }
    }
  }
}</code></pre>
<ol>
<li><strong>Mode</strong> tells you the type of resources for which the policy will be applied. Allowed values are &quot;All&quot; (where all Resource Groups and Resources are evaluated) and &quot;indexed&quot; (where policy is evaluated only for resources which support tags and location)</li>
<li><strong>Parameters</strong> are used for providing inputs to the policy. They can be reused at multiple locations within the policy. You can refer to a parameter as: &quot;<strong><em>[parameters('allowedLocations')]</em></strong>&quot;</li>
<li><strong>Display Name</strong> is used to show the policy in the portal or programmatically.</li>
<li>A <strong>description</strong> is used to provide details for the policy.</li>
<li><strong>Policy Rule</strong> is which defines the policies. This is the section where the restrictions are defined in a Poicy Definition. To understand a policy you need to focus most of your efforts on this section. Every Policy rule has two key sections defined under &quot;if-then&quot; blocks. 
<ol>
<li><strong>Logical Evaluation</strong> is defined under the &quot;if&quot; block. It defines the condition which is evaluated to determine the policy should be applied or not.</li>
<li><strong>Effect</strong> is defined in the &quot;then&quot; block. This defines what will happen if the condition is met. </li>
</ol></li>
</ol>
<h3>Policy Rules</h3>
<p>Let's look at the policy rules in more details. The general syntax of the &quot;if-then&quot; block is as shown below:</p>
<pre><code>{
  "if": {
    &lt;condition&gt; | &lt;logical operator&gt;
  },
  "then": {
    "effect": "deny | audit | append"
  }
}</code></pre>
<p>Supported logical operators are:</p>
<ul>
<li><code>"not": {condition  or operator}</code></li>
<li><code>"allOf": [{condition or operator},{condition or operator}]</code></li>
<li><code>"anyOf": [{condition or operator},{condition or operator}]</code></li>
</ul>
<p><strong>Not</strong> operator means that the opposit of the condition should be true for the policy to be applied. <strong>AllOf</strong> requires all the conditions defined to be true at the same time. <strong>AnyOf</strong> requires any one of the conditions to be true for the policy to be applied.</p>
<p>In the previous example, the policy rule section is as shown below:</p>
<pre><code>    "policyRule": {
      "if": {
        "not": {
          "field": "location",
          "in": "[parameters('allowedLocations')]"
        }
      },
      "then": {
        "effect": "deny"
      }
    }</code></pre>
<p>In simple terms, the rule above says that if the location is not in the list of allowed locations, defined by the parameter <strong><em>allowedLocations</em></strong>, then the effect will be <strong><em>deny</em></strong> i.e. the resource creation will not be allowed.</p>
<p>You can find the complete list of operators and conditional constructs here: <a href="https://docs.microsoft.com/en-us/azure/azure-policy/policy-definition" target="_blank">Azure Policy definition structure
</a></p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-creating-a-custom-policy-part-2-understanding-the-policy-structure</link>
<pubDate>Sat, 30 Dec 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Creating a Custom Policy - Part 1 - Viewing Definition of an existing Policy</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>Creating a custom policy to enforce your custom requirements in the Azure environment is very easy. This provides you with granular control over what a policy should perform and what should be allowed and what should be denied.</p>
<p>Policies are writing in JSON format. It is always good to base your custom policy definition on one of the built-in policies if one exists which is closer to what you are trying to do. </p>
<h3>Viewing Definition of Existing Policies</h3>
<p>You have two options to view the definition of an existing policy.</p>
<ol>
<li>Using PowerShell</li>
<li>Using Azure Portal</li>
</ol>
<h3>Using PowerShell</h3>
<p><strong>Using PowerShell</strong> run the below cmdlet to view all the policies in your environment.</p>
<pre><code>Get-AzureRmPolicyDefinition</code></pre>
<p>Then go through the list and find the policy that you want to use. Find the ResourceId of that policy and then run the below cmdlet to fetch the details of that policy.</p>
<pre><code>Get-AzureRmPolicyDefinition -Id "/providers/Microsoft.Authorization/policyDefinitions/e56962a6-4747-49cd-b67b-bf8b01975c4c"</code></pre>
<h3>Using Azure Portal</h3>
<p><strong>Using Azure Portal</strong> to view the Definition is also very easy. Simply navigate to:</p>
<ul>
<li>the Subscription section in the Azure Portal</li>
<li>Select your subscription</li>
<li>Click on &quot;Policies&quot; under settings. </li>
<li>Within the Policies blade, click on &quot;Definitions&quot;</li>
</ul>
<p>Within the Policy Definitions, select the Policy from the list for which you want to view the definition. Click on the 3 dots to the right. From the context menu select &quot;View definition&quot;.</p>
<img src="/images/15218187805ab51c9c095ce.png" alt="Policy Definitions">
<p>This will open up another blade. Click on the &quot;Json&quot; tab at the top and this will show you the rule part of the definition of the policy. the rule is the most important part of the policy. We will look at other components of the policies later as well.</p>
<img src="/images/15218187845ab51ca0c313d.png" alt="Policy Definition Details">
<p>E.g. The JSON for &quot;Allowed storage account SKUs&quot; built-in policy looks as shown below.</p>
<pre><code>{
  "if": {
    "allOf": [
      {
        "field": "type",
        "equals": "Microsoft.Storage/storageAccounts"
      },
      {
        "not": {
          "field": "Microsoft.Storage/storageAccounts/sku.name",
          "in": "[parameters('listOfAllowedSKUs')]"
        }
      }
    ]
  },
  "then": {
    "effect": "Deny"
  }
}</code></pre>
<p>Next, we will dissect and understand the Policy structure in details.</p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-creating-a-custom-policy-part-1-viewing-definition-of-an-existing-policy</link>
<pubDate>Fri, 29 Dec 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure Policies - 2 - Assigning a Policy</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>In this post, we will view the policies in action. Policy assignment is very easy on the Azure Portal. We will be assigning a built-in policy at a subscription scope.</p>
<h3>Accessing the Policies in the Azure portal</h3>
<p>Begin by accessing the Policies in the Azure Portal. To do this follow the below steps:</p>
<ol>
<li>Navigate to <strong>Subscriptions</strong> (via All Services or the navigation sidebar)</li>
<li>Select the subscription for which you want to view the Policies</li>
<li>Scroll down to the &quot;Settings&quot; category in the menu of the subscription</li>
<li>Click on &quot;<strong>Policies</strong>&quot; to access the policies in Azure</li>
</ol>
<h3>Assigning the Built-in Policies</h3>
<p>To perform the assignment, click on &quot;Assign Policy&quot; from either Compliance or the Assignment tabs.</p>
<img src="/images/15217866825ab49f3a81d6a.png" alt="Assign Policy">
<p>In the new blade, provide the value for the:</p>
<ol>
<li>Policy to be applied</li>
<li>Name and Description of the Policy. The name will be the name of the policy selected by default. As a best practice ensure to provide the detailed description.</li>
<li>Assigned by will be your name by default</li>
<li>You can select the pricing tier between Free and Standard. You will get the compliance evaluation of the resources in your environment against the policy with the Standard pricing tier</li>
<li>Scope for the Policy</li>
<li>Exclusions from the Policy</li>
<li>Any additional parameters related to the policy</li>
</ol>
<p>To select from the policies, click on the blue button with an ellipse (i.e. 3 dots) in front of the Policy box. This will popup another blade for all the Policy definitions.</p>
<img src="/images/15217894075ab4a9df6a23e.png" alt="Assign Policy Blade details">
<p>Scroll through various Built-in policies. Once we define any custom user-defined policies, they will also be displayed here. Select the policy &quot;Allowed locations&quot; from the list of the policies as an example. Click on &quot;Select&quot; once done.</p>
<img src="/images/15217894445ab4aa0494a0d.png" alt="Selecting Policy">
<p>Select the Scope for applying the policy. You can leave the default to the Subscription level. Or you can click on the blue button in front of scope text box and select the Resource Groups under the subscription on which you want to apply the policy.</p>
<p>You can also select the Exclusions if you require. These Resource Group or resources will not be evaluated against the policy.</p>
<img src="/images/15217894615ab4aa15470ec.png" alt="Selecting Scope">
<p>Lastly, you will have additional parameters for the policy related values. These parameters will vary and will depend on the policy you have selected. E.g. For &quot;Allowed locations&quot; policy, you will see the parameter for allowed locations. Select &quot;East US&quot; and &quot;East US 2&quot; for the locations as an example.</p>
<img src="/images/15217894765ab4aa248f15f.png" alt="Providing value for Policy related Parameters">
<p>Once you complete the configurations, click on the &quot;<strong>Assign</strong>&quot; button to apply the policy</p>
<h3>Validating the Policy</h3>
<p>To validate the policy for &quot;Allowed locations&quot; follow these steps:</p>
<ol>
<li>Try to deploy a Storage Account or a VM or any other resource in a location that is NOT allowed. E.g. try deploying a storage account in the &quot;West US&quot; location. This should fail with the validation error stating the policy id.</li>
<li>Perform the same deployment but to one of the allowed location. E.g. try deploying a storage account in the &quot;East US&quot; location. This should succeed without any erros. </li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-policies-2-assigning-a-policy</link>
<pubDate>Wed, 27 Dec 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - RBAC Roles</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p>Azure <strong>RBAC Roles</strong> are a great way to securely provide access to users with limited actions in Azure. The focus is to provide only the access necessary so that an account in your organization doesn't have more access than needed. In case the account gets compromised, this will ensure that does not leave your environment too much vulnerable. This means that if you grant an employee access to manage Virtual Machines in your environment, that employee can't alter the virtual networks by mistake or have any access to delete the storage accounts in the environment. </p>
<h3>Roles Hierarchy</h3>
<p>The RBAC Roles can be assigned at the following three levels in the order of hierarchy:</p>
<ol>
<li><strong>Subscription</strong></li>
<li><strong>Resource Group</strong></li>
<li><strong>Resource</strong></li>
</ol>
<p>Any Resource in Azure, must belong to a Resource Group (under the ARM model). And every Resource Group in Azure must belong to a single subscription.</p>
<p>If a person is assigned scope at Subscription level then he/she will get access to all Resource Groups and to all resources within those resource groups. Next, if a person is assigned access at a Resource Group level then they will automatically get access to all the Resources within that Resource Group. Finally, if a person is provided access only to an individual Resource then they will get access only to that Resource.</p>
<h3>Built-in Roles</h3>
<p>There are various inbuilt roles for this purpose like:</p>
<ol>
<li>Contributor - create and manage but can't grant access to others</li>
<li>Reader - can only view</li>
<li>Owner - full access</li>
</ol>
<p>The complete list of Built-in roles can be viewed here: <a href="https://docs.microsoft.com/en-us/azure/active-directory/role-based-access-built-in-roles" target="_blank">Built-in roles for Azure role-based access control</a></p>
<h3>Viewing and Adding Access</h3>
<p>To view or Add access, first decide the scope where you want to provide the access. Select the scope, e.g. a Resource Group on which you want to view existing access and grant the access to someone. Then follow these steps (as per the image below):</p>
<ol>
<li>Click on &quot;Access Control (IAM)&quot; to access the RBAC access control</li>
<li>View the access in the center area. Scroll down to view all type of roles and Users, Groups or Apps with the access</li>
<li>Click on &quot;+Add&quot; button, as shown below, to add access to a new user, group or application</li>
<li>In the new popup blade, select the Role (e.g. Contributor), Assignment scope and name/email address of the user or app to whom you want to grant the access. You can select multiple users/apps as well.</li>
</ol>
<img src="/images/15216952115ab339eba24e0.png" alt="Viewing and Adding RBAC Access">
<p>Within each subscription, you can grant up to 2000 role assignments.</p>
<p>Learn about creating Custom RBAC Roles here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-custom-rbac-roles/" target="_blank">Demystifying Azure Security - Custom RBAC Roles</a></p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-rbac-roles</link>
<pubDate>Wed, 20 Dec 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Demystifying Azure Security - Azure Policies - 1 - Basics</title>
<description><![CDATA[<p>This blog post is part of the <strong>Demystifying Azure Security</strong> series. All posts in the series can be found here: <a href="http://harvestingclouds.com/post/demystifying-azure-security-series-index/" target="_blank">Demystifying Azure Security - Series Index</a></p>
<p><strong>Azure Policies</strong> are a great tool to make access to Azure more secure by prohibiting certain operations and enforcing various rules in your environment. The policies make your environment compliant and ensure that you adhere to the service level agreements and standards set within your organization. If the end user tries to deploy a resource violating the policies then the deployment fails at the validation step ensuring that your environment remains compliant. If you already have resources which are not compliant then you will be able to review those and take necessary actions.</p>
<h3>Examples of Azure Policies</h3>
<p>Here are a few of most commonly used policies:</p>
<ol>
<li><strong>Restricting the Allowed locations</strong> where a resource can be deployed. You can restrict the allowed locations where resources can be deployed to the ones that are closer to your geographic location. This also ensures that the users within your organization can't deploy resources to noncompliant locations. </li>
<li><strong>Enforcing a Tag</strong> and it's Value. E.g. You have a Resource Group for the Finance department. You want to ensure that any resource deployed in that Resource Group should be tagged with a &quot;CostCenter&quot; and a specific value for that cost center. You can enforce this using a Policy.</li>
<li><strong>Not Allowed Resource Types</strong>. Using this policy you can prohibit the deployment of certain resource types in your environment. </li>
</ol>
<p>There are various other built-in Policies. You can also create your own policies which we will discuss in more detail later.</p>
<h3>Scope where a Policy is Assigned</h3>
<p>A Policy can be assigned at the below levels:</p>
<ol>
<li><strong>Subscription</strong> level - Applied to all Resource Groups and Resources within the subscription</li>
<li><strong>Resource Group</strong> level - Applied only to the Resources within the selected Resource Groups</li>
</ol>
<p><strong>Exclusions</strong>: In addition to the assignment scope, you can exclude certain Resource Groups or individual Resources from the Policy assignments. The Policy will be applied to all the Resources in the scope except the ones excluded by defining the Exclusions. </p>
<h3>How are Policies different from Role-Based Access Control (RBAC)</h3>
<p>Through <strong>Role-Based Access Control (RBAC)</strong>, you focus on the role and scope of access that a user can have in Azure. You provide the access control (IAM) within Azure for Subscription, Resource Group or an individual Resource. You define a role for the user/app/group like Contributor, Reader etc.  You can use the built-in roles or can define custom roles.</p>
<p>With <strong>Policies</strong>, you focus on the properties of the resources with which the user can work at the defined scope. You define what properties are allowed and what is restricted. You can use the built-in policies or can define your own.</p>
<p>E.g. Through RBAC you define that a particular user has Reader role at the subscription and Contributor role at a particular Resource Group. This will mean that the user can view all the resources in the subscription but can only modify or create resources in the Resource Group where he has Contributor access.</p>
<p>Now if you define a policy which restricts the type of resources deployed ( as discussed in the examples above), the user will not be able to deploy the restricted resource types, even though he has contributor access to a Resource Group.</p>
<h3>Accessing Policies in the Azure portal</h3>
<p>To access the Policies in the Azure portal:</p>
<ol>
<li>Navigate to <strong>Subscriptions</strong> (via All Services or the navigation sidebar)</li>
<li>Select the subscription for which you want to view the Policies</li>
<li>Scroll down to the &quot;Settings&quot; category in the menu of the subscription</li>
<li>Click on &quot;<strong>Policies</strong>&quot; to access the policies in Azure</li>
</ol>
<img src="/images/15217849365ab4986888c88.png" alt="Policies in Azure Portal">
<p>We will be discussing more about the Azure Policies in later blogs.</p>]]></description>
<link>http://HarvestingClouds.com/post/demystifying-azure-security-azure-policies-1-basics</link>
<pubDate>Sun, 10 Dec 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Great Way to learn - 2018 Microsoft Azure Community Study Groups</title>
<description><![CDATA[<p>How many times have you started preparation for any Azure certification and have left it midway because of any reason? We all need a little extra nudge. Microsoft Azure Community Study Groups is just the thing you need to create and fulfill your next year resolutions. </p>
<p>Here are the registration links, provided &quot;as is&quot;:</p>
<table border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td width="461" valign="top">
<p><b>Exam</b></p>
</td>
<td width="158" valign="top">
<p><b>Registration Link</b></p>
</td>
<td width="208" valign="top">
<p><b>Dates</b></p>
</td>
</tr>
<tr>
<td width="461" valign="top">
<p><b>70-532: Developing Microsoft Azure Solutions</b></p>
</td>
<td width="158" valign="top">
<p><a href="https://aka.ms/532asg">https://aka.ms/532asg</a></p>
</td>
<td width="208" valign="top">
<p>March 23 – May 24, 2018</p>
</td>
</tr>
<tr>
<td width="461" valign="top">
<p><b>70-533: Implementing Microsoft Azure Infrastructure Solutions</b></p>
</td>
<td width="158" valign="top">
<p><a href="https://aka.ms/533asg">https://aka.ms/533asg</a></p>
</td>
<td width="208" valign="top">
<p>January 12 – April 13, 2018</p>
</td>
</tr>
<tr>
<td width="461" valign="top">
<p><b>70-535: Architecting Microsoft Azure Solutions </b></p>
</td>
<td width="158" valign="top">
<p><a href="https://aka.ms/535asg">https://aka.ms/535asg</a></p>
</td>
<td width="208" valign="top">
<p>January 12 – May 24, 2018</p>
</td>
</tr>
<tr>
<td width="461" valign="top">
<p><b>70-483: Programing in C#</b></p>
</td>
<td width="158" valign="top">
<p><a href="https://aka.ms/483asg">https://aka.ms/483asg</a></p>
</td>
<td width="208" valign="top">
<p>January 12 – March 2, 2018</p>
</td>
</tr>
<tr>
<td width="461" valign="top">
<p><b>70-486: Developing ASP.NET MVC Web Applications</b></p>
</td>
<td width="158" valign="top">
<p><a href="https://aka.ms/486asg">https://aka.ms/486asg</a></p>
</td>
<td width="208" valign="top">
<p>January 12 – March 23, 2018</p>
</td>
</tr>
<tr>
<td width="461" valign="top">
<p><b>70-487: Developing Microsoft Azure and Web Services</b></p>
</td>
<td width="158" valign="top">
<p><a href="https://aka.ms/487asg">https://aka.ms/487asg</a></p>
</td>
<td width="208" valign="top">
<p>March 9 – May 11, 2018</p>
</td>
</tr>
</tbody>
</table>
<p>You can access the main blog from here:
<a href="https://blogs.technet.microsoft.com/uspartner_ts2team/2017/12/04/2018-microsoft-azure-community-study-groups/" target="_blank">2018 Microsoft Azure Community Study Groups</a></p>]]></description>
<link>http://HarvestingClouds.com/post/great-way-to-learn-2018-microsoft-azure-community-study-groups</link>
<pubDate>Tue, 05 Dec 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Introducing the Harvesting Clouds YouTube channel</title>
<description><![CDATA[<p>I am happy to introduce the YouTube channel for Harvesting Clouds. The channel can be accessed here: <a href="https://www.youtube.com/channel/UCIfsXMJ8HMJkx1lMP-lngzw" target="_blank">Harvesting Clouds YouTube Channel</a></p>
<p>Do <strong>Subscribe</strong> to the channel to get notified of any new content on the channel. <strong>Like</strong> the content if you learned something new or got new perspective on the things you knew.</p>
<p>Check out the author introduction video below:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HxudnbeFofk?rel=0" frameborder="0" allowfullscreen></iframe>
<p>The managed and ordered content via <strong>playlists</strong> can be found here: <a href="https://www.youtube.com/channel/UCIfsXMJ8HMJkx1lMP-lngzw/playlists" target="_blank">Playlists - Harvesting Clouds</a></p>
<p><strong>All videos</strong> on the channel can be found here: <a href="https://www.youtube.com/channel/UCIfsXMJ8HMJkx1lMP-lngzw/videos" target="_blank">All Videos - Harvesting Clouds</a></p>]]></description>
<link>http://HarvestingClouds.com/post/introducing-the-harvesting-clouds-youtube-channel</link>
<pubDate>Wed, 29 Nov 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Video - Azure Automation - Introduction</title>
<description><![CDATA[<p>This is the first video in the series of Azure Automation. This video provides the introduction and talks about the various theoretical concepts about Azure Automation</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/8tLdzsg6YVk?rel=0" frameborder="0" allowfullscreen></iframe>
<p>You can view and subscribe to the YouTube channel here: <a href="https://www.youtube.com/channel/UCIfsXMJ8HMJkx1lMP-lngzw" target="_blank">HarvestingClouds on YouTube</a></p>]]></description>
<link>http://HarvestingClouds.com/post/video-azure-automation-introduction</link>
<pubDate>Wed, 29 Nov 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Introducing Azure Reserved VM Instances (RIs)</title>
<description><![CDATA[<p>Azure Reserved VM Instances provides an easy option to save cost for predictive workloads. You commit to either one or three year options and pay the complete cost with discounts upfront. </p>
<h2>When should you use this</h2>
<p>If you know that your workload will be up and running then Azure Reserved VM instances are for you.
The savings can be up to 72% with the use of Reserved VM instances. If your VM is not going to be up and running for most time and you have the option to automate and shut down the Virtual Machine then you don't need Reserved VM Instances.</p>
<h2>How does the cost savings look like</h2>
<p>The cost savings can be very significant especially when clubbed with Azure Hybrid Use Benefit. The below graphic is based on a Dv2 three-year RI with Azure Hybrid Benefit.</p>
<p><img src="http://HarvestingClouds.com/images/15120755665a20712ed8b1b.png" alt="Potential Savings with Azure Reserved VM Instances" /></p>
<h2>Other aspects</h2>
<p>Other aspects of Azure Reserved VM Instances that you need to know are:</p>
<ul>
<li>You can assign RI benefit at either the enrollment level or at the subscription level</li>
<li>The assignment is as easy as providing the Region, VM Series/size and providing the term. The two terms offered today are 1 year and 3 years</li>
<li>You can exchange to a new instance and location as you need in the future</li>
<li>You also have the option to cancel anytime directly with Microsoft for a pro-rated refund</li>
</ul>
<p>Reference: <a href="https://azure.microsoft.com/en-ca/pricing/reserved-vm-instances/" target="_blank">Azure Reserved VM Instances</a></p>]]></description>
<link>http://HarvestingClouds.com/post/introducing-azure-reserved-vm-instances-ris</link>
<pubDate>Sun, 12 Nov 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Why you should be using Azure Managed Disks now?</title>
<description><![CDATA[<p>Azure Managed Disks bring lots of advantages and benefits to the table when compared to Azure Storage Account based VM disks. The first thing to note is that these are only related to VM disks and not general blob storage. In this post, lets take a look at what all benefits you get when you create your virtual machine with a managed disk instead of a storage account based disks.</p>
<h3>More Controlled Access Management</h3>
<p>Let's assume a scenario where all the disks for the virtual machines in your environment belonged to a particular storage account. Let us say that there are VMs belonging to Finance as well as HR departments. Now if you want to give access to a VHD file of a VM belonging to HR department, you will provide the access to the storage account. This was the lowest level where you could provide the access. This inadvertently opened the access to the Finance VM's VHD files as well.</p>
<p>Managed Disks are individual resources in Azure. If a VM has 1 OS disk and 2 data disks, all implemented as a managed disk, then you can even provide the access to one of the data disk and not provide access to any of the other disks.</p>
<h3>No Storage account service limits</h3>
<p>Earlier with storage accounts there were Service Limits related to IOPS at the storage account level. When the infrastructure grew and there comes a time the number of disks grew to a point that this service limit will be hit and this can affect your architecture. With managed disks, you are no longer limited by the storage account limits.</p>
<h3>Ability to take Snapshot</h3>
<p>Now with managed disks you have the capability to take snapshots on the fly. You can later restore from these snapshots as required. You can take these snapshots onto a different storage account.</p>
<h3>Ability to Capture better images</h3>
<p>The images that you capture on the Vms, which are created using managed disks, will not just include the OS disk, but will also include all the data disk.</p>
<h3>Ability to convert a Standard disk to Premium disk and vice versa</h3>
<p>Earlier if you wanted to convert a standard disk to a premium disk (or vice versa) you needed to create a new storage account and copy over the disk. Now with managed disks, this is as easy as shutting down the virtual machine and just changing a value in a drop down.</p>
<h3>Other benefits</h3>
<p>Other benefits include:</p>
<ul>
<li>Better reliability for Availability Sets</li>
<li>Highly durable and available with design for 99.999% availability</li>
<li>Better Azure Backup service support with the ability to create a backup job with time-based backups, easy VM restoration, and backup retention policies</li>
</ul>
<p>Reference: </p>
<ul>
<li><a href="https://azure.microsoft.com/en-ca/services/managed-disks/" target="_blank">Managed Disks product page</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/managed-disks-overview" target="_blank">Managed Disks overview</a></li>
</ul>]]></description>
<link>http://HarvestingClouds.com/post/why-you-should-be-using-azure-managed-disks-now</link>
<pubDate>Sun, 05 Nov 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Cloud Shell - Embedding Cloud Shell in your Websites</title>
<description><![CDATA[<p>Embedding Azure Cloud Shell in your websites gives your visitors an option to directly interact with Azure, without even leaving your website. Your website may be talking about a code sample that your visitor may want to try in Azure. Instead of opening a new instance of Azure Portal, the visitors can try the command right from your site if you have embedded the cloud shell in your website.</p>
<p>All you need is the embed code and you are all set. Just add this code to your site and then you will be able to launch the cloud shell right from there with just a click of a button.Behind the scene, this is nothing but hyperlinking to the URL: <a href="https://shell.azure.com">https://shell.azure.com</a></p>
<h3>Embed Code</h3>
<p>The embed code is as follows if you are using Markdown</p>
<pre><code>[![Launch Cloud Shell](https://shell.azure.com/images/launchcloudshell.png "Launch Cloud Shell")](https://shell.azure.com)</code></pre>
<p>If you are building a website in HTML then the embed code will look like:</p>
<pre><code>&lt;a style="cursor:pointer" onclick='javascript:window.open("https://shell.azure.com", "_blank", "toolbar=no,scrollbars=yes,resizable=yes,menubar=no,location=no,status=no")'&gt;&lt;image src="https://shell.azure.com/images/launchcloudshell.png" /&gt;&lt;/a&gt;</code></pre>
<h3>Code in Action</h3>
<p>Clicking on below image will open a popup. It will navigate you to the <a href="https://shell.azure.com">https://shell.azure.com</a> URL within the popup. You can login to Azure and try any commands. I encourage to inspect the link below and compare with the above embed code.</p>
<p><a style="cursor:pointer" onclick='javascript:window.open("https://shell.azure.com", "_blank", "toolbar=no,scrollbars=yes,resizable=yes,menubar=no,location=no,status=no")'><image src="https://shell.azure.com/images/launchcloudshell.png" /></a></p>
<h3>Choosing Shell Type</h3>
<p>The above examples will open the most recently used cloud shell. If you want to encourage visitors to experience a specific cloud shell, then update the URL section above embed code. Update as below by adding the last part to the URLs:</p>
<ul>
<li>Bash Shell - <a href="https://shell.azure.com/bash">https://shell.azure.com/bash</a></li>
<li>PowerShell Shell - <a href="https://shell.azure.com/powershell">https://shell.azure.com/powershell</a></li>
</ul>
<h3>Reference:</h3>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/cloud-shell/embed-cloud-shell" target="_blank">Embed Azure Cloud Shell</a></li>
</ul>]]></description>
<link>http://HarvestingClouds.com/post/azure-cloud-shell-embedding-cloud-shell-in-your-websites</link>
<pubDate>Mon, 16 Oct 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Cloud Shell - Introduction</title>
<description><![CDATA[<p><strong>Azure Cloud Shell</strong> is a very convenient option to run your automation scripts against your subscriptions. </p>
<p>The shell is provided in two flavors:</p>
<ul>
<li>Bash based shell</li>
<li>PowerShell in Cloud Shell (currently in Preview)</li>
</ul>
<p>The Cloud Shell is a Browser based experience. What this means is that you do not need to install any dependencies for the Azure commands to run. Also, there is no dependency on your machine's hardware. You can work irrespective of your local machine's configurations.</p>
<h3>Support for various tools</h3>
<p>Azure Cloud Shell comes preloaded with various tools depending upon the shell type selected. It comes loaded with Linux tools like bash, sh, tmux and dig. The Azure tools which are preloaded are Azure CLI 2.0 and 1.0, AzCopy,etc.Text editors include vim and nano. Git source control tools are also included. Various other Build, Containers, Databases, etc. tools are also included. It also comes with language support for .net version 2.0.0, Java version 1.8, Node.js 6.9.4, PowerShell 6.0(beta) and Python 2.7 and 3.5 etc. Check the References section for the full list and up to date version of Tools and Language support.</p>
<h3>Key Concepts</h3>
<p>Below are some of the key concepts (in nutshell) about the cloud shell.</p>
<ol>
<li>You can opt for any one of the two shell options available, as per your preferences. You can choose to have a <strong>Bash shell</strong> or <strong>PowerShell in Cloud shell</strong></li>
<li>You can easily switch between the two shells at any time</li>
<li>If you are logged into Azure portal then you are also automatically authenticated into the Azure Cloud Shell</li>
</ol>
<p>Other important concepts as per Microsoft documentation are:</p>
<ol>
<li>Cloud Shell runs on a temporary host provided on a per-session,
per-user basis </li>
<li>Cloud Shell times out after 20 minutes without
interactive activity </li>
<li>Cloud Shell requires an Azure file share to be
mounted </li>
<li>Cloud Shell uses the same Azure file share for both Bash and
PowerShell </li>
<li>Cloud Shell is assigned one machine per user account </li>
<li>Bash persists $Home using a 5-GB image held in your file share</li>
<li>Permissions are set as a regular Linux user in Bash</li>
</ol>
<h3>Pricing</h3>
<p>Cloud Shell does not have any direct cost linked to it. The charge for Cloud Shell is only for the usage of the underlying Azure Storage. The exact charge depends on the amount you store on that storage and the type of the storage. By default, Locally Redundant Storage is created. </p>
<p><strong>References</strong>:</p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/cloud-shell/overview" target="_blank">Azure Cloud Shell Overview</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/cloud-shell/features" target="_blank">Azure Cloud Shell Features and Tools</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/cloud-shell/features-powershell" target="_blank">Features &amp; tools for PowerShell in Azure Cloud Shell</a></li>
</ul>]]></description>
<link>http://HarvestingClouds.com/post/azure-cloud-shell-introduction</link>
<pubDate>Sun, 08 Oct 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Script Sample - Generate Azure Resources Report by Tags</title>
<description><![CDATA[<p>&lt;&lt;<strong>Update</strong>: This post and the sample script is now updated to support latest Azure PowerShell cmdlets&gt;&gt;</p>
<p>When managing resources in Azure, Tags are there to help you. They add very valuable metadata to the Azure resources.
In nutshell, tags are Key-Value pairs. E.g. &quot;Business Unit = Finance&quot;, &quot;Site = Central US&quot; are two such tags.</p>
<p>These <strong>tags help you to</strong>:</p>
<ul>
<li>Organize your resources and manage the same</li>
<li>Get insights into Chargeback categorically</li>
</ul>
<p>Tags go beyond the boundaries of deployments. You can have few resources deployed in one resource group and few other resources into the second resource group. If for these resources in both the resource groups you apply the same tag (i.e. same Key and value combination) then you can view and manage these resources in a single click.</p>
<p>Now once in a while, you want to take a health check of your Azure environment. You want to see what all resources are there and what are the tags applied to these resources. You want to extract this data to a CSV file so that you can apply filters and perform other business intellegence (BI) operations on it. The script below provides exactly that. </p>
<p>The <strong>script gives you</strong> a CSV output report with:</p>
<ul>
<li>All the resources in your Azure Subscription</li>
<li>Type of each resource, so that you can filter on various types</li>
<li>Tags for each of the resource in Azure</li>
</ul>
<p><strong>The Columns</strong> in the Output CSV file (generated by the script) are:</p>
<ol>
<li>Semi-colon separated list of tags</li>
<li>Resource Name</li>
<li>Resource Group Name</li>
<li>Location</li>
<li>Resource Type</li>
<li>Resource Id</li>
<li>Name</li>
<li>Subscription Id</li>
</ol>
<p>You can find this script on GitHub here: <a href="https://github.com/HarvestingClouds/PowerShellSamples/blob/a4eb910aa8eb2cdd340c2866cde150282b47067e/Scripts/Azure%20Resources%20Report%20by%20Tags.ps1">Azure Resources Report by Tags</a></p>
<p><a href="https://raw.githubusercontent.com/HarvestingClouds/PowerShellSamples/master/Scripts/Get-AzureRmTagsReport.ps1">Direct Link to the Script here. Right click and choose Save As</a></p>
<p>You are welcome to make changes and submit Pull Requests to this script or even fork and make your modifications. </p>]]></description>
<link>http://HarvestingClouds.com/post/script-sample-generate-azure-resources-report-by-tags</link>
<pubDate>Thu, 26 Jan 2017 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Application Insights is now generally available</title>
<description><![CDATA[<p>After a long time in Preview, Azure Application Insights is now generally available from Microsoft.</p>]]></description>
<link>http://HarvestingClouds.com/post/azure-application-insights-is-now-generally-available</link>
<pubDate>Tue, 29 Nov 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Site Recovery (ASR) - New feature added to target Resource Groups</title>
<description><![CDATA[<p>A new feature is now added to Azure Site Recovery (ASR) which enables you to target the Resource Groups for failover. You can target a separate Resource Group for each Server/VM being protected. Now post failover, this target Resource Group will be used. Earlier,  during a failover, a new resource group was created with the same name as the server being failed over.</p>
<p>You can configure this setting in two ways:</p>
<ol>

<li>
When you are <b>Enabling Replication of new servers</b> then at the settings for "Target" you will have two new options. 

<ul><li>The first option is "Post-failover resource group" along with a drop down. This is where you select the target Resource Group. You can select one of the existing Resource group here. If you haven't created the one you want to use as a target then please create before enabling replication. </li> 

<li>The second new option is to select "Post-failover deployment model" which has only two options. These options are "Classic" and "Resource Manager". The later will be selected by default and should be the one you should be using to leverage all the new features which Azure has to offer.</li>
</ul>

<br/>
<img src="/images/1479510199582f88b742692.png" alt="Enable Replication">
</li>

<li>
If you have servers which were protected before this feature was introduced or if you configured wrong Resource Group during enabling of replication and now want to change the Resource Group then you can do so using this alternate way. Go to your ASR Vault and then go to Settings. Then go to the "Replicated Items" and click on the server for which you want to change the resource group. Click on "Compute and Network" in the server properties as shown below. You will see a new option here to select or change the target Resource Group. Click "Save" at the top of the blade after selecting or changing the value.

<br/>

<img src="/images/1479493236582f4674a80d3.png" alt="Compute and Network settings">
</li>
</ol>
<p>This new option makes the failover process in Azure Site Recovery (ASR) much more manageable and overall a better experience.</p>]]></description>
<link>http://HarvestingClouds.com/post/azure-site-recovery-asr-new-feature-added-to-target-resource-groups</link>
<pubDate>Thu, 17 Nov 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Troubleshooting Azure Site Recovery (ASR) - Data Replication Not Working</title>
<description><![CDATA[<p>If you have the Azure Site Recovery (ASR) setup in your environment and are facing the issue where the data replication is stuck, then follow this blog to troubleshoot. The data replication can be stuck either during the initial replication or during the delta changes. This can occur for various reasons. We will inspect various components involved in ASR. Major of the troubleshooting is done on the Management Server i.e. the on-premise Configuration/Master target server.</p>
<h3>1. Check Alerts Details</h3>
<p>Go to the Azure Site Recovery Vault and navigate to the settings. Click on the <strong><em>Alerts and Events</em></strong>. Check the alerts for the data replication being blocked etc. Verify that the problem is related to data replication and not something else. </p>
<p>You can also navigate to the &quot;<strong><em>Replicated Items</em></strong>&quot; in the ASR Vault settings. On the blade for replicated items, click on the server for which the data is not being replicated. A new blade will open for this server's properties Then click on the Error Details on the server properties blade's context menu (which you can access by clicking on the top right ellipse i.e. 3 dots).</p>
<p>After verifying the issue, proceed to next sections to troubleshoot.</p>
<h3>2. Check Resource Monitor</h3>
<p>Check if you see any activity in the Resource Monitor. This is also to validate if the issue is there or not. Sometimes the <strong>Low Bandwidth</strong> and <strong>multiple servers</strong> configured against one Management server can cause this issue. Ensure that this is not the scenario in your case.</p>
<p>From the Task manager, go to performance view and check for the bandwidth consumption. Then click on the &quot;Open Resource Monitor&quot; button to launch the Resource Monitor. From the CPU section in the Overview tab, select the below two services:</p>
<ul>
<li>cxps.exe</li>
<li>cbengine.exe</li>
</ul>
<p>Then click on the Network tab and see if there is any traffic going out to Azure. If the data transfer is going on without issues then you should be able to view entries against cbengine going out to a URL which will look something like &quot;blob.aaa1aaa1aa.core.windows.net&quot; and entries against the csps service .</p>
<p><img src="/images/1479242042582b713a13d26.png" alt="Resource Monitor" /></p>
<h3>3. Check ASR Infrastructure Setup</h3>
<p>First of all, check if the ASR infrastructure setup is correct and nothing is wrong there. To view this, navigate to the ASR Vault. Go to the settings and click on the &quot;Site Recovery Infrastructure&quot;. In the next blade, click on the kind of infrastructure you have setup. E.g. If you are replicating from VMWare or Physical Machines from on-premise to Azure then click on the &quot;Configuration Servers&quot; under the &quot;For VMWare &amp; Physical Machines&quot; section.</p>
<p><img src="/images/1479235374582b572e7bd28.png" alt="Site Recovery Infrastructure" /></p>
<p>Here check if the Config Server is showing as &quot;Connected&quot;. If not then the problem is in the communication between Configuration Server and the Azure. Ensure that you are able to connect to the Azure portal from the config server. Also, ensure that all the public URLs for Azure are accessible. Check this link for exact URLs: <a href="https://docs.microsoft.com/en-us/azure/site-recovery/site-recovery-best-practices#verify-url-access">Verify URL Access</a>.</p>
<p>Next, click on the configuration server. This will open another blade with details for the configuration server. Expand the section for &quot;Associated Servers&quot; as marked no. 2 in the screenshot below. Check if all the associated servers, i.e. Process Server, vCenter Server and Master Target servers are connected and showing green tick mark.</p>
<p>Next, check the configuration server health as shown at no. 3 below. Check if all the services are running and showing healthy. Ensure that you have sufficient free space on the configuration server to send the replication data. If you see any services not running then go to the next section to check and start the services on the Management Server on-premise. </p>
<p>You can try refreshing the server after making any configuration changes on it, e.g. increasing memory or freeing up disk space. Click on the &quot;Refresh Server&quot; button as shown at no. 4, at the top of the blade for the configuration server.</p>
<p><img src="/images/1479235804582b58dc26f55.png" alt="Config Server Settings" /></p>
<h3>4. Checking Services on the Management Server</h3>
<p>Check if the services on the Management Server are up and running. You need to check for the below services:</p>
<ul>
<li>InMage PushInstall</li>
<li>InMage Scout Application Service</li>
<li>InMage Scout VX Agent - Sentinel/Outpost</li>
<li>INMAGE-AppScheduler</li>
<li>Microsoft Azure Recovery Services Agent</li>
<li>Microsoft Azure Site Recovery Service</li>
<li>cxprocessserver (This is important service. It is the service for the InMage CX Process Server)</li>
<li>tmansvc (This is the service for the InMage Volsync Thread Manager Service)</li>
</ul>
<p>Start any service which is not running and check if the problem still exists. 90% of the time the problem is going to be because of something related to these services (e.g. a restart or patch stopped one of these services).</p>
<h3>5. Checking Services on the Server being replicated</h3>
<p>Check if the services on the Server being replicated are up and running. You need to check for the below services:</p>
<ul>
<li>Azure Site Recovery VSS Provider</li>
<li>InMage Scout Application Service</li>
<li>InMage Scout VX Agent - Sentinel/Outpost</li>
</ul>
<p>Start any service which is not running and check if the problem still exists.</p>
<h3>6. Verify Service Account credentials are correct and have required access</h3>
<p>The replication can stop if the service account is not correct or it doesn't have required access. Check if the service account's password expired or changed. </p>
<p>You can use the Configuration Server config tool to check and update the service accounts. This tool can be accessed from this directory path: &quot;<em>D:\Program Files (x86)\Microsoft Azure Site Recovery\home\svsystems\bin</em>&quot; where D is your install directory for ASR setup. The tool name under this directory is &quot;<strong>cspsconfigtool.exe</strong>&quot;.</p>
<p><img src="/images/1479238579582b63b3688bd.png" alt="CSPS Config Tool" /></p>
<h3>7. Check Logs</h3>
<p>There are various ASR logs that gets generated in the Management server. Two key logs that you should check are as shown below. This assumes that D is the directory where ASR is installed.</p>
<ul>
<li><strong>Monitoring Logs</strong> - These logs are located at &quot;<em>D:\Program Files (x86)\Microsoft Azure Site Recovery\home\svsystems\var</em>&quot;. Name of the file you should check is &quot;<strong>monitor_ps</strong>&quot;.</li>
<li><strong>VM-Specific ASR Logs</strong> - These logs are located at &quot;<em>D:\Program Files (x86)\Microsoft Azure Site Recovery\home\svsystems</em>&quot;. Then there will be a folder with the name as a GUID for each VM. Navigate to the folder with the Guid and try to find the folder for your VM's GUID. One indication will be the number of disks and the disk sizes. Once you have located the folder for one of the VM having replication problems. Then navigate to internal folders and locate the perf.log file for your VM's disks. Check to see if there are any errors here.</li>
</ul>
<p>These logs should give you an idea as to what may have been causing the issues.</p>
<h3>In Conclusion</h3>
<p>After all these steps and any changes you should Refresh the Configuration Server as shown in the point 3 above. </p>
<p>Let me know if this blog helped in your scenario. </p>]]></description>
<link>http://HarvestingClouds.com/post/troubleshooting-azure-site-recovery-asr-data-replication-not-working</link>
<pubDate>Mon, 14 Nov 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>General Availability - Azure Active Directory (AD) Domain Services</title>
<description><![CDATA[<p><strong>Azure Active Directory Domain Services</strong> are here to revolutionize the way Domain Services are used. They are here to reduce even further the infrastructure management for IT Administrators. </p>
<h3>What is this service</h3>
<p>Using this service you can now setup domains without having to setup the domain controllers. You can set up your Domain in Azure using Domain Services and then you can have virtual machines joining to that domain. At no point, you need to set up any domain controllers. You can also use Group Policy with this service to securely administer your domain joined infrastructure.</p>
<p>This service benefits all customers. For <strong>enterprise customers</strong>, they get the familiar enterprise grade service. For <strong>medium to small business customers</strong>, the service makes even more sense as they get enterprise level service for a smaller price due to small infrastructure and thus a small number of objects in AD.</p>
<p>This service is out of the box a highly available service. It is hosted in globally distributed datacenters.</p>
<h3>When is this service available</h3>
<p>This service is now <strong>Generally Available</strong>. The pricing for this service will start from 1st December 2016.</p>
<p>The payment model is Pay-As-You-Go. The usage will be charged per hour. The chargeback will be based on the total number of AD Objects in your AD Tenant. These objects include users, groups, and domain-joined computers. Directory size and hours are calculated and emitted daily. Usage is prorated to the minute.</p>
<p>Currently, there are 3 tiers. </p>
<ul>
<li>Less than 25,000 directory objects</li>
<li>25,001 to 100,000 directory objects</li>
<li>more than 100,000 directory objects</li>
</ul>
<h3>Is this service available in my region</h3>
<p>At the time of writing of this blog post, this service is available in the following regions:</p>
<ul>
<li>East US</li>
<li>East US 2</li>
<li>Central US</li>
<li>South Central US</li>
<li>West US</li>
</ul>
<p>Check this link for most updated availability: <a href="https://azure.microsoft.com/en-us/regions/services/">Azure Products by Region</a></p>
<p>Also, check the official ADDS page here: <a href="https://azure.microsoft.com/en-us/services/active-directory-ds/">Azure Active Directory Domain Services</a>
And, check the upto date pricing here: <a href="https://azure.microsoft.com/en-us/pricing/details/active-directory-ds/">Pricing</a></p>
<p>Now it is time for you to go and try this new service for yourself and enjoy it's benefits!</p>]]></description>
<link>http://HarvestingClouds.com/post/general-availability-azure-active-directory-ad-domain-services</link>
<pubDate>Thu, 27 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Suspending and Resuming Azure Site Recovery (ASR) Replication on a single or multiple servers</title>
<description><![CDATA[<p>Let us assume that you have enabled the Azure Site Recovery (ASR) replication on various servers. These servers can be:</p>
<ul>
<li>On Premise VMWare VMs</li>
<li>On Premise Physical Servers</li>
<li>Azure ASM (older portal) VMs</li>
</ul>
<p>The purpose could be anything from setting up Disaster Recovery for your infrastructure or using ASR for Migrating workloads from on-premise to Azure. For any reason, you may need to suspend and resume ASR replication on one or more target servers.</p>
<p>Currently, ASR does not have the feature to allow you to suspend and resume the ASR replication. But you can do this manually as easily. </p>
<p>To <strong>Suspend</strong> the ASR replication on a particular server, all you need to do is:</p>
<ol>
<li>Log into the server on which ASR replication is currently going on and you want to suspend the replication.</li>
<li>Open the Services (Run -&gt; services.msc)</li>
<li>Locate the following services and Stop these services.
<ul>
<li>Azure Site Recovery VSS Provider</li>
<li>InMage Scout Application Service</li>
<li>InMage Scout VX Agent - Sentinel/Outpost</li>
</ul></li>
</ol>
<p>Checkout these servers below:</p>
<p><img src="/images/1477354219580ea2eb7f4e6.png" alt="Services 1" /></p>
<p><img src="/images/1477354229580ea2f52b728.png" alt="Services 2" /></p>
<p>To <strong>Resume</strong> the ASR replication, just do the opposite, i.e. Log into the server and Start these services. </p>
<p>Until Azure adds this feature directly in the portal, this easy manual step is the workaround for suspending and resuming the replication on servers.</p>]]></description>
<link>http://HarvestingClouds.com/post/suspending-and-resuming-azure-site-recovery-asr-replication-on-a-single-or-multiple-servers</link>
<pubDate>Tue, 25 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Creating ASR Template from an existing Azure Infrastructure and Modifying It</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>This blog post is for you if:</p>
<ul>
<li>You want to backup an Infrastructure configuration/setup in Azure and want to redeploy it to another environment then this blog is for you. </li>
<li>You want to create similar infrastructure as one of existing deployments in Azure</li>
<li>You want to modify the configurations of existing Azure IaaS infrastructure and redeploy various elements</li>
</ul>
<p>This <strong>Power Tip</strong> is really easy if you know just the option. </p>
<ol>
<li>If you want to make the template for all the resources in a Resource Group in Azure, then go to the properties of the Resource Group and find the option for &quot;<strong>Automation Script</strong>&quot;.</li>
<li>If you want to get the template only for a particular resource, then navigate to that resource in the Azure Portal and then open it's settings. You will find the same &quot;<strong>Automation Script</strong>&quot; option. </li>
</ol>
<p>You can check this option in the below screenshot.</p>
<p><img src="/images/1477349409580e902123ab1.png" alt="Automation Script" /></p>
<p>Once you click on the Automation Script option in the settings (of a resource group or a resource) then you will be presented with the complete JSON template along with the JSON outline on the right side (marked 2 above in the image).</p>
<p>You have various options for the actions to take on the template (marked 3 in the image above):</p>
<ul>
<li>You can download the template</li>
<li>Add to Library to deploy the same resources again and again in your subscription</li>
<li>To directly deploy the resources again with the modifications you make. </li>
</ul>
<p>Normally, you would download the template to make edits to the same. After downloading, you should start cleaning up the template. There are only 4 major tasks that you need to perform as part of the cleanup:</p>
<ol>
<li>Remove any <strong>hard-coded values</strong> for various dependent resources e.g. NIC for a VM, VHD for a VM etc.</li>
<li>Remove any resources and dependent parameters that you don't need.</li>
<li>Create <strong>Parameters</strong> for the values you want to change for each deployment and want the end user to provide during the deployment.</li>
<li>Create <strong>Variables</strong> for the values which can have fixed values but are being used at multiple locations in your template.</li>
</ol>
<p>That's all there is to it. Using this tip you can spearhead your ARM Template developments. You don't need to start from scratch and can base your templates on the existing deployments.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-creating-asr-template-from-an-existing-azure-infrastructure-and-modifying-it</link>
<pubDate>Mon, 24 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step Azure Resource Manager (ARM) Templates - Index</title>
<description><![CDATA[<p><strong>Azure Resource Manager (ARM) Template</strong> is a JavaScript Object Notation (JSON) file that defines one or more resources to deploy to a resource group. It also defines the dependencies between the deployed resources. The template can be used to deploy the resources consistently and repeatedly.</p>
<p>ARM Templates can be used for the deployment of resources on both Azure and Azure Stack. Using these templates for all deployments provides you with various <strong>benefits</strong> including:</p>
<ul>
<li><strong>Declarative Deployment</strong> – All you need to do is declare what you need to deploy. You don't need to create any complex rules or write lengthy scripts.</li>
<li><strong>Idempotency</strong> – You can deploy the same template over and over again without affecting the current resources.</li>
<li><strong>Predictability</strong> - Using Templates you can have accurate predictability when performing large deployments. You reduce any manual errors.</li>
<li><strong>Repitition without Errors</strong> - You can deploy the same infrastructure over and over again (e.g. in Dev-test environments and then in production).</li>
</ul>
<p>This series of posts try to decode and understand the ARM Templates &quot;Step By Step&quot;.</p>
<p>This post is an index of all the posts, in sequence, for understanding the Azure Resource Manager (ARM) Templates. This post will be updated regularly as more posts on this topic are added.</p>
<ol>
<li><a href="#">Index</a> </li>
<li><a href="/post/step-by-step-arm-templates-json-101-for-it-administrators/">JSON 101 for IT Administrators</a></li>
<li><a href="/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-all-components/">What is in an ARM Template - Understanding All Components</a></li>
<li><a href="/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-2-parameters/">What is in an ARM Template - Understanding Components 2 - Parameters</a></li>
<li><a href="/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-3-variables/">What is in an ARM Template - Understanding Components 3 - Variables</a></li>
<li><a href="/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-4-resources/">What is in an ARM Template - Understanding Components 4 - Resources</a></li>
<li><a href="/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-5-outputs/">What is in an ARM Template - Understanding Components 5 - Outputs</a></li>
<li><a href="/post/step-by-step-arm-templates-helper-functions/">Helper Functions in ARM Templates</a></li>
<li><a href="/post/step-by-step-arm-templates-building-your-first-arm-template/">Building your first ARM Template</a></li>
<li><a href="/post/step-by-step-arm-templates-deploying-template-using-azure-portal/">Deploying Template Using Azure Portal</a></li>
<li><a href="/post/step-by-step-arm-templates-deploying-template-using-azure-powershell/">Deploying Template Using Azure PowerShell</a></li>
<li><a href="/post/step-by-step-arm-templates-creating-parameters-file-for-an-arm-template/">Creating Parameters file for an ARM Template</a></li>
<li><a href="/post/step-by-step-arm-templates-authoring-arm-templates-using-visual-studio/">Authoring ARM Templates using Visual Studio</a></li>
<li><a href="/post/step-by-step-arm-templates-deploying-arm-templates-using-visual-studio/">Deploying ARM Templates using Visual Studio</a></li>
<li><a href="/post/step-by-step-arm-templates-iterating-and-creating-multiple-instances-of-a-resource/">Iterating and creating multiple instances of a resource</a></li>
<li><a href="/post/step-by-step-arm-templates-visualizing-arm-templates-and-generating-diagrams/">Visualizing ARM Templates and Generating Diagrams</a></li>
<li><a href="/post/step-by-step-arm-templates-using-key-vault-to-securely-provide-information-in-arm-templates/">Using Key Vault to Securely Provide Information in ARM Templates</a></li>
<li><a href="/post/step-by-step-arm-templates-providing-powershell-scripts-to-run-after-vm-deployment-via-arm-template/">Providing PowerShell Scripts to Run after VM deployment via ARM Template</a></li>
<li><a href="/post/step-by-step-arm-templates-deploying-a-windows-vm-with-oms-integration/">Deploying a Windows VM with OMS integration</a></li>
<li><a href="/post/step-by-step-arm-templates-creating-asr-template-from-an-existing-azure-infrastructure-and-modifying-it/">Creating ASR Template from an existing Azure Infrastructure and Modifying It</a></li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-azure-resource-manager-arm-templates-index</link>
<pubDate>Fri, 21 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Providing PowerShell Scripts to Run after VM deployment via ARM Template</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>By providing PowerShell Scripts to Run after VM deployment via ARM Template, you can accomplish various activities. </p>
<ul>
<li>You can setup different features and roles on the VM. </li>
<li>You can setup web server. </li>
<li>You can setup SQL Database and configure it.</li>
<li>You can configure custom policies</li>
<li>And so on...</li>
</ul>
<p>You first need to have PowerShell script files uploaded to a storage account.
To do this you add an <strong>Extension resource</strong> (<em>Microsoft.Compute/virtualMachines/extensions</em>) nested inside a VM. This extension resource should be of type &quot;<strong>CustomScriptExtension</strong>&quot;. You provide the URLs to the PowerShell scripts inside this custom script extension.</p>
<h3>Preparation</h3>
<p>As part of the preparation process you need to:</p>
<ul>
<li>Ensure that the PowerShell scripts are uploaded to the Storage Account and that you have the complete URL to the blob. </li>
<li>Or you can upload the scripts to the GitHub and get the Raw file URL</li>
<li>If there are more than one scripts then there should be one master script amongst all ps1 files which will internally invoke other files. This master file will be triggered via the template. Information of all file URLs will also be provided via the Template</li>
</ul>
<h3>Providing and configuring Scripts to Run After VM Deployment</h3>
<p>Define the below resource to provide PowerShell scripts to be run after VM deployment:</p>
<pre><code>{
   "type": "Microsoft.Compute/virtualMachines/extensions",
   "name": "MyCustomScriptExtension",
   "apiVersion": "2015-05-01-preview",
   "location": "[parameters('location')]",
   "dependsOn": [
       "[concat('Microsoft.Compute/virtualMachines/',parameters('vmName'))]"
   ],
   "properties": {
       "publisher": "Microsoft.Compute",
       "type": "CustomScriptExtension",
       "typeHandlerVersion": "1.7",
       "autoUpgradeMinorVersion":true,
       "settings": {
           "fileUris": [
           "http://Yourstorageaccount.blob.core.windows.net/customscriptfiles/start.ps1",
           "http://Yourstorageaccount.blob.core.windows.net/customscriptfiles/secondaryScript.ps1",

       ],
       "commandToExecute": "powershell.exe -ExecutionPolicy Unrestricted -File start.ps1"
     }
   }
 }</code></pre>
<p><strong>How it works:</strong></p>
<ul>
<li>Both the files i.e. start.ps1 and secondaryScript.ps1 are picked up from the storage account after VM deployment. Ensure to replace the URLs with your actual storage account blob URLs. You can add more files if needed.</li>
<li>The &quot;start.ps1&quot; is the main powerShell script which should be invoking the secondaryScript.ps1 internally</li>
<li>CommandToExecutre property is used to invoke the start.ps1 powerShell script on the deployed VM</li>
</ul>
<h3>Passing Parameters to the PowerShell Script dynamically</h3>
<p>To pass the parameters to the PowerShell script use commandToExecute property. </p>
<p>One such example to pass the parameters is shown below:</p>
<pre><code>"commandToExecute": "[concat('powershell.exe -ExecutionPolicy Unrestricted -File start.ps1', ' -domainName ', parameters('domainNameParameter')]"</code></pre>
<p>Note the use of &quot;concat&quot; helper function to create the value of the &quot;commandToExecute&quot;. Also note that there is starting and trailing space in the second argument of the concat i.e. &quot; -domainName &quot;.</p>
<p>The parameter &quot;domainNameParameter&quot; should already be defined in the template in the parameters section. If the value of parameter &quot;domainNameParameter&quot; is &quot;testdomain.com&quot; then the dynamically generated command will become:</p>
<pre><code>powershell.exe -ExecutionPolicy Unrestricted -File start.ps1 -domainName testdomain.com</code></pre>
<h3>Securing the Access to the PowerShell Script File in Storage account</h3>
<p>Let us assume you want to deploy Windows VM with Protected settings. Then use the below sample to provide the PowerShell files.</p>
<pre><code>{
    "publisher": "Microsoft.Compute",
    "type": "CustomScriptExtension",
    "typeHandlerVersion": "1.7",
    "settings": {
        "fileUris": [
            "http: //Yourstorageaccount.blob.core.windows.net/customscriptfiles/start.ps1"
        ]
    },
    "protectedSettings": {
        "commandToExecute": "powershell.exe -ExecutionPolicy Unrestricted -start.ps1",
        "storageAccountName": "yourStorageAccountName",
        "storageAccountKey": "yourStorageAccountKey"
    }
}</code></pre>
<p>Note the use of &quot;protectedSettings&quot; above. This time you also specify the Storage Account Name and the Storage Account Key.</p>
<p>You can also refer the official documentation here: <a href="https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-windows-extensions-customscript/">Windows VM Custom Script extensions with Azure Resource Manager templates</a>.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-providing-powershell-scripts-to-run-after-vm-deployment-via-arm-template</link>
<pubDate>Wed, 19 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Using Key Vault to Securely Provide Information in ARM Templates</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>When providing passwords and other secure and confidential information in ARM Templates, you need to ensure that you don't hard code these values anywhere. You don't need to compromise the security of the system while trying to automate deployments. Your end goal is to try to automate as much as possible and reduce manual involvement. </p>
<p><strong>Key Vaults</strong> are there to solve this problem without compromising any security. In fact, they make the whole solution more secure with least manual intervention.</p>
<h3>Setting up the Key Vault</h3>
<p>We first need to setup the Key Vault in Azure to be able to use it via ARM Template parameters.</p>
<ol>
    <li>
<b>Create a Key Vault in Azure</b> by going to <i>New -> Security + Identity -> Key Vault</i>. Provide a name, subscription, resource group etc. and provision the Key Vault. Once it is created navigate to it by clicking on "More Services" and searching for Key Vault. Click on the name of the vault you created. E.g. In this example we have named the key vault to "TestKeyVault101".

<br />
<b>Note</b> that this feature is in Preview  at the time of writing of this blog.

    </li>

    <li>
        Next, we need to <b>Add a Secret</b> in the key vault. Click on the Secrets and then the + Add button at the top, as shown below:
<br /><br />
<img alt="Adding Secret" src="/images/1476818644580676d447b16.png" />
<br /><br />
Next, in the "Create a secret" blade, set the Upload Options to Manual. Provide a name and value to the secret. Value is the password you want to securely save.
Ensure that the Enabled is set to Yes. Optionally you can set the activation and expiration dates. In this example, we are setting the Secret Name to "DefaultAdminPasswordSecret".
<br /><br />
<img alt="Creating Secret" src="/images/1476818650580676da89fee.png" />
    </li>

    <li>
        Next, we will set the <b>Access Policies </b> to provide access to the user under the context of which the template will be deployed. This is the user which will be accessing the Key Vault. Go to Key Vault settings and select Access Policies. Add the new user as shown below:
<br /><br />
<img alt="Access Policies" src="/images/14768190995806789bad0c4.png" />
<br /><br />

    </li>

    <li>
        Next, we will set the <b>Advanced Access Policies </b> to indicate that this key vault can be accessed via ARM Templates. Go to Key Vault settings and select Advanced Access Policies. Ensure that the checkbox for "<i>Enable access to Azure Resource Manager for template deployment</i>" is checked as shown below:
<br /><br />
<img alt="Access Policies" src="/images/1476819105580678a1cfc9f.png" />
<br /> <br />

    </li>

</ol>
<p>We are now all set with our Key Vault. Next, we will be using the secret we created to set the local Administrator user's password.</p>
<h3>Using the Key Vault Secret in ARM Template</h3>
<p>Let us assume that you have a JSON ARM Template which deploys a VM. One of the parameters in this template is AdminPassword. You want to use the Key Vault Secret to provide the value for this parameter. </p>
<p><strong>First</strong>, ensure that the parameter is declared as <i>securestring</i> as shown below:</p>
<pre><code>"adminPassword": {
    "type": "securestring",
    "metadata": {
        "description": "Password for local admin account."
    }
}</code></pre>
<p><strong>Next</strong>, we need to use the parameters file for this template. If you don't have one already create a new one. We can provide the reference to the Key Vault Secret as the value of admin user's password parameter in this file. General Syntax of providing reference is as shown follow:</p>
<pre><code>"adminPassword": {
  "reference": {
    "keyVault": {
      "id": "Key Vault Id Here"
    },
    "secretName": "Name of the secret in Azure Key Vault"
  }
}</code></pre>
<p>Now the ID in the above Syntax can be provided as:</p>
<p><em>/subscriptions/{guid}/resourceGroups/{group-name}/providers/Microsoft.KeyVault/vaults/{vault-name}</em>. </p>
<p>Note to replace the <em>{guid}</em> with actual GUID for the subscription (without the curly braces), replace <em>{group-name}</em> with the actual name of the resource group and <em>{vault-name}</em> with the actual name of the Key Vault.</p>
<p>You can also find the Resource ID for the Key Vault by navigating to it in the Azure Portal and then checking it's properties as shown below:</p>
<p><img src="/images/147682308858068830f2f8e.png" alt="Key Vault Resource ID" /></p>
<p>The complete parameter file looks like below:</p>
<pre><code>{
  "$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "OtherParameter": {
      "value": "otherValue"
    },
    "adminPassword": {
      "reference": {
        "keyVault": {
          "id": "/subscriptions/11111aaa-1a11-1a11-a1aa-1a1111a111a1/resourceGroups/TestRG101/providers/Microsoft.KeyVault/vaults/TestKeyVault101"
        },
        "secretName": "DefaultAdminPasswordSecret"
      }
    }
  }
}</code></pre>
<p>Next, deploy the template using PowerShell and pass this parameters file as explained here: <a href="/post/step-by-step-arm-templates-deploying-template-using-azure-powershell/">Deploying Template Using Azure PowerShell</a>. </p>
<p>Example PowerShell cmdlet to deploy will look like:</p>
<pre><code>New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName TestResourceGroup01 -TemplateFile .\TemplateFile.json -TemplateParameterFile .\ParametersFile.json</code></pre>
<p>Now that you know how to use values from Key Vaults, you can make the automated deployment of resources more secure in your environment.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-using-key-vault-to-securely-provide-information-in-arm-templates</link>
<pubDate>Tue, 18 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Visualizing ARM Templates and Generating Diagrams</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>When developing ARM Templates, from time to time you will need to:</p>
<ul>
<li>Visualize your ARM Templates</li>
<li>Generate Diagrams for your ARM Templates</li>
</ul>
<p>Microsoft has provided an Open Source tool for this named &quot;ARMVIZ&quot; (short for ARM Visualizer). This tool can be accessed by navigating to the below URL:</p>
<p><a href="http://armviz.io/"><a href="http://armviz.io/">http://armviz.io/</a></a></p>
<h3>Navigating ARMVIZ</h3>
<p>ARMVIZ is a nice in-browser application to visualize all the components in a template. It also shows the dependencies between various components. Using this web application you can:</p>
<ul>
<li>Either visualize your own developed template,</li>
<li>Or inspect existing templates on GitHub</li>
</ul>
<p>Let's take a quick tour of the interface:</p>
<p><img src="/images/14768067785806487ad77dd.png" alt="ARMVIZ Interface" /></p>
<p>I have numbered various elements of the interface in the above diagram. Let's quickly review these elements:</p>
<ol>

    <li>
              <b>Designer</b> - This is represented by an "eye" icon on the left bar. This should be selected by default. If you are in the editor mode then you can click this and the diagram will be shown in the middle portion of the screen.
    </li>

    <li>
              <b>Editor</b> - This is represented by "</>" text for code on the left bar. Clicking on this will take you to the editor portion of the ARMVIZ tool. In this area, you can edit your template while still in the tool. You can add or remove components. You can even edit the components or add dependencies.
    </li>

     <li>
              <b>Canvas area</b> - This is the main screen (the middle area) where the template is displayed.
    </li>

    <li>
              <b>File Menu</b> - This is the main and simple menu in the whole web application in the top bar. It has two options:
              <ol type="a">
                       <li>
                             <b>Open Local Template</b> - You can open an ARM Template JSON from your local computer to visualize using this menu option.
                       </li>
                       <li>
                             <b>Download Template</b> - You can download the current template by using this menu option.
                       </li>
              </ol>
    </li>

    <li>
              <b>Quickstart ARM Templates</b> - This is the link to external library of Quickstart ARM Templates on GitHub. These starter templates can help you save a lot of time. Instead of starting from scratch you can use these templates to fasten the ARM Templates Development.
    </li>

</ol>
<p>This is how the Editor portion of the tool looks like. Use this area to edit or update your template.
<strong>Note:</strong> If there will be mistakes, such as missing parenthesis in your template, the designer will not show any diagram. </p>
<p><img src="/images/147680732358064a9ba1310.png" alt="Editor Area" /></p>
<p>You can zoom into and zoom out of your template diagram by rolling the mouse wheel. You can also drag and reposition various elements.
Take a screenshot once you have repositioned the elements as per your requirements and have zoomed into an appropriate level.</p>
<p>Below screenshot is taken from a much more complex template.</p>
<p><img src="/images/1476806791580648877fed3.png" alt="Complex ARM Template Diagram" /></p>
<p>In conclusion, ARMVIZ can enable you to easily visualize your ARM Templates. It can empower you to generate diagrams for your documentation and to present to your team.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-visualizing-arm-templates-and-generating-diagrams</link>
<pubDate>Mon, 17 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Deploying a Windows VM with OMS integration</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>You can deploy a Windows VM with OMS integration. You can have the OMS extension installed. And then you can onboard the VM to a specified workspace. </p>
<h3>Prerequisites</h3>
<p>You need already have an OMS workspace setup in your subscription. You need to have the following information about this OMS Workspace:</p>
<ol>
<li>OMS workspace ID</li>
<li>OMS workspace Key</li>
</ol>
<p>You may obtain the workspace ID and key by going to the Connected Sources tab in the Settings page in the OMS Portal or to the Direct Agent blade in the Azure portal.</p>
<p>In the Azure Portal go to the Log Analytics -&gt; Click on the OMS Workspace you want to use. Click on the &quot;OMS Portal&quot; to navigate to the OMS Portal.</p>
<p><img src="/images/14768464545806e3762d97c.png" alt="Link to OMS Portal" /></p>
<p>In the OMS portal, navigate to the Settings.</p>
<p><img src="/images/14768422705806d31e60125.png" alt="OMS Portal Settings" /></p>
<p>In Settings, go to the Connected Sources -&gt; Windows Servers. Note the Workspace ID and the Primary Key as shown below:</p>
<p><img src="/images/14768422755806d323b2caa.png" alt="OMS Portal ID and Key" /></p>
<h3>ARM Template Sections for OMS integration</h3>
<p>Within the VM resource, you need to define the OMS extension as shown below:</p>
<pre><code>  "resources": [
    {
      "type": "extensions",
      "name": "Microsoft.EnterpriseCloud.Monitoring",
      "apiVersion": "[variables('apiVersion')]",
      "location": "[resourceGroup().location]",
      "dependsOn": [
        "[concat('Microsoft.Compute/virtualMachines/', variables('vmName'))]"
      ],
      "properties": {
        "publisher": "Microsoft.EnterpriseCloud.Monitoring",
        "type": "MicrosoftMonitoringAgent",
        "typeHandlerVersion": "1.0",
        "autoUpgradeMinorVersion": true,
        "settings": {
          "workspaceId": "Your Workspace ID Here"
        },
        "protectedSettings": {
          "workspaceKey": "Your Workspace Key Here"
        }
      }
    }
  ]</code></pre>
<p>The above configures the OMS on the VM. Note that you need the nested extension resource of type &quot;Microsoft.EnterpriseCloud.Monitoring&quot;. </p>
<p>Also, note the Workspace Id and Key in the template section above. Enter the values as per your environment which we found in the Prerequisites section above. </p>
<h3>Providing the Workspace ID and Workspace Key Dynamically</h3>
<p>You can also provide the Workspace Id and the Workspace Key dynamically by only using the OMS Workspace name. Follow the below sample. Note the use of reference, listKeys, and resourceId helper functions.</p>
<pre><code>"settings": {
          "workspaceId": "[reference(resourceId('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName')), '2015-03-20').customerId]"
        },
        "protectedSettings": {
          "workspaceKey": "[listKeys(resourceId('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName')), '2015-03-20').primarySharedKey]"
        }</code></pre>
<p><strong>Reference:</strong> You can check the complete quick starter template for OMS integration here: <a href="https://github.com/Azure/azure-quickstart-templates/tree/master/201-oms-extension-windows-vm">GitHub Sample - Deployment of a Windows VM with OMS Extension</a></p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-deploying-a-windows-vm-with-oms-integration</link>
<pubDate>Sun, 16 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Automation Preview Solution - Start/Stop VMs during off-hours </title>
<description><![CDATA[<p>Starting and Stopping VMs during off-hours can mean lots of cost optimizations for you. We have been implementing this via custom Runbooks and schedules for various customers. Now there is out of the box support for this within Azure. The feature is currently in Preview but you can build on this.</p>
<h3>What do I need - Prerequisites</h3>
<p>Before beginning check that your region has this feature available. Just like with any other automation solution, you will need to have:</p>
<ol>
<li>OMS Workspace (or you can create new while adding the solution)</li>
<li>Automation Account (or you can create new)</li>
<li>Azure Run As account (and not the Microsoft Account)</li>
<li>For email support, Office 365 business-class subscription is required</li>
</ol>
<p>Note: The VMs that you want to manage should be in the same subscription and resource group as where the Automation account resides.</p>
<h3>How to Add</h3>
<p>To Add the solution, click on &quot;+ New&quot; symbol and search for &quot;Start/Stop VMs during off-hours&quot;. You will find the below solution available to be created:</p>
<p><img src="/images/1476808753580650317ab68.png" alt="Start and Stop VMs Preview Solution" /></p>
<h3>What does it Contain</h3>
<p>The solution is a combination of various automation assets:</p>
<ol>
<li>Runbooks</li>
<li>Variables</li>
<li>Schedules</li>
<li>Credentials</li>
</ol>
<p>You can change some configurations during and some after the deployment. </p>
<p>Find out more here: <a href="https://azure.microsoft.com/en-us/documentation/articles/automation-solution-vm-management/">Start/Stop VMs during off-hours [Preview]</a></p>]]></description>
<link>http://HarvestingClouds.com/post/azure-automation-preview-solution-start-stop-vms-during-off-hours</link>
<pubDate>Tue, 11 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure RemoteApp is going away. New Purchases in portal are now stopped </title>
<description><![CDATA[<p>Today I got an email saying &quot;Action recommended: Deleting unnecessary RemoteApp collections can save you money&quot;. This email is also a reminder from Microsoft that the RemoteApp is going away. We need to plan for the migration for the existing RemoteApps to other platforms.</p>
<h3>What this means</h3>
<p>This means various things to you. Most prominently:</p>
<ul>
<li>Over the next year, the support for Remote App is going away</li>
<li>You need to plan and migrate the existing RemoteApp application to other platforms</li>
<li>New Purchases in the Azure portal for RemoteApp are no longer available</li>
</ul>
<h3>When is the service coming to Stop</h3>
<p>The service will have support through <strong>August 31st, 2017</strong>. That's when this service will come to a stop. New service purchase was stopped effective October 1st, 2016.</p>
<h3>What are my Options</h3>
<p>You have various options for migration. The option being <strong>recommended</strong> by Microsoft is using &quot;<strong>Citrix XenApp express</strong>&quot;. In fact, Microsoft is partnering up with Citrix on this. This service is not yet available and is currently under development. As this will be the native option in Azure this will be your best bet once it is announced. You can learn more about this solution here on Citrix site: <a href="https://www.citrix.com/global-partners/microsoft/remote-app.html">Citrix and Microsoft</a></p>
<p>The second option is to use Remote Desktop Services (RDS) deployed on Azure IaaS. This means to set up the infrastructure yourself and then deploy and host the RDS solution on that infrastructure in Azure. You can know more about the steps here: <a href="https://technet.microsoft.com/en-us/windows-server-docs/compute/remote-desktop-services/host-desktops-and-apps-in-remote-desktop-services">Host desktops and apps in Remote Desktop Services on Azure</a></p>
<p>Another option is to use  hosted solutions from various 3rd party vendors. You can find such solution from various partners from the Azure marketplace. You can also read and get to know the complete list of these hosting partners here: <a href="https://technet.microsoft.com/en-us/windows-server-docs/compute/remote-desktop-services/rds-hosting-partners">RDS - Partners for hosting desktops and apps</a></p>
<p><strong>In conclusion,</strong> I will recommend to wait and look out for Citrix XenApp express solution. If you need new remote application solutions then you need to either deploy your own solution or use one of the hosted solutions by 3rd party vendors. You can read more about the official announcement here: <a href="https://blogs.technet.microsoft.com/enterprisemobility/2016/08/12/application-remoting-and-the-cloud/?WT.mc_id=azurebg_email_Trans_1218_No_Usage_Azure_RemoteApp">Application Remoting and the Cloud</a></p>]]></description>
<link>http://HarvestingClouds.com/post/azure-remoteapp-is-going-away-new-purchases-in-portal-are-now-stopped</link>
<pubDate>Mon, 10 Oct 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Iterating and creating multiple instances of a resource</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>In Azure Resource Manager (ARM) templates, you can define the variable once and then iterate/loop over that definition and create multiple instances of that resource.
There are 3 special constructs in ARM templates to help you with this. </p>
<p>These <strong>constructs</strong> are:</p>
<ul>
<li><strong>copy</strong> - This is a property that is defined within the resource. This is the construct which when defined indicates that this resource needs to be looped over and created multiple times. It also specifies the number of times to iterate via &quot;count&quot; property.</li>
<li><strong>copyIndex()</strong> - Used to access the current iteration value. Its value for the first iteration is <strong>zero</strong>. For the second iteration, its value is 1 and so on... You can pass it an integer (number) as a parameter. Whatever number you pass that will become the value for the first iteration and subsequent iterations. E.g. copyIndex(20) will compute to 20 in the first iteration, 21 in the second iteration and so on.</li>
<li><strong>length</strong> - This is the method of arrays. It computes the number of elements in an array. It can be used to set the &quot;count&quot; property of &quot;copy&quot; construct.</li>
</ul>
<p><strong>Note:</strong> Arrays are always <strong>zero indexed</strong>. What that means is that the first element of the array is indexed at 0, the second element of the array is indexed at 1, and so on...</p>
<h3>1. Simple Example</h3>
<p>Let us understand these constructs using an example.</p>
<pre><code>"parameters": { 
  "count": { 
    "type": "int", 
    "defaultValue": 3 
  } 
}, 
"resources": [ 
  { 
      "name": "[concat('HarvestingClouds-', copyIndex(100))]", 
      "type": "Microsoft.Web/sites", 
      "location": "Central US", 
      "apiVersion": "2015-08-01",
      "copy": { 
         "name": "websitescopy", 
         "count": "[parameters('count')]" 
      }, 
      "properties": {
          "serverFarmId": "hostingPlanName"
      }
  } 
]</code></pre>
<p>The above example will <strong>result</strong> in creation of below 3 web apps in Azure:</p>
<ul>
<li>HarvestingClouds-100</li>
<li>HarvestingClouds-101</li>
<li>HarvestingClouds-102</li>
</ul>
<p>Note the usage of &quot;copy&quot; property in the above code example:</p>
<pre><code> "copy": { 
             "name": "websitescopy", 
             "count": "[parameters('count')]" 
          }</code></pre>
<p>As you can notice above, the value of this property is another JSON object. This object has further two properties: </p>
<ul>
<li>First is the name property, which provides the name to the looping construct. This can be any meaningful name. </li>
<li>The second property is the count, which specifies how many times this resource definition should be deployed. Note that the value is set to the parameter named &quot;count&quot;. The name of the parameter can be anything but the value of the parameter has to be a number (i.e. an integer).</li>
</ul>
<p>Next, note how the name of the web application is constructed using the copyIndex() helper function.</p>
<pre><code>"name": "[concat('HarvestingClouds-', copyIndex(100))]"</code></pre>
<p>The above value uses two helper functions. First is the &quot;concat()&quot; which is concatenating (i.e. joining) two values. First value is the prefix string &quot;HarvestingClouds-&quot;. Second parameter and the second helper function is <code>copyIndex(100)</code>. This specifies the current iteration value, which is offset with 100. So for the first iteration, the value will be 0+100 = 100, for the second iteration the value will be 1+100 = 101 and so on...</p>
<h3>2. Example with an Array</h3>
<p>Let's assume that you want to deploy multiple web apps for different purposes. You need one web app for Production, one for Staging or testing and one for Development. You want to name the web apps deployed with the purpose concatenated.
The below example uses an array to set the values for the web app name:</p>
<pre><code>"parameters": { 
  "purpose": { 
     "type": "array", 
         "defaultValue": [ 
         "Production", 
         "Staging", 
         "Development" 
      ] 
  }
}, 
"resources": [ 
  { 
      "name": "[concat('HarvestingClouds-', parameters('purpose')[copyIndex()])]", 
      "type": "Microsoft.Web/sites", 
      "location": "Central US", 
      "apiVersion": "2015-08-01",
      "copy": { 
         "name": "websitescopy", 
         "count": "[length(parameters('purpose'))]" 
      }, 
      "properties": {
          "serverFarmId": "hostingPlanName"
      } 
  } 
]</code></pre>
<p>The <strong>output</strong> of the above sample will be 3 web apps deployed in Azure with following names:</p>
<ul>
<li>HarvestingClouds-Production</li>
<li>HarvestingClouds-Staging</li>
<li>HarvestingClouds-Development</li>
</ul>
<p>Note in the above code sample that the parameter &quot;purpose&quot; is an array with 3 values i.e. Production, Staging, and Development. Then in the &quot;copy&quot; construct the count property is set using the length of this array as shown below. As there are 3 elements in the array, the value of count will be 3 and the resource will be deployed 3 times.</p>
<pre><code>"count": "[length(parameters('purpose'))]" </code></pre>
<p>Next, the name of the web app is set using the copyIndex() and the array itself as shown below:</p>
<pre><code>"name": "[concat('HarvestingClouds-', parameters('purpose')[copyIndex()])]"</code></pre>
<p>As earlier, it uses concat helper function to add two strings. The first string is simple text i.e. &quot;HarvestingClouds-&quot;, which becomes the prefix for the web app name. Second is finding out the value of the array based on the current iteration. For the first iteration, copyIndex() will compute to zero, therefore the second parameter becomes <code>parameters('purpose')[0]</code>. This will fetch the 0th element of the array which is Production. Similarly, for the second iteration, copyIndex() will compute to 1, therefore the second parameter becomes <code>parameters('purpose')[1]</code>. This will fetch the second element of the array (or element at index value 1) which is Staging, and so on...</p>
<h3>3. Depending upon resources being deployed by the copy Loop</h3>
<p>Let's assume you want to deploy a storage account. But you want to deploy it only after all the web apps are deployed by the loop. In this scenario, the dependsOn property of a resource is set to the name of the &quot;copy&quot; property of the resource, rather than the resource itself.</p>
<pre><code>    {
        "apiVersion": "2015-06-15",
        "type": "Microsoft.Storage/storageAccounts",
        "name": "teststorage101",
        "location": "[resourceGroup().location]",
        "properties": {
            "accountType": "Standard_LRS"
         }
       "dependsOn": ["websitescopy"]
    }</code></pre>
<p>Note above that the dependsOn property is set to the name property of the copy in the earlier web app example. This storage account will not be deployed until all 3 web apps are not deployed.</p>
<h3>4. Limitations</h3>
<p>There are two limitations on the use of the copy to iterate and create multiple resource instances:</p>
<ol>
<li><strong>Nested Resources</strong> - You cannot use a copy loop for a nested resource. If you need to create multiple instances of a resource that you typically define as nested within another resource, you must instead create the resource as a top-level resource and define the relationship with the parent resource through the <strong>type</strong> and <strong>name</strong> properties.</li>
<li><strong>Looping Properties of a Resource</strong> - You can only use copy on resource types, not on properties within a resource type. E.g. Creating multiple data disks within a VM.</li>
</ol>
<p>That is all there is to iterate and creating multiple resources from a single definition. When your templates will start becoming complex then these constructs/helper functions will help you a lot. E.g. you may need to deploy multiple load balanced resources, then you can use the concepts defined in this post.</p>
<p>You can also refer the official documentation here: <a href="https://azure.microsoft.com/en-us/documentation/articles/resource-group-create-multiple/">copy, copyIndex, and length</a></p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-iterating-and-creating-multiple-instances-of-a-resource</link>
<pubDate>Tue, 27 Sep 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Deploying ARM Templates using Visual Studio</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>In the last blog we saw <a href="/post/step-by-step-arm-templates-authoring-arm-templates-using-visual-studio/">How to use Visual Studio to author ARM templates</a>. In this blog we will see how to use Visual Studio (VS) to deploy the template without leaving VS.</p>
<p>Deploying with Visual Studio is very simple, straightforward and very intuitive. Just follow the below steps.</p>
<ol>

    <li>
Either go to the Solution Explorer -> Right Click on the project and select "Deploy -> New Deployment" as shown below:<br />
<img src="/images/147578624957f6b6090d5eb.png" alt="New Deployment - Solution Explorer" /><br />

Or you can go to the menu option Project -> Deploy -> New Deployment as shown below:<br />
<img src="/images/147578624157f6b601b647b.png" alt="New Deployment - Project Menu" />

<br />
Once you click on the "New Deployment", you will be presented with the below Dialog for the deployment.<br />
<img src="/images/147578742957f6baa5bfb1c.png" alt="New Deployment Dialog" /><br />

If you are not logged in then it will ask you to log into your Azure account. <br />

    </li>

    <li>
In the Dialog for "Deploy to Resource Group" select the Subscription by clicking on the first drop down.<br />
    </li>

    <li>
Next click on the drop down for the Resource Group. You can either select an existing Resource Group or you can click on "<Create New...>" option to create a new resource group for the current deployment.
<br />
<img src="/images/147578768757f6bba7dd5ec.png" alt="Resource Group creation" />
<br />
If you click on "<Create New...>" option to create a new Resource Group then you will be presented with an additional popup.
<br />
<img src="/images/147578794457f6bca893e66.png" alt="Resource Group creation additional Popup" /><br />
In this additional popup, type the name for your new resoruce group and the location in Azure where this should be created. Click "Create" once done in the additional popup. <br /><br />
    </li>

    <li>
Next, we are going to provide the value for the parameters. Go ahead and click on the "Edit Parameters..." link in the "Deploy to Resource Group" dialog. This will open another popup to provide the parameters. <br />
Button to edit parameters is shown below:<br />
<img src="/images/147578820657f6bdae623f9.png" alt="Edit Parameters" /> <br />

Additional dialog to provide parameters is shown below: <br />
<img src="/images/147578821157f6bdb3ec989.png" alt="Providing Parameters" /> <br />

Note the following points in the parameters:
<ol type="a">
<li>Corresponding to the string parameters, a text box is provided.</li>
<li>For the secure string parameters like password, a secure password text box is provided.</li>
<li>Corresponding to the parameters for which you have defined the "Allowed Values" in your template, a combo box (or drop Down) is provided with the "default Value" selected by default.</li>
</ol>
 Click Ok once done
   </li>

    <li>
Next, click on the Deploy button to deploy the template to Azure.
    </li>

    <li>
You can check the results in the <b>Outputs</b> window in the Visual Studio. Along with time stamp, it will show you what steps Visual Studio took to perform the deployment. It uses the values of parameters you provided and uses the PowerShell script to deploy the resources. You will notice the PowerShell window opening and prompt for the Admin Password. 
<br />
<b>Note 1: </b>The PowerShell window may not come above as active window. Just search and click on the window in your Taskbar. <br />
Provide the password and hit Enter as shown below:
<img src="/images/147578930357f6c1f7c0b65.png" alt="PowerShell window" /> <br />
<br />
<b>Note 2: </b>It may take some time to complete the deployment after that. Wait and do not close the PowerShell window. It should automatically close once done.
<br />
<b>Note 3: </b>Once the deployment completes the last line in Output window in Visual Studio will be: "Successfully deployed template..." as shown below:
<br />
<img src="/images/147578931057f6c1fecaa6a.png" alt="Success - Output Window" /> <br />

    </li>

</ol>
<p>This is it! Navigate to the Azure portal and validate the deployed resources in your selected resource group.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-deploying-arm-templates-using-visual-studio</link>
<pubDate>Tue, 20 Sep 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Authoring ARM Templates using Visual Studio</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p><strong>Visual Studio</strong> is a very powerful tool when it comes to authoring ARM Templates. </p>
<p><strong>Key features</strong> which make it a tool of our choice are:</p>
<ul>
<li>In-house support for ARM Templates</li>
<li>Smart IntelliSense</li>
<li>Pre-populated templates for various Azure resources</li>
<li>JSON Outlining</li>
<li>Easy Deployment options</li>
</ul>
<p>The screenshots in this blog post are from Visual Studio 2013. You can use other newer versions as well.</p>
<h3>Pre-Requisites</h3>
<p>You need to have Azure SDK installed to get true power of Visual Studio with Azure integration. If you don't have it already, you can install the same from here: <a href="https://azure.microsoft.com/en-us/downloads/">Azure SDK Downloads</a></p>
<h3>Authoring First ARM Template in Visual Studio</h3>
<p>Authoring with Visual Studio is very easy. </p>
<ol type="1">
<li>To get started just launch the Visual Studio from the Start menu.</li>
<li>Next, Create a new Project of type "Azure Resource Group" by navigating to Templates -> Visual C# -> Cloud <br /><br />

<img src="/images/147576108957f653c1768fc.png" alt="New Project" />
<br />

</li>

<li>
    Next, you will be presented with a dialog to "Select Azure Template". If you want to author from scratch then choose a Blank Template. Else select one of the starter template. For this blog, we will be using "Windows Virtual Machine" Template.<br /><br />
    <img src="/images/147576828457f66fdc0f0e6.png" alt="Selecting Azure Template" />
    <br />
</li>

<li>
    Project is created with various folders and files. You can explore the project in the solution explorer in Visual Studio.<br /><br />
    <img src="/images/147576986357f6760758619.png" alt="Solution Explorer" />
    <br />

Let us see what these folders and files are:
        <ol type="a">
            <li><b>Scripts</b>: The single PS1 file is to create a new Resource Group and deploy the ARM Template. It uses "New-AzureRmResourceGroupDeployment" PowerShell cmdlet to deploy the template. </li>
            <li><b>Templates</b>: "<i>WindowsVirtualMachine.json</i>" is the main ARM Template file that we are interested in. Also, "<i>WindowsVirtualMachine.parameters.json</i>" is the parameters file for the ARM template.</li>
            <li><b>Tools</b>: This folder contains the "AzCopy.exe" file to help you copy any artifacts to Azure.</li>
        </ol>
</li>

<li>
    Double click and open the "<i>WindowsVirtualMachine.json</i>" file to open it. You will be presented with a huge JSON file. Collapse the section by clicking the small "-" signs to the left of the file. Also notice the <b>JSON Outline</b> panel to the left. This is your biggest friend in Visual Studio when authoring ARM Templates.<br /><br />
    <img src="/images/147577487757f6899d297b5.png" alt="" />
    <br />
You can immediately notice that key sections both in the template in the middle and in the JSON Oultine panel on the left (in the image above) are:
               <ol type="a">
                   <li>parameters</li>
                   <li>variables</li>
                   <li>resources</li>
               </ol>
You can click on any of the elements in the left JSON Outline panel and the same section will be highlighted in the center, in the JSON template file.
    <br />

</li>

<li>
    Next, let us look at JSON Outline panel and check how it can provide us more information and help us in authoring templates.<br /><br />
    <img src="/images/147578115457f6a2226c2be.png" alt="JSON Outline Panel" />
    <br />
    You can see that the panel provides a special icon for each type of the resource. In our current template the various resources listed are:
    <ol type="a">
        <li>StorageAccount</li>
        <li>PublicIPAddress</li>
        <li>VirtualNetwork</li>
        <li>NetworkInterface</li>
        <li>VirtualMachine</li>
    </ol>

Click on each of the resources and inspect how their JSON structure looks and differs. You will immediately notice that the major difference in each of these resources is in their <b>Type</b> and <b>Properties</b>.

<h3>Adding New Resource</h3> 
Let's assume you want to add a new resource to this template. You have 2 ways to achieve the same:
    <ol>
    <li><b>Method 1</b> - Create a new resource by modifying and adding the JSON for the new resource in the template.</li>
    <li><b>Method 2</b> - Let Visual Studio add the resource for you. Right click anywhere in the resources area of the JSON Outline Panel or the small "+" box at the top left of the panel (as shown in the image above) and VS will give you a new popup to add the resource from pre-defined resources as shown below.</li>
    </ol>
    <img src="/images/147578170157f6a4457bae0.png" alt="Add Resource" />
<br />
Once you click Add the JSON for resource will be added to the template and the corresponding new element will appear in the JSON Outline Panel.
<br />

<h3>Deleting a Resource</h3> <br />
If you need to delete a resource, simply right click on that resource in the JSON outline panel on the left and then select "Delete Resource".
<br />

<h3>Using Intellisense</h3>     
The last thing to notice is the use of <b>Intellisense</b> in Visual Studio which helps you as you are editing the templates.<br /><br />
    <img src="/images/147578465457f6afce4debb.png" alt="Intellisense" />
    <br />
     When you type quotes the closing quotes are automatically provided. Also, as you can see in the above image the various valid values, that can come there are also shown along with small tooltip about the data type. If the Intellisense doesn't come up automatically, then press Ctrl + Space to get Intellisense.
</li>

</ol>
<p>In the end, the Visual Studio makes authoring ARM templates much more manageable and easy for you.</p>
<p>In the next blog, we will see how to use Visual Studio to Deploy the templates.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-authoring-arm-templates-using-visual-studio</link>
<pubDate>Sat, 17 Sep 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Creating Parameters file for an ARM Template</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>You can pass the input values for the Parameters in your ARM template using an additional JSON file. This additional file is what we will be referring to as <strong>Parameters File</strong>.</p>
<p>The only restriction on a parameters file is that the size of the parameter file cannot be more than 64 KB.</p>
<p>Parameters file follows a similar structure to the ARM Template. They are very simple as compared to the ARM template. In all they have 3 sections as explained below:</p>
<ol>
<li><strong>$schema</strong> - Required Object - Location of the JSON schema file that describes the version of the template language.</li>
<li><strong>contentVersion</strong> - Required Object - Version of the template (such as 1.2.0.20). When deploying resources using the template, this value can be used to make sure that the right template is being used.</li>
<li><strong>parameters</strong> - Required Object - This is a JSON object which contains various objects as it's members. Each object within the &quot;parameters&quot; object represent a value for a parameter corresponding to your ARM template.</li>
</ol>
<p>Let's check how the parameters file will look like for the ARM template we have built earlier for deploying Storage Account and a Virtual Network.</p>
<pre><code>{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "vhdStorageName": {
            "value": "harvestingstorage101"
        },
        "virtualNetworkName": {
            "value": "testvNet101"
        }
    }
}</code></pre>
<p>Note that the only 2 parameter values are provided. These correspond to the parameters in the ARM template. </p>
<p><strong>Note:</strong> The parameter names should match to the parameters defined in the ARM template.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-creating-parameters-file-for-an-arm-template</link>
<pubDate>Wed, 14 Sep 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Deploying Template Using Azure PowerShell</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>In the previous blog you learned <a href="/post/step-by-step-arm-templates-building-your-first-arm-template/">How to build your first ARM Template</a> . Now that you have a fully functional ARM template we want to deploy this template to Azure.</p>
<p>There are various options to deploy a template to Azure. We already saw in the last blog <a href="/post/step-by-step-arm-templates-deploying-template-using-azure-portal/">How to deploy template using <strong>Azure Portal</strong></a>. Now we will look at <strong>Azure PowerShell</strong> as more programmatic and automated way to deploy the template.</p>
<h2>Pre-requisites</h2>
<p>Things you should know before deployment</p>
<ol>
<li><strong>Azure PowerShell</strong> - This should be installed on the machine from where the Steps will be followed. If you don't have this then use this link to get it: <a href="https://azure.microsoft.com/en-us/documentation/articles/powershell-install-configure/">Get Azure PowerShell</a></li>
<li><strong>Azure Subscription</strong> - where you want to deploy your template</li>
<li><strong>Resource Group</strong> - This is the resource group in Azure where you will be deploying your template. You can create a new resource group (for the resources that will be deployed by the template) or use an existing one.</li>
<li><strong>Parameters</strong> - Value of the input parameters to the template should be known to you for the deployment. Follow all your naming conventions when defining the parameters for deployments of resources in Azure. </li>
<li><strong>Internet Connectivity</strong> - This should be present on the machine from where the Steps will be followed for connectivity to Azure</li>
</ol>
<h2>Steps for Deployment</h2>
<ul>
<li>First, launch a PowerShell window as an Administrator</li>
<li>Then, log into the Azure account. </li>
</ul>
<p>Run the below cmdlet to log into Azure:</p>
<pre><code>Add-AzureRmAccount</code></pre>
<ul>
<li>Select appropriate Azure Subscription</li>
</ul>
<p>You have two choices here. You can either use below cmdlet to use Subscription ID</p>
<pre><code>Set-AzureRmContext -SubscriptionID &lt;YourSubscriptionId&gt;</code></pre>
<p>Or you can use the Subscription name with the below cmdlet:</p>
<pre><code>Select-AzureRmSubscription -SubscriptionName "&lt;Your Subscription Name&gt;"</code></pre>
<ul>
<li>Next, if you already have a resource group to which you want to deploy the template then skip this step. Else create a new resource group. A resource in Azure ARM architecture can only exist in a resource group. </li>
</ul>
<p>Use below cmdlet to create a new Resource Group:</p>
<pre><code>New-AzureRmResourceGroup -Name TestResourceGroup01 -Location "Central US"</code></pre>
<ul>
<li>Before deploying the Resource Template to Azure, you should Test it. This step is optional but highly recommended.</li>
</ul>
<p>Use the below cmdlet to test and validate your template:</p>
<pre><code>Test-AzureRmResourceGroupDeployment -ResourceGroupName TestResourceGroup01 -TemplateFile &lt;PathToJsonTemplate&gt;</code></pre>
<ul>
<li>Now comes the last step i.e. to deploy the template. You have two options when deploying the template. You can either deploy a template without any parameters (if none are required) or you need to specify the parameters. Let's check both these options next.</li>
</ul>
<h3>Deploying Template which doesn't need Parameters</h3>
<p>You can deploy such template using <code>New-AzureRmResourceGroupDeployment</code> cmdlet.
If the template file is on a local directory then use the below cmdlet:</p>
<pre><code>New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName TestResourceGroup01 -TemplateFile &lt;PathToTemplate&gt;</code></pre>
<p>If the template file is uploaded to some hosted location and is accessible via a link, then use the below cmdlet to deploy the template:</p>
<pre><code>New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName TestResourceGroup01 -TemplateUri &lt;LinkToTemplate&gt;</code></pre>
<h3>Deploying Template with Parameters</h3>
<p>Deploying of the template is exactly similar as the previous section. You use the same cmdlet. To specify the parameter, you have 4 options. Use the below cmdlets for the option you want to use.</p>
<p><strong>Option 1</strong> - Using Inline Parameter</p>
<pre><code>New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName TestResourceGroup01 -TemplateFile &lt;PathToTemplate&gt; -myParameterName "parameterValue" -secondParameterName "secondParameterValue"</code></pre>
<p><strong>Option 2</strong> - Using Parameter Object</p>
<pre><code>$parameters = @{"&lt;ParameterName&gt;"="&lt;Parameter Value&gt;"}
New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName TestResourceGroup01 -TemplateFile &lt;PathToTemplate&gt; -TemplateParameterObject $parameters</code></pre>
<p><strong>Option 3</strong> - Using Parameter file which is in local environment</p>
<pre><code>New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName TestResourceGroup01 -TemplateFile &lt;PathToTemplate&gt; -TemplateParameterFile &lt;PathToParameterFile&gt;</code></pre>
<p><strong>Option 4</strong> - Using Parameter file which is located externally and can be referenced via Link</p>
<pre><code>New-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName TestResourceGroup01 -TemplateUri &lt;LinkToTemplate&gt; -TemplateParameterUri &lt;LinkToParameterFile&gt;</code></pre>
<h3>Key Gotchas</h3>
<ol>
<li>If you provide values for a parameter in both the local parameter file and inline, the inline value takes precedence.</li>
<li>You cannot use inline parameters with an external parameter file. All inline parameters are ignored when you specify &quot;TemplateParameterUri&quot; parameter.</li>
<li>As a best practice, do not store sensitive infomation in the parameters file e.g. Local admin password. Instead either provide these dynamically using inline parameters. Or store them using the Azure Key vault and then reference the key vault in your parameters file.</li>
</ol>
<p>You can find more details about these cmdlets here: <a href="https://azure.microsoft.com/en-us/documentation/articles/resource-group-template-deploy/#deploy-with-powershell">Deploy resources with Resource Manager templates and Azure PowerShell</a></p>
<p>In the next blog, we will see how to create a Parameters File for providing parameters dynamically to the template.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-deploying-template-using-azure-powershell</link>
<pubDate>Sun, 11 Sep 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Deploying Template Using Azure Portal</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>In the last blog you learned <a href="/post/step-by-step-arm-templates-building-your-first-arm-template/">How to build your first ARM Template</a> . Now that you have a fully functional ARM template we want to deploy this template to Azure.</p>
<p>There are various options to deploy a template to Azure. Using Azure portal is by far the easiest and most intuitive option for the deployment. Follow the steps in this blog to deploy your template to Azure.</p>
<h2>Pre-requisites</h2>
<p>Things you should know before deployment</p>
<ol>
<li><strong>Azure Subscription</strong> - where you want to deploy your template</li>
<li><strong>Resource Group</strong> - This is the resource group in Azure where you will be deploying your template. You can create a new resource group (for the resources that will be deployed by the template) or use an existing one.</li>
<li><strong>Parameters</strong> - Value of the input parameters to the template should be known to you for the deployment. Follow all your naming conventions when defining the parameters for deployments of resources in Azure.</li>
</ol>
<h2>Steps for Deployment</h2>
<ol>
<li>First, log into the Azure Portal.</li>
<li>
<p>Next, go to &quot;New&quot; and type &quot;Template deployment&quot; in the search box and hit enter.</p>
<p><img src="/images/147559616157f3cf81381da.png" alt="New Deployment" /></p>
</li>
<li>
<p>Next, click on the <strong>Template Deployment</strong> and then click on &quot;Create&quot;</p>
<p><img src="/images/147559654057f3d0fc1bf5c.png" alt="Create Deployment" /></p>
</li>
<li>
<p>Now click on the &quot;<strong>Template (Edit Template)</strong>&quot;. It will open a panel to paste your template. Delete whatever is auto populated in the template area. Copy your whole json template and paste it here. Note that the left section in the new panel will update to show you what parameters, variables, and resources you have in the template. Click on &quot;Save&quot; once done.</p>
<p><img src="/images/147559697957f3d2b32ff67.png" alt="Editing Template" /></p>
</li>
<li>
<p>Next, click on the &quot;<strong>Parameters (Edit Parameters)</strong>&quot; on the left side. The parameters will be automatically picked from the template. The parameters for which the default value is provided will be automatically populated. Rest you will have to provide the inputs. Click Ok once done.</p>
<p><img src="/images/147559719657f3d38c652e3.png" alt="Providing Parameters" /></p>
</li>
<li>
<p>Next, you have the option to select the <strong>Resource Group</strong>. You can either create a new resource group (for all the resources that will be deployed via the template) or you can use and existing resource group.</p>
<p><img src="/images/147559727457f3d3da45879.png" alt="Resource Group Selection" /></p>
</li>
<li>
<p>The last option is to click on the &quot;<strong>Legal Terms</strong>&quot; and read through the terms. If you agree then click on the &quot;Purchase&quot; button. </p>
<p><img src="/images/147559762057f3d534eae90.png" alt="Legal Terms" /></p>
</li>
<li>Finally, click on the <strong>Create</strong> to submit the deployment. </li>
</ol>
<p>You can monitor the job performing the deployments and progress of the same. After some time the deployment will finish successfully and you can view the resources in the resource group you selected.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-deploying-template-using-azure-portal</link>
<pubDate>Thu, 08 Sep 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Building your first ARM Template</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>In this blog post, we will use the knowledge learned in previous blogs and will build a basic ARM template.
If you haven't checked previous blog posts then have a quick read of your preferred topics here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>To follow this blog, you can use any text editor which can provide JSON syntax highlighting. We will be looking at using Visual Studio to author ARM templates in a future blog post. Visual Studio can provide JSON outlining and is a very powerful tool for authoring ARM templates.</p>
<p>Let us assume that you want to deploy a storage account and build a virtual network in Azure. You want to automate the process and need to repeat the process in various environments. ARM templates fit the bill for the solution of this problem.</p>
<p>In the next few sections, we will build each section of the template and then at the end will check the complete template.</p>
<h3>1. Template Header</h3>
<p>This section is very basic and contains just the schema and the content version. You can use the content version to manage the development versions of the template as you make changes to your templates in the future.</p>
<pre><code>"$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
"contentVersion": "1.0.0.0",</code></pre>
<h3>2. Parameters</h3>
<p>Here we define all the inputs we need from the end users. We provide default values for those parameters for which we know what the most common values will be based on our environment. For the current template we define two parameters:</p>
<ul>
<li><strong>vhdStorageName</strong> - This is the name of the storage account in Azure which will be created by the deployment of this template.</li>
<li><strong>virtualNetworkName</strong> - This is the name of the Virtual Network which will be created by the deployment of this template.</li>
</ul>
<p>As a best practice, provide the metadata, describing what each parameter is for. Also, note that we have used pascal casing to name the parameters with very descriptive names.</p>
<pre><code>"parameters": {
    "vhdStorageName": {
        "type": "string",
        "minLength": 1,
        "defaultValue": "mystorage101",
        "metadata": {
            "description": "Name of the Storage Account."
        }
    },
    "virtualNetworkName": {
        "type": "string",
        "metadata": {
            "description": "Name of the virtual network."
        }
    }
},</code></pre>
<h3>3. Variables</h3>
<p>Next, we add some variables for the values which will be reused later in the template in the resources section. We create variables for all those reusable values for which we know what their value at deployment will be. We define 4 variables in this template:</p>
<ul>
<li><strong>addressPrefix</strong> - Address prefix for the Virtual Network </li>
<li><strong>subnetName</strong> - Subnet name which will be created under the virtual network</li>
<li><strong>subnetPrefix</strong> - Subnet prefix for the subnet, which will be created under the virtual network</li>
<li><strong>vhdStorageType</strong> - Type of the storage account. Here we used Standard locally redundant storage (LRS)</li>
</ul>
<p>Variables section look as below:</p>
<pre><code>"variables": {
    "addressPrefix": "10.0.0.0/16",
    "subnetName": "Subnet",
    "subnetPrefix": "10.0.0.0/24",
    "vhdStorageType": "Standard_LRS"
},</code></pre>
<h3>4. Resources</h3>
<p>Now comes the last and main section i.e. Resources. Here we define both the resources for our template:</p>
<ul>
<li>Storage Account</li>
<li>Virtual Network</li>
</ul>
<p>Let us look at each of these resources one by one.</p>
<p><strong>A. Storage Account Resource</strong></p>
<p>This resource has below properties (or key-value pairs):</p>
<ol>
<li><strong>Type</strong> - Type of the resource is set to Microsoft.Storage/storageAccounts. This is what tells the Azure that the current resource is a Storage Account</li>
<li><strong>Name</strong> - This defines the name of the storage account to be deployed based on the parameter to the template</li>
<li><strong>API Version</strong> - this is the standard version for the REST API in Azure</li>
<li><strong>Location</strong> - This is the Azure location. The location is found dynamically based on the location of the resource group to which this template will be deployed.</li>
<li><strong>tags</strong> - only one tag is defined for the display name. You should have more tags in case of a production ready template</li>
<li><strong>properties</strong> - This is where you tell Azure what kind of storage account you need. Here the account type is set using the value of the variable vhdStorageType.</li>
</ol>
<p><strong>B. Virtual Network Resource</strong></p>
<p>This resource has below properties (or key-value pairs):</p>
<ol>
<li><strong>Type</strong> - Type of the resource is set to Microsoft.Network/virtualNetworks. This is what tells the Azure that the current resource is a Virtual Network</li>
<li><strong>Name</strong> - This defines the name of the virtual network to be deployed based on the parameter to the template</li>
<li><strong>API Version</strong> - this is the standard version for the REST API in Azure</li>
<li><strong>Location</strong> - This is the Azure location. The location is found dynamically based on the location of the resource group to which this template will be deployed.</li>
<li><strong>tags</strong> - only one tag is defined for the display name. You should have more tags in case of a production ready template</li>
<li><strong>properties</strong> - This is where you define the address space for the virtual network. You also define the subnet under the virtual network here.</li>
</ol>
<p>The resources section look like below:</p>
<pre><code>"resources": [
    {
        "type": "Microsoft.Storage/storageAccounts",
        "name": "[parameters('vhdStorageName')]",
        "apiVersion": "2015-06-15",
        "location": "[resourceGroup().location]",
        "tags": {
            "displayName": "StorageAccount"
        },
        "properties": {
            "accountType": "[variables('vhdStorageType')]"
        }
    },
    {
        "apiVersion": "2015-06-15",
        "type": "Microsoft.Network/virtualNetworks",
        "name": "[parameters('virtualNetworkName')]",
        "location": "[resourceGroup().location]",
        "tags": {
            "displayName": "VirtualNetwork"
        },
        "properties": {
            "addressSpace": {
                "addressPrefixes": [
                    "[variables('addressPrefix')]"
                ]
            },
            "subnets": [
                {
                    "name": "[variables('subnetName')]",
                    "properties": {
                        "addressPrefix": "[variables('subnetPrefix')]"
                    }
                }
            ]
        }
    }
]</code></pre>
<h3>Complete Template</h3>
<p>Here is the complete template build from all the sections discussed above. You can copy and use this template for testing and working along with next deployment posts.</p>
<pre><code>{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "vhdStorageName": {
            "type": "string",
            "minLength": 1,
            "defaultValue": "mystorage101",
            "metadata": {
                "description": "Name of the Storage Account."
            }
        },
        "virtualNetworkName": {
            "type": "string",
            "metadata": {
                "description": "Name of the virtual network."
            }
        }
    },
    "variables": {
        "addressPrefix": "10.0.0.0/16",
        "subnetName": "Subnet",
        "subnetPrefix": "10.0.0.0/24",
        "vhdStorageType": "Standard_LRS"
    },
    "resources": [
        {
            "type": "Microsoft.Storage/storageAccounts",
            "name": "[parameters('vhdStorageName')]",
            "apiVersion": "2015-06-15",
            "location": "[resourceGroup().location]",
            "tags": {
                "displayName": "StorageAccount"
            },
            "properties": {
                "accountType": "[variables('vhdStorageType')]"
            }
        },
        {
            "apiVersion": "2015-06-15",
            "type": "Microsoft.Network/virtualNetworks",
            "name": "[parameters('virtualNetworkName')]",
            "location": "[resourceGroup().location]",
            "tags": {
                "displayName": "VirtualNetwork"
            },
            "properties": {
                "addressSpace": {
                    "addressPrefixes": [
                        "[variables('addressPrefix')]"
                    ]
                },
                "subnets": [
                    {
                        "name": "[variables('subnetName')]",
                        "properties": {
                            "addressPrefix": "[variables('subnetPrefix')]"
                        }
                    }
                ]
            }
        }
    ]
}</code></pre>
<p>In the next blog, we will learn how to deploy this template.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-building-your-first-arm-template</link>
<pubDate>Sun, 04 Sep 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - Helper Functions</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>ARM Templates has various dynamic constructs called <strong>Helper Functions</strong> which can make your template more generic. These constructs reduce the hard coded values in your templates. You can use the information from this blog to make your existing templates more dynamic and start writing new templates with a much generic approach.</p>
<p>Let's look at the most important helper functions and their practical usage one by one. </p>
<h3>1. Resource Id - Resource Function</h3>
<p>You use this function to determine the ID of a resource. This is only used when the resource (whose ID is needed) is not being deployed in the current template and it already exists in Azure.</p>
<p>The generic syntax to use this is:</p>
<pre><code>resourceId ([subscriptionId], [resourceGroupName], resourceType, resourceName1, [resourceName2]...)</code></pre>
<p>Only required parameters of this helper function are resourceType and resourceName1.</p>
<p>These parameters are as follows:</p>
<ul>
<li>subscription ID - This is only needed if you want to refer a different subscription. Default value is the current subscription</li>
<li>resource Group Name - Name of the resource group where the resource exists. Default is the current resource group, in which you are deploying the template</li>
<li>resource Type - Type of resource including resource provider namespace</li>
<li>resource Name 1  - Name of the resource</li>
<li>resource Name 2  - Next resource name segment if resource is nested. E.g. a VM Extension</li>
</ul>
<p><strong>Example</strong></p>
<pre><code>"vnetId1": "[resourceId('AE06-Mgmt-RG','Microsoft.Network/virtualNetworks', parameters('virtualNetworkName'))]",
"vnetId2": "[resourceId('Microsoft.Network/virtualNetworks', variables('virtualNetworkName'))]"</code></pre>
<p>The above example shows two ways of using the resource ID helper function to determine the Id of a virtual network. First one uses the resource group, resource type and resource name. Second example uses only the resource Type and resource name. Second example assumes the resource group to be same as the template being deployed to.</p>
<h3>2. Resource Group - Resource Function</h3>
<p>This helper function returns an object that represents the current resource group to which the template is being deployed.</p>
<p>The generic syntax to use this is:</p>
<pre><code>resourceGroup()</code></pre>
<p>No parameters are needed in this helper function.</p>
<p><strong>Example</strong></p>
<pre><code>"vhdStorageName": "[concat('vhdstorage', uniqueString(resourceGroup().id))]",
 "storageAccountResourceGroup": "[resourcegroup().name]",
 "location": "[resourceGroup().location]"</code></pre>
<p>The above example shows 3 uses of the resource group helper functions. First one uses the ID of the resource group, second uses the name property and third uses the location for the current resource group.</p>
<h3>3. Subscription - Resource Function</h3>
<p>The generic syntax to use this is:</p>
<pre><code>subscription()</code></pre>
<p>No parameters are needed in this helper function.</p>
<p><strong>Example</strong></p>
<pre><code>"subscriptionId": "[subscription().subscriptionId]"</code></pre>
<p>The above example is straightforward. It fetches the subscription Id of the current subscription.</p>
<h3>4. Concat - String Function</h3>
<p>This function is used to concatinate (i.e. combine) two or more values.</p>
<p>The generic syntax to use this is:</p>
<pre><code>concat (array1, array2, array3, ...)</code></pre>
<p>At least 1 array is needed for concat to work. </p>
<p><strong>Example</strong></p>
<pre><code>"subnetRef": "[concat(variables('vNetId'), '/subnets/', variables('subnetName'))]"</code></pre>
<p>The above example combines (or concatinates) 3 text values. First value is the value of variable vNetId. Second value is a string &quot;/subnets/&quot;. Third value is the value of the variable subnet Name.</p>
<p>These are the most common Helper functions that you will use in 80%-90% of the templates. </p>
<p>To check the complete list of Helper Functions, check this official link: <a href="https://azure.microsoft.com/en-us/documentation/articles/resource-group-template-functions/#resource-functions">Azure Resource Manager template functions</a></p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-helper-functions</link>
<pubDate>Wed, 31 Aug 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - What is in an ARM Template - Understanding Components 5 - Outputs</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>If you haven't checked the previous blog on the overall structure of the ARM template, I suggest you give it a quick read before checking the component described in this post in detail.</p>
<ol>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-all-components/">Understanding all components</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-2-parameters/">Parameters</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-3-variables/">Variables</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-4-resources/">Resources</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-5-outputs/">Outputs - This blog post</a></li>
</ol>
<h2>Outputs</h2>
<p>This section is used to output any values after the deployment of the ARM Template. This can output any Ids or connection strings based on the deployed resources. </p>
<p>This is a single JSON object with various output objects (just like Parameters. The overall JSON structure looks like below:</p>
<pre><code>"outputs": { 
    "output1" : {
                     "type":"string",
                     "value": "value1"
      },
    "output2" : {
                     "type":"string",
                     "value": "value2"
      },
}</code></pre>
<p>Each output object has 2 properties:</p>
<ol>
<li>Type - Data type of the output</li>
<li>Value - value of the output</li>
</ol>
<p>A real life example with look like below:</p>
<pre><code>"outputs": {
    "adminUsername": {
        "type": "string",
        "value": "[parameters('adminUsername')]"
    }
}</code></pre>
<p>The above example will output the administrator Username using the parameter from the template.</p>
<p>That's all there is to Outputs in ARM Templates. If you have any doubts, please comment in the below section. Use the links at the Top to know all about other components in an ARM Template.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-5-outputs</link>
<pubDate>Tue, 30 Aug 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - What is in an ARM Template - Understanding Components 4 - Resources</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>If you haven't checked the previous blog on the overall structure of the ARM template, I suggest you give it a quick read before checking the component described in this post in detail.</p>
<ol>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-all-components/">Understanding all components</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-2-parameters/">Parameters</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-3-variables/">Variables</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-4-resources/">Resources  - This blog post</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-5-outputs/">Outputs</a></li>
</ol>
<h2>Resources</h2>
<p>This is the major section of the whole ARM template. This is where you define what resources should be deployed in Azure. You also define dependencies between resources in this section. </p>
<p>The resources section consist of an array of JSON Objects as shown below:</p>
<pre><code>"resources": [
        { },
        { },
]</code></pre>
<p>Each object in the array (represented via curly braces) is an Azure resource. You can deploy multiple resources in a single ARM template. E.g. You can deploy a new Storage Account, new Virtual Network and three Virtual Machines in that virtual network within a single template.
Within the object, various properties (and nested properties) are used to provide the configurations of each resource. </p>
<h3>Elements</h3>
<p>Different elements in a single resource object can be one of the following:</p>
<ol>
<li><strong>apiVersion</strong> - <strong><em>Required</em></strong> - Version of the API. e.g. &quot;2015-06-15&quot;</li>
<li><strong>type</strong> - <strong><em>Required</em></strong> - Type of the resource. This value is a combination of the namespace of the resource provider and the resource type that the resource provider supports. e.g. Azure Storage Account will have type as &quot;Microsoft.Storage/storageAccounts&quot;.</li>
<li><strong>name</strong> - <strong><em>Required</em></strong> - Name of the resource. The name must follow URI component restrictions and also the Azure naming restrictions if any. E.g. Storage account name can only be in small letters and has to be unique.</li>
<li><strong>location</strong> - Optional - Use supported geo-locations of the provided resource without any spaces. Or use the resource group's location dynamically.</li>
<li><strong>tags</strong> - Optional - Tags that are associated with the resource.</li>
<li><strong>dependsOn</strong> - Optional - Other resources in the same template, that the current resource being defined depends on. The dependencies between resources are evaluated and resources are deployed in their dependent order. When resources are not dependent on each other, they are attempted to be deployed in parallel. The value can be a comma-separated list of resource names or resource unique identifiers.</li>
<li><strong>properties</strong> - Optional - Resource specific configuration settings. E.g. Account type property for a storage account name.</li>
<li><strong>resources</strong> - Optional - Child resources that depend on the resource being defined. E.g. Extension resources for a Virtual Machine resource.</li>
</ol>
<h3>Examples</h3>
<p>Let's look at two examples. First, we will take a simple resource example to deploy a storage account in Azure:</p>
<pre><code>{
            "type": "Microsoft.Storage/storageAccounts",
            "name": "[variables('vhdStorageName')]",
            "apiVersion": "2015-06-15",
            "location": "[resourceGroup().location]",
            "tags": {
                "displayName": "StorageAccount",
                "department" : "Finance",
                "application" : "database"
            },
            "properties": {
                "accountType": "[variables('vhdStorageType')]"
            }
        }</code></pre>
<p>Above example will deploy a storage account with the name from &quot;vhdStorageName&quot; variable. It will apply 3 tags to the resource after deployment. It will use the account type (i.e. standard or premium) based on the value of the &quot;vhdStorageType&quot; variable. If you want to deploy 2 or more similar storage accounts, then just copy and paste the json for the resource, separated by comma. It will become another object in the Resources array.</p>
<p>Now let's look at a complex and larger example of deploying a single virtual machine with one extension for Diagnostics.</p>
<pre><code>    {
        "apiVersion": "2015-06-15",
        "type": "Microsoft.Compute/virtualMachines",
        "name": "[variables('vmName')]",
        "location": "[resourceGroup().location]",
        "tags": {
            "displayName": "VirtualMachine"
        },
        "dependsOn": [
            "[concat('Microsoft.Storage/storageAccounts/', variables('vhdStorageName'))]",
            "[concat('Microsoft.Network/networkInterfaces/', variables('nicName'))]"
        ],
        "properties": {
            "hardwareProfile": {
                "vmSize": "[variables('vmSize')]"
            },
            "osProfile": {
                "computerName": "[variables('vmName')]",
                "adminUsername": "[parameters('adminUsername')]",
                "adminPassword": "[parameters('adminPassword')]"
            },
            "storageProfile": {
                "imageReference": {
                    "publisher": "[variables('imagePublisher')]",
                    "offer": "[variables('imageOffer')]",
                    "sku": "[parameters('windowsOSVersion')]",
                    "version": "latest"
                },
                "osDisk": {
                    "name": "osdisk",
                    "vhd": {
                        "uri": "[concat('http://', variables('vhdStorageName'), '.blob.core.windows.net/', variables('vhdStorageContainerName'), '/', variables('OSDiskName'), '.vhd')]"
                    },
                    "caching": "ReadWrite",
                    "createOption": "FromImage"
                }
            },
            "networkProfile": {
                "networkInterfaces": [
                    {
                        "id": "[resourceId('Microsoft.Network/networkInterfaces', variables('nicName'))]"
                    }
                ]
            },
            "diagnosticsProfile": {
                "bootDiagnostics": {
                    "enabled": true,
                    "storageUri": "[concat('http://', variables('diagnosticsStorageAccountName'), '.blob.core.windows.net')]"
                }
            }
        },
        "resources": [
            {
                "type": "extensions",
                "name": "Microsoft.Insights.VMDiagnosticsSettings",
                "apiVersion": "2015-06-15",
                "location": "[resourceGroup().location]",
                "tags": {
                    "displayName": "AzureDiagnostics"
                },
                "dependsOn": [
                    "[concat('Microsoft.Compute/virtualMachines/', variables('vmName'))]"
                ],
                "properties": {
                    "publisher": "Microsoft.Azure.Diagnostics",
                    "type": "IaaSDiagnostics",
                    "typeHandlerVersion": "1.5",
                    "autoUpgradeMinorVersion": true,
                    "settings": {
                        "xmlCfg": "[base64(concat(variables('wadcfgxstart'), variables('wadmetricsresourceid'), variables('wadcfgxend')))]",
                        "storageAccount": "[variables('diagnosticsStorageAccountName')]"
                    },
                    "protectedSettings": {
                        "storageAccountName": "[variables('diagnosticsStorageAccountName')]",
                        "storageAccountKey": "[listkeys(variables('accountid'), '2015-06-15').key1]",
                        "storageAccountEndPoint": "https://core.windows.net"
                    }
                }
            }
        ]
    }</code></pre>
<p>Note that the above code snippet defines a single virtual machine. Let us decode various sections of this complex resource:</p>
<ul>
<li>It begins with simple properties like apiVersion, type, name, location and tags as discussed in the previous example. These are straightforward and thus values are provided to these attributes.</li>
<li>Next is the <strong>dependsOn</strong> section. This defines the dependency between resources. In the above example, the virtual machine resource is dependent on the storage account and a network interface, which are also defined in the template. These 2 resources will be created before the virtual machine creation/deployment. If these resources are not created in the template then it will check for the presence of these resources in the current subscription. If they are not present the template will not get deployed and will error out.</li>
<li>Next are various <strong>properties</strong> to configure the Virtual machine, like hardware profile, os profile, storage profile, os disk, network profile, diagnostics profile etc.</li>
<li>Next, we have additional <strong>sub-resources</strong>. These are Azure resources which will be created and linked to the current resource. Only one sub-resource is created in the above example which is an extension for VM diagnostics settings.</li>
</ul>
<p>That's all there is to Resources in ARM Templates. If you have any doubts, please comment in the below section. Use the links at the Top to know all about other components in an ARM Template.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-4-resources</link>
<pubDate>Mon, 29 Aug 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - What is in an ARM Template - Understanding Components 3 - Variables</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>If you haven't checked the previous blog on the overall structure of the ARM template, I suggest you give it a quick read before checking the component described in this post in detail.</p>
<ol>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-all-components/">Understanding all components</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-2-parameters/">Parameters</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-3-variables/">Variables - This blog post</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-4-resources/">Resources</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-5-outputs/">Outputs</a></li>
</ol>
<h2>Variables</h2>
<p>Variables are values that you either know beforehand or you can construct from the input parameters. These variables can then be reused at multiple locations in the resources section. If you later want to change the value of a variable then it automatically gets updated at all other locations. They can be used to define a resource property.</p>
<h3>Defining Variables</h3>
<p>Variable is a one huge JSON object. Each property can be one of the simple data type (like integer, bool, string etc.) or can be another complex JSON object. The general structure is as shown below:</p>
<pre><code>"variables": {
      "variable 1" : "value 1",
      "variable 2" : "value 2",
      "variable 3" : 1024,
      "variable 4" : {}
}</code></pre>
<p>Note that in the above example, the first 3 variables are of simple value type. The 4rth variable is however of a complex JSON object type.</p>
<p>Let's now check a real variables section from an actual ARM template:</p>
<pre><code>"variables": {
        "vmSize": "Standard_A2",
        "virtualNetworkName": "MyVNETName",
        "vnetId1": "[resourceId('Microsoft.Network/virtualNetworks', variables('virtualNetworkName'))]",
        "vnetId2": "[resourceId(parameters('vNetRG'),'Microsoft.Network/virtualNetworks',parameters('virtualNetworkName'))]",
        "subnetRef": "[concat(variables('vnetId'), '/subnets/', variables('subnetName'))]",
        "vhdStorageName": "[concat('vhdstorage', uniqueString(resourceGroup().id))]",
        "storageAccountResourceGroup": "[resourcegroup().name]",
        "location": "[resourceGroup().location]",
        "subscriptionId": "[subscription().subscriptionId]"
    }</code></pre>
<p>There are lots of key constructs in the above code snippet. I have tried to capture as many different constructs in this snippets as I could. Let us decode each variable one by one.</p>
<ol>
<li>vmSize - Simple String</li>
<li>virtualNetworkName - Simple string name</li>
<li>vnetId1 - This uses a special function named &quot;<strong>resourceId</strong>&quot; to find out the resource ID of the virtual network. This function is invoked by using the syntax <code>"[resourceId(Input)]"</code> .  This gets the resource ID of a resource which is defined by the Input to this. Also, note the use of another variable as an input to this.</li>
<li>vnetId2 - This also fetches the resource Id of a virtual network using &quot;resourceId&quot; method. Note the use of the value of a parameter in this to find out Resource Group of the existing Virtual network (parameter &quot;vNetRG&quot;).</li>
<li>subnetRef - This variable uses another function &quot;<strong>concat</strong>&quot; in ARM template i.e. <code>"[concat(input1,input2,...)]"</code>. This function can take many inputs and will concatinate (i.e. club together) the value of all the inputs provided. You can use parameters or another variable.</li>
<li>vhdStorageName - This also uses concat function to dynamically generate a storage name. However it uses &quot;<strong>resourcegroup</strong>&quot; function as <code>"[resourcegroup()]"</code>. This function always returns the resource group to which you are deploying the current ARM template. Then the variable uses the id property of the resource group returned.</li>
<li>storageAccountResourceGroup - This uses the &quot;name&quot; property of the current resource group</li>
<li>location - This uses the &quot;location&quot; property of the current resource group.</li>
<li>subscriptionId - This uses the &quot;<strong>subscription</strong>&quot; function as <code>"[subscription()]"</code> to find out the current subscription to which the current ARM template is being deployed.  Then it uses the subscription Id property of the subscription to get the required Id.</li>
</ol>
<p>Note that these constructs are very powerfull and can be used to dynamically construct your ARM template. These constructs are also known as Helper Functions and are explained in detail here: <a href="../step-by-step-arm-templates-helper-functions/">Step by Step ARM Templates - Helper Functions</a></p>
<h3>Using Variables</h3>
<p>Using variables is very easy and is similar to using parameters. In fact, you already saw the usage of variables above, while defining other variables.</p>
<p>You use the square parenthesis to indicate to the ARM engine to evaluate whatever is inside the parenthesis. You use the &quot;variable&quot; keyword and then you pass the name of the variable as input. Check the example below.</p>
<pre><code>"storageAccountName": "[variables('storageAccountName')]"</code></pre>
<h3>Best Practices</h3>
<p>Best practices are similar to the Parameters.</p>
<ul>
<li>Provide complete descriptive names, no matter how long.</li>
<li>Use <strong>Pascal Casing</strong> to name your parameters. i.e. First letter should be a small letter. Then every new word will have the first letter as a capital. No space between words. E.g. storageAccountName</li>
<li>Use the constructs explained in the previous section to dynamically generate variables. This reduces any human errors.</li>
<li>Anything that is used more than once and is not required to be entered by an end user, should be created as a variable. Later on, this helps by minimizing the number of places you need to change the value.</li>
</ul>
<p>That's all there is to Variables in ARM Templates. If you have any doubts, please comment in the below section. Use the links at the Top to know all about other components in an ARM Template.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-3-variables</link>
<pubDate>Fri, 26 Aug 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - What is in an ARM Template - Understanding Components 2 - Parameters</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>If you haven't checked the previous blog on the overall structure of the ARM template, I suggest you give it a quick read before checking the component described in this post in detail.</p>
<ol>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-all-components/">Understanding all components</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-2-parameters/">Parameters - This blog post</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-3-variables/">Variables</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-4-resources/">Resources</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-5-outputs/">Outputs</a></li>
</ol>
<h2>Parameters</h2>
<p>As mentioned earlier, parameters are the way to customize the templates, each time you deploy it to create resources in Azure. These parameters are the end-user inputs for various aspects of the template. E.g. If you are deploying an Azure Virtual Machine via an ARM Template then the name of the VM can be an input parameter. Operating System type can be another parameter.</p>
<p>The parameters can be referred and used in other parts of the ARM Template.</p>
<h3>1. Defining Parameters</h3>
<p>Parameters is a one huge JSON object with multiple JSON properties. Each property is one parameter which is represented as another JSON object. Let us look at its structure at a high level.</p>
<pre><code>"parameters": {
               "parameter 1" : {},
               "parameter 2" : {},
               "parameter 3" : {}
}</code></pre>
<p>E.g. If you are creating a template to deploy a Windows Virtual Machine then the parameters will look something like below:</p>
<pre><code>"parameters": {
               "VMName" : {},
               "AdminUserName" : {},
               "AdminPassword" : {},
               "WindowsOSVersion" : {}
}</code></pre>
<p>Now let us look at one of the parameters. E.g. The AdminUserName parameter will look like:</p>
<pre><code>"adminUsername": {
            "type": "string",
            "minLength": 1,
            "metadata": {
                "description": "Username for the Virtual Machine."
            }
        }</code></pre>
<p>The parameter object, as shown above, has following parts:</p>
<ol>
<li><strong>Type</strong> - This is the data Type of the parameter.</li>
<li><strong>minLength</strong> - This is the minimum length the parameter must have</li>
<li><strong>Metadata</strong> - This is just to provide a description as to what the parameter means.</li>
</ol>
<p>The <strong>Data Type</strong> allowed for the parameter are:</p>
<ul>
<li>string or secureString – any valid JSON string</li>
<li>int – any valid JSON integer</li>
<li>bool – any valid JSON boolean </li>
<li>object – any valid JSON object </li>
<li>array – any valid JSON array</li>
</ul>
<p>A more complex parameter e.g. Windows OS Version, with few more properties, is shown below:</p>
<pre><code>"windowsOSVersion": {
            "type": "string",
            "defaultValue": "2012-R2-Datacenter",
            "allowedValues": [
                "2008-R2-SP1",
                "2012-Datacenter",
                "2012-R2-Datacenter"
            ],
            "metadata": {
                "description": "The Windows version for the VM. This will pick a fully patched image of this given Windows version. Allowed values: 2008-R2-SP1, 2012-Datacenter, 2012-R2-Datacenter."
            }
        }</code></pre>
<p>It has additional below properties:</p>
<ol>
<li><strong>Default Value</strong> - This is the default value. End User will be able to change this value when deploying the template. If no value is provided then this value is picked.</li>
<li><strong>Allowed Values</strong> - This is an Array of values which are allowed for the parameter. Only value from this set is allowed as an input.</li>
</ol>
<h3>2. Using Parameters</h3>
<p>Using parameters is easy. Wherever in your template (in variables or resources section) you want to use the value of a parameter, just use the parameter function as shown below with the name of the parameter as input, enclosed in square brackets. </p>
<pre><code>[parameters('windowsOSVersion')]</code></pre>
<p>If the parameter value is assigned to a property, enclosing it in double quotes, as shown below:</p>
<pre><code>"sku": "[parameters('windowsOSVersion')]"</code></pre>
<h3>3. Best Practices</h3>
<ul>
<li>Try to always provide Default Values</li>
<li>Provide metadata so that you can provide insight as to what the parameter is meant for</li>
<li>Provide complete descriptive names, no matter how long.</li>
<li>Use <strong>Pascal Casing</strong> to name your parameters. i.e. First letter should be a small letter. Then every new word will have the first letter as a capital. No space between words. E.g. windowsOSVersion</li>
<li>Use properties like minLength and Allowed values to impose restrictions. This reduces any human errors.</li>
</ul>
<p>That's all there is to Parameters in ARM Templates. If you have any doubts, please comment in the below section. Use the links at the Top to know all about other components in an ARM Template.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-2-parameters</link>
<pubDate>Wed, 24 Aug 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - What is in an ARM Template - Understanding All Components</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>As we discussed <a href="../step-by-step-azure-resource-manager-arm-templates-index/">earlier in the introduction</a> <strong>Azure Resource Manager (ARM) Template</strong> is a JavaScript Object Notation (JSON) file that defines one or more resources to deploy to a resource group. It also defines the dependencies between the deployed resources.</p>
<p>In this post, we will deconstruct any basic ARM template and will understand it's various components.</p>
<p>Any ARM Template will look like below:</p>
<pre><code>{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {},
    "variables": {},
    "resources": [ {}, {} ]
}</code></pre>
<p>Snapshot of the Template at root level, as generated via Visual Studio:</p>
<p><img src="http://HarvestingClouds.com/images/147520488857edd71873f9b.png" alt="ARM Template Components" /></p>
<p>As you can see the components (or properties) of any ARM template includes:</p>
<ol>
<li>Schema</li>
<li>Content Version</li>
<li>Parameters</li>
<li>Variables</li>
<li>Resources</li>
</ol>
<p>Let's look at these in more detail.</p>
<table border="1" cellpadding="4" cellspacing="4">
        <colgroup>
            <col>
            <col>
            <col>
            <col>
        </colgroup>
        <tbody valign="top">
            <tr>
                <th>Element name</th>
                <th>Required</th>
                <th>JSON Type</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>$schema</td>
                <td>Yes</td>
                <td>String Value</td>
                <td>Location of the JSON schema file that describes the version of the template language.</td>
            </tr>
            <tr>
                <td>contentVersion</td>
                <td>Yes</td>
                <td>String Value</td>
                <td>Version of the template (such as 1.2.0.20). When deploying resources using the template, this value can be used to make sure that the right template is being used.</td>
            </tr>
            <tr>
                <td>parameters</td>
                <td>No</td>
                <td>JSON Object</td>
                <td>Values that are provided by the end user (manually or via a parameters file) when deployment is executed to customize resource deployment.</td>
            </tr>
            <tr>
                <td>variables</td>
                <td>No</td>
                <td>JSON Object</td>
                <td>Values that are reused multiple times in the template. You can update these values. They are different from Parameters as their value is known and they are not required as inputs from the end user.</td>
            </tr>
            <tr>
                <td>resources</td>
                <td>Yes</td>
                <td>Array of Objects</td>
                <td>Types of services that are deployed or updated in a resource group. Each JSON object in this Array denotes an Azure Resource.</td>
            </tr>
            <tr>
                <td>outputs</td>
                <td>No</td>
                <td>JSON Object</td>
                <td>Values that are returned after deployment.</td>
            </tr>
        </tbody>
</table>
<p>Now that you know what each part is at a high level, in the next posts, we will look at the key 4 components in detail.</p>
<ol>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-2-parameters/">Parameters</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-3-variables/">Variables</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-4-resources/">Resources</a></li>
<li><a href="../step-by-step-arm-templates-what-is-in-an-arm-template-understanding-components-5-outputs/">Outputs</a></li>
</ol>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-what-is-in-an-arm-template-understanding-all-components</link>
<pubDate>Mon, 22 Aug 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Step by Step ARM Templates - JSON 101 for IT Administrators</title>
<description><![CDATA[<p><strong>Index</strong> of all blogs in this <strong>Step by Step ARM Templates series</strong> is located here: <a href="/post/step-by-step-azure-resource-manager-arm-templates-index/">Step by Step Azure Resource Manager (ARM) Templates - Index</a></p>
<p>Azure Resource Manager (ARM) templates are written in JSON or <strong>JavaScript Object Notation</strong>. To understand ARM templates, you need to understand few quick basics about JSON. These will enable you to lay a great foundation which will enable you to understand ARM templates very easily.</p>
<p>JSON or JavaScript Object Notation (pronounce like &quot;Jay-son&quot;) is a text-based data format that's designed to be human-readable, lightweight, and easy to transmit between a server and a web client. Its syntax is derived from JavaScript. Think of this as an even more compact version of XML files.</p>
<p>JSON is a popular notation for transmitting data through RESTful web services. The official internet media type for JSON is <code>application/json</code>, and JSON files typically have a <code>.json</code> extension.</p>
<p>To understand JSON we need to understand <strong>3 main components</strong>. These components are like building blocks, using which you can build very complex JSON files.</p>
<h2>1. Objects</h2>
<p>Objects are the heart of JSON. Object denotes a real life object, e.g. an Employee. Just like a real life object, these have various properties and a value for each of these properties. E.g. An Employee will have Name property with value as John. Further, an employee object can have various another properties like Age, Salary, Department etc. So to denote an object in JSON you:</p>
<ul>
<li>One object will be represented by curly brackets. It will begin from opening curly bracket i.e. <code>{</code> and will end at closing curly bracket i.e. <code>}</code></li>
<li>Denote the property and corresponding values as <code>"key" : "value"</code> or <code>"property" : "value"</code> pairs.</li>
<li>You can only use double quotes for Properties as they will always be of type string</li>
<li>You will have double quotes around Values if they are of string type. You will not have any quotes in case of a number or a boolean value.</li>
<li>Each property will be separated from next property by a comma</li>
</ul>
<p><strong>Note:</strong> Each JSON file is also a single JSON object. At root level it starts with an opening curly bracket i.e. <code>{</code> and will end with closing curly bracket i.e. <code>}</code>. There can't be any other objects at the root level. Think of this similar to how in an XML file there can be only one element at the root level. </p>
<p>Example Employee object is shown below:</p>
<pre><code>{
    "Name" : "John",
    "Age" : 34,
    "Department" : "Finance",
    "Salary" : "100000",
    "IsAdmin" : true
}</code></pre>
<h2>2. Arrays</h2>
<p>Simply put, arrays are a collection of items. In JSON the <strong>square brackets</strong> represents an Array. E.g. An array of 3 employees will look like below:</p>
<pre><code>[
  {
        "Name" : "John",
        "Age" : 34
    },
   {
        "Name" : "Mary",
        "Age" : 32
    },
   {
        "Name" : "Matthew",
        "Age" : 29
    }
]</code></pre>
<h2>3. Nesting of Objects</h2>
<p>Now things get more interesting with nesting of Objects. What Nesting means is that one object can have it's property as another complex object. Don't worry if that sounds confusing. Let's understand that statement using an example. An Address where a person lives can be represented by an object. This object will look like below:</p>
<pre><code>{
  "StreetNumber" : "50",
  "StreetName" : "Brian Harrison Way",
  "Unit Number" : 22,
  "City" : "Toronto",
  "Country" : "Canada"
}</code></pre>
<p>Now an Employee Object will have an Address object as one of it's property (because employee need to live somewhere). This new complex Employee object will look like below, with nested Address object as one of it's property:</p>
<pre><code> {
        "Name" : "John",
        "Age" : 34,
        "Department" : "Finance",
        "Salary" : "100000",
        "IsAdmin" : true,
        "Address" :   {
                          "StreetNumber" : "50",
                          "StreetName" : "Brian Harrison Way",
                          "Unit Number" : 22,
                          "City" : "Toronto",
                          "Country" : "Canada"
                       }
    }</code></pre>
<p>That's all there is to it. Now you can use these 3 components and build very complex json files/templates. Even the most complex template can be broken into these 3 components. </p>
<p>Below is a complex example with all 3 components. </p>
<pre><code>{
    "Department": "Finance",
    "TotalEmployees": 2,
    "Employees": [
        {
            "Name": "John",
            "Age": 34,
            "Department": "Finance",
            "Salary": "100000",
            "IsAdmin": true,
            "Address": {
                "StreetNumber": "50",
                "StreetName": "Brian Harrison Way",
                "Unit Number": 22,
                "City": "Toronto",
                "Country": "Canada"
            }
        },
        {
            "Name": "John",
            "Age": 34,
            "Department": "Finance",
            "Salary": "100000",
            "IsAdmin": true,
            "Address": {
                "StreetNumber": "50",
                "StreetName": "Brian Harrison Way",
                "Unit Number": 22,
                "City": "Toronto",
                "Country": "Canada"
            }
        }
    ]
}</code></pre>
<p>The above JSON object denotes one department with name as Finance and total number of employees as 2. Then the &quot;Employees&quot; object is an array of 2 emplyees. Each emplyee object further have a complex property as Address, which is another object. </p>
<p>If you understood each of the 3 components, you should be able to build/understand most complex JSON files with ease.</p>]]></description>
<link>http://HarvestingClouds.com/post/step-by-step-arm-templates-json-101-for-it-administrators</link>
<pubDate>Wed, 17 Aug 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>ASR Setup for VMs running in VMWare without VMware level User Access</title>
<description><![CDATA[<h3>Problem Statement</h3>
<p>Recently I was setting up <strong>Azure Site Recovery</strong> (or ASR) in an environment. We had multiple VMs in VMWare environment. The environment was managed by Third Party who did not want to give any service account for VMWare as their environment was shared with different customers. So we had access only to the VMs. Without relevant access, ASR for VMWare was out of the question for us.</p>
<h3>Solution</h3>
<p>We treated the VMs, in such environment, as physical machines when setting up ASR to replicate these machines to Azure.
We used the option of &quot;<strong><em>Not virtualized/other</em></strong>&quot; in the &quot;<strong><em>Prepare Infrastructure</em></strong>&quot; wizard of ASR. We <strong>treated the VMs as physical servers</strong> and did not face any issues during the migration. </p>
<p>Refer below screenshot for the exact option discussed above.</p>
<p><img src="http://HarvestingClouds.com/images/14696297155798c513ea908.png" alt="Protection Goal" /></p>
<p>Later when enabling the Replication for any Server, run the &quot;<strong><em>Enable Replication</em></strong>&quot; wizard by clicking on &quot;<strong>+Replicate</strong>&quot; on the ASR vault's blade. Then select &quot;<strong>Machine Type</strong>&quot; as &quot;<strong><em>Physical Machine</em></strong>&quot; and add the Physical Machines by mentioning their IP addresses. </p>
<p><strong>Note:</strong> For this approach to work, the Configuration server should be on the same network as the VM being considered as Physical Machine.</p>
<p>We were able to migrate many servers successfully and without any issues using this approach.</p>]]></description>
<link>http://HarvestingClouds.com/post/asr-setup-for-vms-running-in-vmware-without-vmware-level-user-access</link>
<pubDate>Tue, 26 Jul 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Uploading and Downloading files securely from Azure Storage Blob via PowerShell</title>
<description><![CDATA[<p><strong>Azure blob storage</strong> can provide a very highly available way to store your files in the cloud. You can dynamically add or remove the files in an automated fashion. These files can then be used for any number of purposes. E.g. A parameter file for ARM template can be kept in Azure blob storage and then dynamically read while creating resources from an ARM template.</p>
<p><strong>The whole process can be broken down into 3 parts</strong>:</p>
<ol>
<li>Generating the context to the storage container</li>
<li>Uploading the files using the context</li>
<li>Downloading the files using the context</li>
</ol>
<h3>1. Generating the context to the storage container</h3>
<p>The context to the storage blob container can be created in one of the 3 ways, based on your security requirements. All methods use the <code>New-AzureStorageContext</code> cmdlet to generate the storage context. The methods differ on how you pass the parameters to this cmdlet.</p>
<p><strong>A. Via fetching the Azure Storage Key</strong></p>
<p>This first method uses the <code>Get-AzureStorageKey</code> to fetch the storage key. This key is then used to generate the context as shown below.</p>
<pre><code>$StorageAccountName = "yourstorageaccount"
$StorageAccountKey = Get-AzureStorageKey -StorageAccountName $StorageAccountName
$Ctx = New-AzureStorageContext $StorageAccountName -StorageAccountKey $StorageAccountKey.Primary</code></pre>
<p><strong>B. Via fetching the Azure Storage Container SAS Token</strong></p>
<p>This second method uses the <code>New-AzureStorageContainerSASToken</code> to create a new SAS token to securely access the storage container. This token is then used to generate the context as shown below.</p>
<pre><code>$sasToken = New-AzureStorageContainerSASToken -Container abc -Permission rl
$Ctx = New-AzureStorageContext -StorageAccountName $StorageAccountName -SasToken $sasToken</code></pre>
<p><strong>C. Via Connectin String</strong></p>
<p>This third method uses a connection string, entered manually, which is then used to generate the context as shown below.</p>
<pre><code>$ConnectionString = "DefaultEndpointsProtocol=http;BlobEndpoint=&lt;blobEndpoint&gt;;QueueEndpoint=&lt;QueueEndpoint&gt;;TableEndpoint=&lt;TableEndpoint&gt;;AccountName=&lt;AccountName&gt;;AccountKey=&lt;AccountKey&gt;"
$Ctx = New-AzureStorageContext -ConnectionString $ConnectionString</code></pre>
<h3>2. Uploading the files using the context</h3>
<p>Now that you have the context to the storage account you can upload and download files from the storage blob container.
Use the below code to upload a file named &quot;<em>Parameters.json</em>&quot;, located on the local machine at &quot;<em>C:\Temp</em>&quot; directory.</p>
<pre><code>#Uploading File
$BlobName = "Parameters.json"
$localFile = "C:\Temp\" + $BlobName
$ContainerName  = "vhds"

#Note the Force switch will overwrite if the file already exists in the Azure container
Set-AzureStorageBlobContent -File $localFile -Container $ContainerName -Blob $BlobName -Context $Ctx -Force</code></pre>
<h3>3. Downloading the files using the context</h3>
<p>Download works in almost identical manner. You use the Get cmdlet instead of Set as shown below to download a file to a local folder, located at &quot;<em>C:\Downloads</em>&quot;.</p>
<pre><code>#Download File
$BlobName = "Parameters.json"
$localTargetDirectory = "C:\Downloads"
$ContainerName  = "vhds"

Get-AzureStorageBlobContent -Blob $BlobName -Container $ContainerName -Destination $localTargetDirectory -Context $ctx</code></pre>
<p>I hope this helps simplify the automated usage of Azure Storage container. Let us know your concerns or questions if any.</p>
<p>You can find the <strong>complete sample</strong> at the below link on GitHub. Right-click and select Save As to save the file: <a href="https://raw.githubusercontent.com/HarvestingClouds/PowerShellSamples/master/Scripts/StorageAccountBlobManagement.ps1">StorageAccountBlobManagement.ps1</a></p>
<p><strong>Reference:</strong> <a href="https://azure.microsoft.com/en-us/documentation/articles/storage-powershell-guide-full/" target="_blank">Using Azure PowerShell with Azure Storage</a></p>]]></description>
<link>http://HarvestingClouds.com/post/uploading-and-downloading-files-securely-from-azure-storage-blob-via-powershell</link>
<pubDate>Wed, 18 May 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure comes to Canada (along with Office 365)</title>
<description><![CDATA[<p>Last week marked the general availability of Azure datacenter for Canada locations. Also Office 365 has been released last week. Microsoft has set up 2 new datacenters in Canada. </p>
<h3>Where Exactly are these datacenters located</h3>
<ol>
<li><strong>Canada Central</strong> - The first datacenter is located in Toronto.</li>
<li><strong>Canada East</strong> - The second datacenter is located in Quebec City.</li>
</ol>
<p>Now when you are creating a new resource (like a Virtual Machine) you will see these two options.</p>
<p><img src="http://HarvestingClouds.com/images/1463686879573e16df4f64c.png" alt="New Locations" /></p>
<p>Check out the brief announcement video by <strong>Janet Kennedy</strong>, President of Microsoft Canada:</p>
<iframe src="https://channel9.msdn.com/Blogs/CANITPRO/The-Microsoft-Canada-Cloud-is-Open-for-Business/player" width="560" height="315" allowFullScreen frameBorder="0"></iframe>
<h3>Key Resources:</h3>
<ul>
<li>These locations are also listed in the official Microsoft Regions list here: <a href="https://azure.microsoft.com/en-us/regions/#services?WT.mc_id=azurebg_email_Trans_1106_Tier2_Release_MOSP" target="_blank">Azure Regions</a></li>
<li>Various resources and information for cloud in Canada are available here at <a href="https://www.microsoft.com/en-ca/sites/datacentre/default.aspx" target="_blank">Cloud Accelerate site for Canada</a>.</li>
<li>You can read about this announcement and upcoming features here: <a href="https://azure.microsoft.com/en-us/blog/microsoft-cloud-accelerates-in-canada-and-expands-to-south-korea/?WT.mc_id=azurebg_email_Trans_1106_Tier2_Release_MOSP" target="_blank">Microsoft Cloud accelerates in Canada and expands to South Korea</a></li>
</ul>]]></description>
<link>http://HarvestingClouds.com/post/azure-comes-to-canada-along-with-office-365</link>
<pubDate>Mon, 16 May 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>NEW Feature - Azure Cool Blob Storage</title>
<description><![CDATA[<p>Have you heard about the new <strong>Azure Cool Blob Storage</strong>? </p>
<p>If you haven’t heard about it, this is Microsoft's low-cost storage for <strong>Cool</strong> object data. “Example use cases for cool storage include backups, media content, scientific data, compliance and archival data. In general, any data which lives for a longer period of time and is accessed less than once a month is a perfect candidate for cool storage.” It is similar to what <strong>Glacier storage tier</strong> provides in Amazon Web Services.</p>
<ul>
<li><strong>Pricing:</strong> Its cost is as low as $0.01/GB.</li>
<li><strong>Availability:</strong> 99% (as compared to 99.9% for Hot Storage). With Read-access geo-redundant storage (or RA-GRS) the SLA is 99.9% (as compared to 99.99% for Hot).</li>
<li><strong>Deciding which AccessTier to use:</strong> If the objects in the storage account will be more frequently accessed, then go with <strong>Hot Tier</strong>. Select the <strong>Cold Tier</strong> for infrequently accessed data.</li>
</ul>
<p>Now when you go to New -&gt; &quot;Data + Storage&quot; -&gt; Storage Account, and try to create a Blob Storage account then you can select from one of the options for <strong>Access Tier</strong> from Cold or Hot tier. </p>
<p><img src="http://HarvestingClouds.com/images/146232375357294a2980ece.png" alt="Storage Tiers" /></p>
<p>Also, note that at the time of this writing, Blob storage account is <strong>only available in these locations</strong>: Central US, East US 2, North Central US, North Europe, West Europe, Southeast Asia, Japan East, Japan West, Central India, South India, West India.</p>
<p><strong>Resources to know more:</strong>  </p>
<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/introducing-azure-cool-storage/" target="_blank">Official Announcement</a></li>
<li><a href="https://azure.microsoft.com/en-us/documentation/articles/storage-blob-storage-tiers/" target="_blank">Getting started guide</a></li>
</ul>]]></description>
<link>http://HarvestingClouds.com/post/new-feature-azure-cool-blob-storage</link>
<pubDate>Mon, 02 May 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Taking Automatic Remediation Action on Azure VM Alert Generation</title>
<description><![CDATA[<p>Using a new feature in Azure, now you can easily configure to trigger an Azure Automation Runbook when an Alert is triggered on an Azure Virtual Machine to take a remediation action. To leverage this feature all you need to do is link the alert on Azure VM to an already existing Azure Automation Runbook.</p>
<blockquote>
<p>Note: This feature is supported only for the V2 Virtual Machines, i.e. the VMs created using ARM portal.</p>
</blockquote>
<p>To access this feature open your Virtual Machine. Then go to the Manage alerts section in the Settings:</p>
<p><img src="http://HarvestingClouds.com/images/14618976705722c9c653752.png" alt="Setting - Manage alerts" /></p>
<p>Then open an existing alert or click on &quot;Add alert&quot; to create a new one. Specify the criteria for the alert. Scroll down to the bottom and you can view the new section to link the alert to an Automation Runbook.</p>
<p><img src="http://HarvestingClouds.com/images/14618990795722cf4763dce.png" alt="Automation Runbook for Alert" /></p>
<h3>Under the hood</h3>
<p>The alert will send data to your Runbook in a special format. Your Runbook should be expecting this. Under the hood this happens via WebHooks. The alert data is passed via a HTTP POST request. The Automation webhook service extracts the alert data from the POST request and passes it to the runbook in a parameter called <strong>&quot;WebhookData&quot;</strong>. The Runbook will look like below:</p>
<pre><code>[OutputType("PSAzureOperationResponse")]

param ( [object] $WebhookData )

if ($WebhookData)
{
    # Get the data object from WebhookData
    $WebhookBody = (ConvertFrom-Json -InputObject $WebhookData.RequestBody)

    #Rest of the script comes here
}</code></pre>
<p><strong>In Nutshell</strong>, now you can now trigger Azure Automation Runbooks to take remediation actions on Virtual Machines in case an alert is triggered. </p>
<p><strong>Reference with complete Runbook sample:</strong> <a href="https://azure.microsoft.com/en-us/documentation/articles/automation-azure-vm-alert-integration/">Azure Automation solution - remediate Azure VM alerts</a></p>]]></description>
<link>http://HarvestingClouds.com/post/taking-automatic-remediation-action-on-azure-vm-alert-generation</link>
<pubDate>Wed, 27 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>ROADMAP - Solutions to help with Migration from Azure ASM to ARM portal</title>
<description><![CDATA[<p>In additional to the tool I mentioned yesterday regarding <a href="http://harvestingclouds.com/Migrating-from-Azure-ASM-to-ARM-portal">Migrating from Azure ASM to ARM portal</a> there are various solutions in the pipeline. This post looks at the high level Roadmap for the same from Microsoft.</p>
<p>Microsoft has promised that they are committed to make the migration more easier from ASM (older) to ARM (newer) portal. Various solutions are already in the pipeline for this.
Below are the details and roadmap for the tentative timelines for these solutions.</p>
<table border="1" cellpadding="0" cellspacing="0"> <tbody> <tr> <td valign="top" width="29%"> <p><b>Solution</b></p> </td> <td valign="top" width="51%"> <p><b>Customer Experience</b></p> </td> <td valign="top" width="18%"> <p><b>Expected availability in 2016</b></p> </td> </tr> <tr> <td valign="top" width="29%"> <p>Script migration</p> </td> <td valign="top" width="51%"> <p>VM is rebooted as it is recreated in the Resource Manager model. While the Virtual Machines for the environment are recreated, the network is disconnected.</p> </td> <td valign="top" width="18%"> <p align="center">Q1</p> </td> </tr> <tr> <td valign="top" width="29%"> <p>Virtual Machines, no VNET</p> </td> <td valign="top" width="51%"> <p>As all Virtual Machines deployed in the Resource Manager model must be in a VNet, Virtual Machines will be migrated and placed in a new VNET. This will result in a change in network configuration, requiring a reboot to reconnect.</p> </td> <td valign="top" width="18%"> <p align="center">Q2</p> </td> </tr> <tr> <td valign="top" width="29%"> <p>Virtual Machines with VNET</p> </td> <td valign="top" width="51%"> <p>Starting in Q2, the platform will offer Virtual Machine migration from ASM to Resource Manager model without disrupting the running Virtual Machine. This will require disconnecting any VNets connected on-premises, whether via ExpressRoute or VPN, before doing the migration.</p> </td> <td valign="top" width="18%"> <p align="center">Q2</p> </td> </tr> <tr> <td valign="top" width="29%"> <p>Virtual Machines with basic hybrid (one connection)</p> </td> <td valign="top" width="51%"> <p>Starting in Q3, the platform will offer Virtual Machine migration from ASM to Resource Manager model without disrupting the running Virtual Machine and with minimal disruption to a basic hybrid connection, limited to just one connection back on-premises. More complex connections will require disconnecting before doing the migration.</p> </td> <td valign="top" width="18%"> <p align="center">Q3</p> </td> </tr> </tbody> </table>
<p>Reference: <a href="https://azure.microsoft.com/en-us/blog/transitioning-to-the-resource-manager-model/">Transitioning to the Resource Manager model</a></p>]]></description>
<link>http://HarvestingClouds.com/post/roadmap-solutions-to-help-with-migration-from-azure-asm-to-arm-portal</link>
<pubDate>Fri, 22 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Migrating from Azure ASM to ARM portal</title>
<description><![CDATA[<p>With co-existing Azure Service Management or ASM portal (older) and Azure Resource Manager or ARM portal (newer) there has been lots of confusions and problems for IT administrators.
The bottom line of all the discussion around the two portals is that <strong>ARM is the future and is here to stay</strong>. It means that you need to <strong>plan and migrate</strong> your resources from ASM portal to the ARM portal.</p>
<p>The key resource is your infrastructure which primarily consists of virtual machines. To migrate a single Virtual Machine (VM) from ASM portal to ARM portal you can leverage a set of PowerShell scripts called ASM2ARM.
You can download these scripts and check their description on <a href="https://github.com/fullscale180/asm2arm">GitHub here on the <strong>ASM2ARM</strong> page</a>. You can check the detailed instructions there too.</p>
<p>To plan this right now is very important as the transitioning to Azure Resource Manager model is already underway. Any future development and investment seems to be happening only in the newer portal only.</p>
<p><strong>Reference:</strong> <a href="https://github.com/fullscale180/asm2arm">ASM2ARM scripts on GitHub</a></p>]]></description>
<link>http://HarvestingClouds.com/post/migrating-from-azure-asm-to-arm-portal</link>
<pubDate>Thu, 21 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>PowerShell DSC - Partial Configurations</title>
<description><![CDATA[<p><strong>Partial Configurations</strong> is a new feature in PowerShell 5.0 Desired State Configuration or DSC. It allows the configurations to be delivered in parts or fragments. These configurations can come from various sources.
The Local Configuration Manager or LCM on the target node puts these partial configurations from different sources together and after that apply the same as a single configuration.</p>
<p>This opens various possibilities for Enterprises to manage their infrastructure and designate the responsibility to various teams for a single node. The team expert in a particular field can focus on that feature without worrying about other features.</p>
<p>You can have partial configurations in following modes:</p>
<ol>
<li>Push Mode</li>
<li>Pull Mode</li>
<li>Hybrid Mode (i.e. combination of Push and Pull)</li>
</ol>
<h3>Configuration for the PUSH Mode</h3>
<p>You need to follow three steps to configure Partial configurations for the PUSH mode:</p>
<ul>
<li>Configure the LCM, on the target node, to expect partial configurations</li>
<li>Push each partial configuration from different sources using <strong>Publish-DSCConfiguration</strong> cmdlet. Target node will automatically combine the partial configurations into single configuration.</li>
<li>Apply the configuration by calling the <strong>Start-DSCConfiguration</strong>cmdlet</li>
</ul>
<h3>Configuration for the PULL Mode</h3>
<p>This is bit complex than the Push mode. In nutshell you only need couple of steps:</p>
<ul>
<li>Configure the LCM, on the target node, to receive partial configurations but from PULL servers</li>
<li>Name and locate the configuration documents properly on the pull servers</li>
</ul>
<p>To know more about DSC Partial configurations follow the below references:</p>
<ul>
<li><a href="https://automationnext.wordpress.com/2016/04/19/powershell-desired-state-configuration-partial-configurations-without-configurationid/">Detailed Blog by AutomationNext with very valuable insights</a></li>
<li><a href="https://msdn.microsoft.com/en-us/powershell/dsc/partialconfigs">Official MSDN Article</a></li>
</ul>]]></description>
<link>http://HarvestingClouds.com/post/powershell-dsc-partial-configurations</link>
<pubDate>Wed, 20 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Container Service hits General Availability</title>
<description><![CDATA[<p>Azure Container Service has finally hit General Availability today. </p>
<p>If you don't know already, it is the &quot;container hosting solution&quot; which is optimized for Microsoft's Azure cloud.
All the tools that you may be familiar with when working with a Container Service should work like Apache Mesos or Docker Swarm. It only uses open source components in the orchestration layers to give you portability of full applications.</p>
<p>You can find the announcement here: <a href="https://azure.microsoft.com/en-us/updates/general-availability-azure-container-service/">GA for Azure Container Service</a></p>
<p>You can learn more about the Container Service as offered by Azure on the product page here: <a href="https://azure.microsoft.com/en-us/services/container-service/">Azure Container Service</a></p>]]></description>
<link>http://HarvestingClouds.com/post/azure-container-service-hits-general-availability</link>
<pubDate>Tue, 19 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Azure Authentication - Authenticating any Azure API Request in your Application</title>
<description><![CDATA[<p>I have created a code sample to showcase how you can authenticate any request programatically with Azure.
This also contains <strong>a Reusable Authentication Helper class</strong> which you can directly use in your code.</p>
<h3>Where is the code</h3>
<p>You can find the complete code sample along with the reusable Azure Authentication Helper class library from this GitHub repo:
<a href="https://github.com/HarvestingClouds/AzureAuthentication">Azure Authentication Sample</a></p>
<h3>What are my authentication Options</h3>
<p>You have the following options</p>
<ul>
<li>Authenticating by <strong>Prompting</strong> for Credentials from end user. (This needs end user interaction)</li>
<li>Authenticating by <strong>Credentials</strong> i.e. using a password. (This does not need any end user interaction)</li>
<li>Authenticating by using a <strong>Certificate</strong> ( This also does not need any end user interaction)</li>
</ul>
<p>I have provided this functionality in 3 separate methods, in a separate class file along with it's interface.
You can follow the instructions in the ReadMe file in the GitHub repo and start using any one of the method.</p>
<p>I hope you find this usefull and this will avoid the trouble of figuring things out, which I have already undergone. </p>
<p>Let me know in the comments below if you have any questions or anything to add to this.</p>]]></description>
<link>http://HarvestingClouds.com/post/azure-authentication-authenticating-any-azure-api-request-in-your-application</link>
<pubDate>Fri, 15 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Run Azure Automation Runbooks via PowerShell ISE</title>
<description><![CDATA[<p>Today I came across this blog post from my friend: <a href="https://scomanswers.wordpress.com/2016/04/11/azure-automation-powershell-ise-add-on/">Azure Automation PowerShell ISE add-on</a></p>
<p>What I came to know is that now you can Run the Azure Automation Runbooks via PowerShell ISE. This solves a big pain point for all Azure developers.
Now you will be able to develop and test your scripts right from the convenience of your laptop's local PowerShell ISE. </p>
<h3>What you need to do</h3>
<p>All you need to do is install the PowerShell Add-On using the below cmdlet:</p>
<pre><code class="language-powershell">Find-Module AzureAutomationAuthoringToolkit | Install-Module -Scope CurrentUser</code></pre>
<p>Then import the module using below cmdlet:</p>
<pre><code class="language-powershell">Import-Module AzureAutomationAuthoringToolkit</code></pre>
<p>You can configure the Add-On using a Configuration tab in the add-on and start getting your hands dirty. </p>
<h3>Official Information from the Add-On Help</h3>
<h4>Capabilities</h4>
<ul>
<li>Test runbooks on your local machine and in the Azure Automation service: </li>
<li>Store and edit Automation Assets locally </li>
<li>Use Automation Activities (Get-AutomationVariable, Get-AutomationPSCredential, etc) in local PowerShell scripts </li>
<li>Sync changes back to your Automation Account </li>
<li>Run test jobs in Automation and view results </li>
</ul>
<h4>Notes</h4>
<p>Assets</p>
<ul>
<li>Secret values (passwords, encrypted variables) are not downloaded automatically; they need to be set manually the first time the account is synced </li>
<li>Values that haven't been downloaded will be highlighted </li>
<li>Asset values you enter locally will not get overwritten when you sync from the cloud </li>
</ul>
<p>Runbooks </p>
<ul>
<li>Native PowerShell and PowerShell Workflow runbooks are supported </li>
</ul>
<p>Check the screenshot regarding this information below:
<img src="http://HarvestingClouds.com/images/1461735632572050d069253.png" alt="Official Notes" title="Official Notes" /></p>
<h3>How much time it would take me</h3>
<p>In all it would take you under 10 mins to get setup and rolling.</p>
<h3>Where is more information on this and screenshots</h3>
<p>Go to the official <a href="https://blogs.technet.microsoft.com/msoms/2016/04/08/the-way-cool-azure-automation-powershell-ise-add-on/">Technet blog by clicking HERE.</a></p>
<p>Start playing around and let us know your initial impression in the comments below. If you have any doubts and I will be happy to address them.</p>]]></description>
<link>http://HarvestingClouds.com/post/run-azure-automation-runbooks-via-powershell-ise</link>
<pubDate>Thu, 14 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Various Options Added to Buy Microsoft Azure Active Directory Basic</title>
<description><![CDATA[<p>Today Microsoft announced that they have added various options to buy Microsoft Azure Active Directory (AAD) Basic.
You can now buy it through the Direct program as well as through following options:</p>
<ul>
<li><a href="https://www.microsoft.com/en-us/licensing/licensing-programs/enterprise.aspx?WT.mc_id=azurebg_email_Trans_1065_Tier2_Release_MOSP">Microsoft Enterprise Agreement</a></li>
<li><a href="https://www.microsoft.com/en-us/licensing/licensing-programs/open-license.aspx?WT.mc_id=azurebg_email_Trans_1065_Tier2_Release_MOSP">Open Volume License Program</a></li>
<li><a href="https://partner.microsoft.com/en-US/Solutions/cloud-reseller-overview?WT.mc_id=azurebg_email_Trans_1065_Tier2_Release_MOSP">Microsoft Cloud Solution Provider</a></li>
</ul>
<p>To purchase, sign in to the <a href="https://portal.office.com">Office 365 Administration Portal</a></p>
<p>You can also watch the below video for details. Although the video is for AAD Premium, the steps are essentially similar for AAD Basic.</p>
<iframe src="https://channel9.msdn.com/Series/Azure-Active-Directory-Videos-Demos/How-to-Purchase-Azure-Active-Directory-Premium-Existing-Customer/player" width="560" height="315" allowFullScreen frameBorder="0"></iframe>
<p>You can also engage a partner to assist you with the purchase and your Azure Active Directory related any requirements.
<a href="http://www.infrontconsulting.com/">Infront Consulting Group</a> (where I currently work) is one such partner who are highly respected in market and are Microsoft Gold Certified Partner. </p>
<p>Thanks for reading! If you have any questions please ask in the comments below.</p>]]></description>
<link>http://HarvestingClouds.com/post/various-options-added-to-buy-microsoft-azure-active-directory-basic</link>
<pubDate>Wed, 13 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Multiple Values In Grid.Mvc Single Column Filter via Checkboxes with Code Sample</title>
<description><![CDATA[<p>I have been struggling to implement multiple filters in a single column in Grid.Mvc tool. I have solved this by altering the code and updating the custom widget.
<strong>Note:</strong> The WithMultipleFilters() option will not help you in this. That option enables multiple filters on different columns. To have multiple filters in the same column you need to update the way filtering works in the tool itself.</p>
<p>I have used a list of checkboxes and any or all of the elements selected in this checkbox list will be used for filtering the column values.</p>
<p>You can find the code in my fork of the official Grid.Mvc repo at below link:
<a href="https://github.com/HarvestingClouds/Grid.Mvc" target="_blank">Fork of Grid.Mvc repo with Advance Filters</a></p>
<p>I have also created a pull request for the same so that more people get benefit from this if they refer the master branch of the main repo.</p>
<h3>What are the changes I have done?</h3>
<p>I have made changes to two files:</p>
<ol>
<li><strong>DefaultColumnFilter.cs</strong> file in &quot;<strong>GridMvc</strong>&quot; class library project under the Filters folder. I have updated the GetFilterExpression method to create multiple expressions based on the pipeline character in filter values.</li>
<li><strong>gridmvc.customwidgets.js</strong> file in &quot;<strong>GridMvc.Site</strong>&quot; web application project</li>
</ol>
<p>Both of these paths are shown below:
Location of DefaultColumnFilter.cs:
<img src="http://HarvestingClouds.com/images/146173541957204ffb99678.png" alt="DefaultColumnFilter.cs" title="DefaultColumnFilter.cs" /></p>
<p>Location of gridmvc.customwidgets.js:
<img src="http://HarvestingClouds.com/images/146173543057205006529e6.png" alt="gridmvc.customwidgets.js" title="gridmvc.customwidgets.js" /></p>
<p>How the end result look like:
<img src="http://HarvestingClouds.com/images/14617354255720500171fe7.png" alt="Checkbox Filtering" title="Checkbox Filtering" /></p>
<p>You can directly use the code if you want. Just honor the license of the original author.</p>
<p>Let me know in the comments below if you have any doubts and I will be happy to address them.</p>]]></description>
<link>http://HarvestingClouds.com/post/multiple-values-in-gridmvc-single-column-filter-via-checkboxes-with-code-sample</link>
<pubDate>Tue, 12 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Getting Started - Azure Site Recovery (ASR) In New Azure Portal</title>
<description><![CDATA[<p>Azure Site Recovery or ASR is now available in the new Azure Resource Manager or ARM portal (codename Ibiza) with modern user interface. It is in preview at this stage. But it is production ready for all the Hyper-V related scenarios.
<strong>Your older Vaults (created via Classic ASM Azure Portal) will not be available in ASR preview feature.</strong></p>
<h3>What are the new features</h3>
<p>The new features include:</p>
<ul>
<li>All the goodness of Azure Resource Manager in ASR</li>
<li>Lean experience for various ASR scenarios</li>
<li>Enhancements to the specific Site Recovery scenarios</li>
</ul>
<h3>Lets take a quick look at some of these.</h3>
<p>If you Browse and search for &quot;Recovery&quot; you get Recovery Services Vaults as Preview feature.
<img src="http://HarvestingClouds.com/images/14617358075720517f78904.png" alt="Browse and Search" title="Browse and Search" /></p>
<p>Clicking on it will open up the blade for &quot;Recovery Services valuts&quot;. Notice that Microsoft has PREVIEW text in this.
<img src="http://HarvestingClouds.com/images/1461735702572051161683f.png" alt="alt text" title="ASR Vault" /></p>
<p>Clicking on the Add button brings up the ASR vault creation blade. Notice the locations available for vault creation here.
<img src="http://HarvestingClouds.com/images/14617359385720520210878.png" alt="Vault Creation" title="Vault Creation" /></p>
<p>After you hit create the Vault gets deployed really quickly. I tested for East US location and it was created in under 10 secs.
Refresh to view your newly created vault. Click on it to open the NEW ASR Vault features. Notice that the Backup feature is also there in the ASR vault now.
<img src="http://HarvestingClouds.com/images/1461735907572051e31549a.png" alt="New Vault" title="New Vault" /></p>
<p>To find the options for replication go to Settings -&gt; Getting Started section -&gt; Site Recovery -&gt; Follow Wizard.
<img src="http://HarvestingClouds.com/images/1461735869572051bd2d6fa.png" alt="New Site Recovery Wizard" title="New Site Recovery Wizard" /></p>
<p>The Scenario Types available are only two. But all the scenarios are covered here:</p>
<ul>
<li>From my site to Azure</li>
<li>From my site to another site</li>
</ul>
<p>Based on the scenario you select you are asked for different options. The options for Virtualization/Management Server type for &quot;From my site to Azure&quot; are:</p>
<ul>
<li>VMM</li>
<li>Stand alone Hyper-V hosts</li>
<li>vCenter</li>
<li>Physical machines (not virtualized)
<img src="http://HarvestingClouds.com/images/14617358385720519e578ba.png" alt="Creation Options" title="Creation Options" /></li>
</ul>
<h3>Backup in ASR vault</h3>
<p>Another feature is creation of Backups from the same vault. Click on the + icon for Backup in the Vault main blade and then follow the wizard for the preview feature.
<img src="http://HarvestingClouds.com/images/14617357575720514d5d1d8.png" alt="Backup In ASR" title="Backup In ASR" />
Notice in the screenshot above that the backup types available are:</p>
<ul>
<li>Azure virtual machine backup</li>
<li>File Folder backup</li>
<li>System Center Data Protection Manager</li>
</ul>
<p>Selecting each option provides you with details for next steps. You can then create a backup policy and configure Items to backup.</p>
<p>Give these features a try and let us know in comments below how you find the new features.
Happy Exploring!</p>]]></description>
<link>http://HarvestingClouds.com/post/getting-started-azure-site-recovery-asr-in-new-azure-portal</link>
<pubDate>Sat, 09 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Coming Soon - Windows 10 Anniversary Update</title>
<description><![CDATA[<p>Windows 10 Anniversary Update is coming this summary. It will be available for free download for the following devices (which is almost every device):</p>
<ul>
<li>PCs</li>
<li>Tablets</li>
<li>Phones</li>
<li>Xbox One</li>
<li>Microsoft HoloLens</li>
<li>IoT</li>
</ul>
<h2>What this means to you:</h2>
<ul>
<li>Improved Biometric Security</li>
<li>Microsoft Edge browser</li>
<li>Windows Ink (where just one click of pen will bring up all the gamut available for use with your Pen device)</li>
<li>Universal Windows Platform or UWP apps are coming to XBox through a Unified Windows Store. Also if you own a XBox you will be able to turn it into a dev box and do development with it</li>
<li>Various improvements to Cortana</li>
</ul>
<p><a href="https://www.microsoft.com/en-us/windows/upcoming-features" target="_blank">Check out more details here</a></p>]]></description>
<link>http://HarvestingClouds.com/post/coming-soon-windows-10-anniversary-update</link>
<pubDate>Fri, 08 Apr 2016 00:00:00 +0500</pubDate>
</item>
<item>
<title>Introducing Harvesting Clouds</title>
<description><![CDATA[<p>Harvesting Clouds is a blog about all things Cloud. Be it Private Cloud or Public Cloud, I will try to cover various aspects of both.</p>
<h3>Private Cloud</h3>
<p>My key areas of interest in Private Cloud include the following:</p>
<ul>
<li>PowerShell Scripting</li>
<li>Windows Azure Pack or WAP</li>
<li>Service Management Automation or SMA</li>
<li>Azure Stack</li>
<li>System Center Orchestrator</li>
<li>System Center VMM and other products like Service Manager, Ops Mgr, etc.</li>
</ul>
<h3>Public Cloud</h3>
<p>In addition to the Private Cloud the areas of interest in Public Cloud are:</p>
<ul>
<li>Microsoft Azure and Amazon Web Services - both IaaS and PaaS</li>
<li>Azure Automation</li>
<li>Desired State Configurations</li>
<li>Application Insights</li>
<li>Azure Web Apps</li>
<li>Web APIs</li>
<li>Azure Site Recovery and Backup</li>
<li>Migrations from Private to Public Clouds</li>
</ul>
<h3>Common Areas &amp; Best of both worlds</h3>
<p>I have also been involved in creating Hybrid clouds leveraging the best of both worlds. I will try to share my knowledge on this with you. The key aspects in this area are:</p>
<ul>
<li>Building Hybrid Solutions</li>
<li>Developing Web or Desktop Applications targetting either or both the clouds (using MVC, Dot Net)</li>
<li>Using TFS Online, Visual Studio, GitHub to better collaborate and work in an automated fashion</li>
<li>Release Manager to automate your release workflows</li>
</ul>
<h3>Primary Focus</h3>
<p>As you must have guessed by now, the primary focus for this blog will be Microsoft Technologies. We will also explore beyond this and will be talking about various emerging open source technologies and the new Better Together world with the amalgamation of various technologies in one solution.</p>
<p>I invite to take this journey with me!
Keep learning!</p>]]></description>
<link>http://HarvestingClouds.com/post/introducing-harvesting-clouds</link>
<pubDate>Fri, 01 Apr 2016 00:00:00 +0500</pubDate>
</item>
</channel>
</rss>
